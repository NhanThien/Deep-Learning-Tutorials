{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mid level APIs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GBjyyADDiIO0",
        "colab_type": "code",
        "outputId": "69907220-32a1-422b-c465-566ec2c42ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2094
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Load the data\n",
        "wine_data = datasets.load_wine()\n",
        "\n",
        "# Display information about the dataset\n",
        "print(wine_data.keys())\n",
        "for key,value in wine_data.items():\n",
        "    print(key,'\\n',value,'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n",
            "data \n",
            " [[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
            " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
            " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
            " ...\n",
            " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
            " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
            " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]] \n",
            "\n",
            "target \n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] \n",
            "\n",
            "target_names \n",
            " ['class_0' 'class_1' 'class_2'] \n",
            "\n",
            "DESCR \n",
            " .. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178 (50 in each of three classes)\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  Comparison of Classifiers in High Dimensional Settings, \n",
            "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Technometrics). \n",
            "\n",
            "  The data was used with many others for comparing various \n",
            "  classifiers. The classes are separable, though only RDA \n",
            "  has achieved 100% correct classification. \n",
            "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "  (All results using the leave-one-out technique) \n",
            "\n",
            "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "  Mathematics and Statistics, James Cook University of North Queensland. \n",
            "  (Also submitted to Journal of Chemometrics).\n",
            " \n",
            "\n",
            "feature_names \n",
            " ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xii0pEfCtiwY",
        "colab_type": "code",
        "outputId": "68bcba73-1763-42e6-c7c5-8def5800afab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert to Pandas dataframe\n",
        "wineDF = pd.DataFrame(data=wine_data['data'],columns=wine_data['feature_names'])\n",
        "wineDF['target']=wine_data['target']\n",
        "wineDF.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "\n",
              "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0        3.06                  0.28             2.29             5.64  1.04   \n",
              "1        2.76                  0.26             1.28             4.38  1.05   \n",
              "2        3.24                  0.30             2.81             5.68  1.03   \n",
              "3        3.49                  0.24             2.18             7.80  0.86   \n",
              "4        2.69                  0.39             1.82             4.32  1.04   \n",
              "\n",
              "   od280/od315_of_diluted_wines  proline  target  \n",
              "0                          3.92   1065.0       0  \n",
              "1                          3.40   1050.0       0  \n",
              "2                          3.17   1185.0       0  \n",
              "3                          3.45   1480.0       0  \n",
              "4                          2.93    735.0       0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "2agVGXHaIERw",
        "colab_type": "code",
        "outputId": "da50a916-d51a-4ab0-96f8-39547e2df1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "#Scale data to [0,1]\n",
        "data=wine_data['data']\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "data_scaled = min_max_scaler.fit_transform(data)\n",
        "wineDF_scaled = pd.DataFrame(data=data_scaled,columns=wine_data['feature_names'])\n",
        "wineDF_scaled.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.191700</td>\n",
              "      <td>0.572193</td>\n",
              "      <td>0.257732</td>\n",
              "      <td>0.619565</td>\n",
              "      <td>0.627586</td>\n",
              "      <td>0.573840</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.593060</td>\n",
              "      <td>0.372014</td>\n",
              "      <td>0.455285</td>\n",
              "      <td>0.970696</td>\n",
              "      <td>0.561341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.571053</td>\n",
              "      <td>0.205534</td>\n",
              "      <td>0.417112</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>0.326087</td>\n",
              "      <td>0.575862</td>\n",
              "      <td>0.510549</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.274448</td>\n",
              "      <td>0.264505</td>\n",
              "      <td>0.463415</td>\n",
              "      <td>0.780220</td>\n",
              "      <td>0.550642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.560526</td>\n",
              "      <td>0.320158</td>\n",
              "      <td>0.700535</td>\n",
              "      <td>0.412371</td>\n",
              "      <td>0.336957</td>\n",
              "      <td>0.627586</td>\n",
              "      <td>0.611814</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.757098</td>\n",
              "      <td>0.375427</td>\n",
              "      <td>0.447154</td>\n",
              "      <td>0.695971</td>\n",
              "      <td>0.646933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.878947</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.609626</td>\n",
              "      <td>0.319588</td>\n",
              "      <td>0.467391</td>\n",
              "      <td>0.989655</td>\n",
              "      <td>0.664557</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.558360</td>\n",
              "      <td>0.556314</td>\n",
              "      <td>0.308943</td>\n",
              "      <td>0.798535</td>\n",
              "      <td>0.857347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.581579</td>\n",
              "      <td>0.365613</td>\n",
              "      <td>0.807487</td>\n",
              "      <td>0.536082</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.627586</td>\n",
              "      <td>0.495781</td>\n",
              "      <td>0.490566</td>\n",
              "      <td>0.444795</td>\n",
              "      <td>0.259386</td>\n",
              "      <td>0.455285</td>\n",
              "      <td>0.608059</td>\n",
              "      <td>0.325963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
              "0  0.842105    0.191700  0.572193           0.257732   0.619565   \n",
              "1  0.571053    0.205534  0.417112           0.030928   0.326087   \n",
              "2  0.560526    0.320158  0.700535           0.412371   0.336957   \n",
              "3  0.878947    0.239130  0.609626           0.319588   0.467391   \n",
              "4  0.581579    0.365613  0.807487           0.536082   0.521739   \n",
              "\n",
              "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
              "0       0.627586    0.573840              0.283019         0.593060   \n",
              "1       0.575862    0.510549              0.245283         0.274448   \n",
              "2       0.627586    0.611814              0.320755         0.757098   \n",
              "3       0.989655    0.664557              0.207547         0.558360   \n",
              "4       0.627586    0.495781              0.490566         0.444795   \n",
              "\n",
              "   color_intensity       hue  od280/od315_of_diluted_wines   proline  \n",
              "0         0.372014  0.455285                      0.970696  0.561341  \n",
              "1         0.264505  0.463415                      0.780220  0.550642  \n",
              "2         0.375427  0.447154                      0.695971  0.646933  \n",
              "3         0.556314  0.308943                      0.798535  0.857347  \n",
              "4         0.259386  0.455285                      0.608059  0.325963  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "9BQ3CgMPtizS",
        "colab_type": "code",
        "outputId": "0e32c215-1c1c-452a-e291-d2fb0f1e6666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#Split into train and test set\n",
        "train_data, test_data, train_label, test_label = train_test_split(data_scaled,wine_data['target'],test_size=0.2)\n",
        "print('Train data: ',train_data.shape)\n",
        "print('Train label: ',train_label.shape)\n",
        "print('Test data: ',test_data.shape)\n",
        "print('Test label: ',test_label.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:  (142, 13)\n",
            "Train label:  (142,)\n",
            "Test data:  (36, 13)\n",
            "Test label:  (36,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xQuFB-Xbti5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define number of node in input, output and each hidden layer \n",
        "n_features=train_data.shape[1]\n",
        "n_classes=3\n",
        "n_hidden1=128\n",
        "n_hidden2=64\n",
        "\n",
        "# Reset the default graph in the case if we want to rerun this code blick\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Define placeholder for inputData\n",
        "inputData=tf.placeholder(tf.float64,(None,n_features),name=\"input\")\n",
        "\n",
        "# Define each layer for the neural network\n",
        "hidden1=tf.layers.dense(inputData,n_hidden1,activation=tf.nn.relu,name=\"layer1\")\n",
        "hidden2=tf.layers.dense(hidden1,n_hidden2,activation=tf.nn.relu,name=\"layer2\")\n",
        "logit=tf.layers.dense(hidden2,n_classes,name=\"logit\")\n",
        "\n",
        "# Define the loss function\n",
        "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_label,logits=logit)\n",
        "loss=tf.reduce_mean(cross_entropy,name=\"loss\")\n",
        "\n",
        "# Define Optimization algorithm\n",
        "learning_rate=0.01\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
        "training_op=optimizer.minimize(loss)                       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6rrCY0960wK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the Model\n",
        "\n",
        "# Define placeholder for the actual labels\n",
        "actual_labels=tf.placeholder(tf.int64,name=\"labels\")\n",
        "\n",
        "# Take the highest value as the prediction \n",
        "pred=tf.argmax(logit,1)\n",
        "\n",
        "# Compare the prediction to the actual label\n",
        "correct_pred=tf.cast(tf.equal(pred,actual_labels),tf.float64)\n",
        "\n",
        "# Calculate the accuracy by averaging the correct_pred\n",
        "accuracy=tf.reduce_mean(correct_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frjblk-nWR6z",
        "colab_type": "code",
        "outputId": "64b036a9-5f93-4e5c-9c64-9d3e85a228e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the number of epoch\n",
        "epoches=50\n",
        "\n",
        "# Create a session and initialse all variables\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "train_accuracyList=[]\n",
        "test_accuracyList=[]\n",
        "\n",
        "for i in range(epoches):\n",
        "  # Train the model and calculate the accuracy of the current model\n",
        "  _,train_accuracy=sess.run((training_op,accuracy),feed_dict={inputData:train_data,actual_labels:train_label})\n",
        "  train_accuracyList.append(train_accuracy)\n",
        "  \n",
        "  # Calculate the accuracy of the Model with the testset\n",
        "  test_accuracy=sess.run(accuracy,feed_dict={inputData:test_data,actual_labels:test_label})\n",
        "  test_accuracyList.append(test_accuracy)\n",
        "  \n",
        "  # Display the accuracy result\n",
        "  print(i+1,'. Train accuracy: ',train_accuracy,'Test accuracy: ',test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 . Train accuracy:  0.2535211267605634 Test accuracy:  0.5\n",
            "2 . Train accuracy:  0.5704225352112676 Test accuracy:  0.8055555555555556\n",
            "3 . Train accuracy:  0.9154929577464789 Test accuracy:  0.9444444444444444\n",
            "4 . Train accuracy:  0.971830985915493 Test accuracy:  1.0\n",
            "5 . Train accuracy:  0.9507042253521126 Test accuracy:  1.0\n",
            "6 . Train accuracy:  0.9507042253521126 Test accuracy:  1.0\n",
            "7 . Train accuracy:  0.9647887323943662 Test accuracy:  1.0\n",
            "8 . Train accuracy:  0.9788732394366197 Test accuracy:  1.0\n",
            "9 . Train accuracy:  0.9788732394366197 Test accuracy:  1.0\n",
            "10 . Train accuracy:  0.9788732394366197 Test accuracy:  1.0\n",
            "11 . Train accuracy:  0.9788732394366197 Test accuracy:  1.0\n",
            "12 . Train accuracy:  0.9788732394366197 Test accuracy:  0.9722222222222222\n",
            "13 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9722222222222222\n",
            "14 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9722222222222222\n",
            "15 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9722222222222222\n",
            "16 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9722222222222222\n",
            "17 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9444444444444444\n",
            "18 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9444444444444444\n",
            "19 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9444444444444444\n",
            "20 . Train accuracy:  0.9929577464788732 Test accuracy:  0.9444444444444444\n",
            "21 . Train accuracy:  1.0 Test accuracy:  0.9444444444444444\n",
            "22 . Train accuracy:  1.0 Test accuracy:  0.9444444444444444\n",
            "23 . Train accuracy:  1.0 Test accuracy:  0.9444444444444444\n",
            "24 . Train accuracy:  1.0 Test accuracy:  0.9444444444444444\n",
            "25 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "26 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "27 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "28 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "29 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "30 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "31 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "32 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "33 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "34 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "35 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "36 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "37 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "38 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "39 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "40 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "41 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "42 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "43 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "44 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "45 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "46 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "47 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "48 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "49 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n",
            "50 . Train accuracy:  1.0 Test accuracy:  0.9722222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W-_ak93K8FYC",
        "colab_type": "code",
        "outputId": "e3142032-4cc0-487e-884c-6525bbea0e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the train_accuracy and test_accuracy\n",
        "plt.plot(train_accuracyList)\n",
        "plt.plot(test_accuracyList)\n",
        "plt.xlabel(\"No Of Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(['train_accuracy', 'test_accuracy'], loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6af2f19ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "w-sRlqVDKaH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}