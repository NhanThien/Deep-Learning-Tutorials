{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq with Data pipeline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOWMm2uIwPN",
        "colab_type": "code",
        "outputId": "d0b7a999-f52c-4747-de0e-46acf08e7d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AnHhwrvJc2s",
        "colab_type": "code",
        "outputId": "c0881fd2-2883-48fc-fab4-96583f221e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Change the working directory\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Deep learning with Colab')\n",
        "\n",
        "# Set paths of train and validation data\n",
        "train_article_path = \"Data/Harvardnlp_sent_summary/train.article.txt\"\n",
        "train_title_path   = \"Data/Harvardnlp_sent_summary/train.title.txt\"\n",
        "valid_article_path = \"Data/Harvardnlp_sent_summary/valid.article.filter.txt\"\n",
        "valid_title_path   = \"Data/Harvardnlp_sent_summary/valid.title.filter.txt\"\n",
        "\n",
        "\n",
        "def getSentList(path, n_sents):\n",
        "  sentList=[]\n",
        "  with open(path) as f:\n",
        "    # using 100 to test the code only\n",
        "    for line in f.readlines()[:n_sents]:\n",
        "    #for line in f.readlines():\n",
        "      # Revove begining and ending space\n",
        "      sent=line.strip()\n",
        "\n",
        "      # Replace ## or ##.#, #,## .. by #\n",
        "      sent=re.sub(\"#(\\W)*\",\"# \",sent)   \n",
        "      \n",
        "      # Add sent to the list\n",
        "      sentList.append(sent)\n",
        "  \n",
        "  return sentList\n",
        "\n",
        "# Get train_article and train_title\n",
        "train_article=getSentList(train_article_path,100000)\n",
        "train_title=getSentList(train_title_path,100000)\n",
        "\n",
        "# Get all words in train_article and train_title\n",
        "words=[]\n",
        "# Need to modify this later\n",
        "for sent in train_article+train_title:\n",
        "  for word in word_tokenize(sent):\n",
        "    words.append(word)\n",
        "    \n",
        "# Create word2int and int2word dictionary\n",
        "word_counter = collections.Counter(words).most_common()\n",
        "\n",
        "word2int = dict()\n",
        "word2int[\"<pad>\"] = 0\n",
        "word2int[\"<unk>\"] = 1\n",
        "word2int[\"<s>\"] = 2\n",
        "word2int[\"</s>\"] = 3\n",
        "for word, _ in word_counter:\n",
        "    word2int[word] = len(word2int)\n",
        "    \n",
        "int2word = dict(zip(word2int.values(), word2int.keys()))\n",
        "\n",
        "# Get the word embedding from Glove\n",
        "glove_path = \"Data/Word Embedding/glove.6B.50d.txt\"\n",
        "word_emb_glove=dict()\n",
        "with open(glove_path) as f:\n",
        "  for line in f:\n",
        "    el=line.split()\n",
        "    word=el[0]\n",
        "    emb=[float(val) for val in el[1:]]\n",
        "    word_emb_glove[word]=emb \n",
        "    \n",
        "# Get the list of word embedding corresponding to int value in ascending order\n",
        "word_emb_list=list()\n",
        "embedding_size=len(word_emb_glove['the'])\n",
        "for i in int2word:\n",
        "  word=int2word[i]\n",
        "  # Add Glove embedding if it exists\n",
        "  if(word in word_emb_glove):\n",
        "    word_emb_list.append(word_emb_glove[word])\n",
        "  \n",
        "  # Otherwise, the value of word embedding is 0\n",
        "  else:\n",
        "    word_emb_list.append(np.zeros([embedding_size], dtype=np.float32))\n",
        "    \n",
        "# Assign random vector to <s>, </s> token\n",
        "word_emb_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "word_emb_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "# the final word embedding\n",
        "word_emb=np.array(word_emb_list)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoXcmZv8WPzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_trainDatasets(src_data,tgt_data,src_maxlen,tgt_maxlen,word2int,batch_size,num_epochs):\n",
        "  \n",
        "  # Create the Dataset\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((src_data,tgt_data))\n",
        "\n",
        "  # Split sents into words\n",
        "  train_dataset = train_dataset.map(lambda src, tgt: (tf.string_split([src]).values,tf.string_split([tgt]).values))\n",
        "\n",
        "  # Truncate src_data and tgt_data to max length\n",
        "  if src_maxlen:\n",
        "    train_dataset = train_dataset.map(lambda src, tgt: (src[:src_maxlen],tgt))\n",
        "  if tgt_maxlen:\n",
        "    train_dataset = train_dataset.map(lambda src, tgt: (src,tgt[:tgt_maxlen]))\n",
        "\n",
        "  # Convert sequence of words to sequence of int\n",
        "  vocab_list =list(word2int.keys())\n",
        "  mapping_strings_tensor=tf.constant(vocab_list)\n",
        "  vocab_table=tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings_tensor, default_value=word2int[\"<unk>\"])\n",
        "  train_dataset = train_dataset.map(lambda src, tgt: (tf.cast(vocab_table.lookup(src), tf.int32),tf.cast(vocab_table.lookup(tgt), tf.int32)))\n",
        "\n",
        "  # Create input and output for decoder\n",
        "  # Decoder input: adding \"<s>\" to the start of tgt_data\n",
        "  # Decoder output: adding \"</s>\" to the end of tgt_data\n",
        "  train_dataset = train_dataset.map(lambda src, tgt: (src,tf.concat(([word2int[\"<s>\"]],tgt), 0),tf.concat((tgt,[word2int[\"</s>\"]]), 0)))\n",
        "\n",
        "  # Adding the length of encoder's input and decoder's input\n",
        "  train_dataset = train_dataset.map(lambda src, tgt_in, tgt_out: (src,tgt_in,tgt_out,tf.size(src),tf.size(tgt_in)))\n",
        "\n",
        "  # Shuffle the dataset\n",
        "  train_dataset = train_dataset.shuffle(len(src_data))\n",
        "  \n",
        "  # Using padded_batch to create batches\n",
        "  train_dataset = train_dataset.padded_batch(batch_size,\n",
        "                                             padded_shapes=(  tf.TensorShape([None]),  # src\n",
        "                                                              tf.TensorShape([None]),  # tgt_input\n",
        "                                                              tf.TensorShape([None]),  # tgt_output\n",
        "                                                              tf.TensorShape([]),  # src_len\n",
        "                                                              tf.TensorShape([])),  # tgt_len)\n",
        "                                             padding_values=( word2int[\"<pad>\"],  # src\n",
        "                                                              word2int[\"<pad>\"],  # tgt_input\n",
        "                                                              word2int[\"<pad>\"],  # tgt_output\n",
        "                                                              0,  # src_len -- unused\n",
        "                                                              0),  # tgt_len -- unused\n",
        "                                             drop_remainder= True\n",
        "                                            )\n",
        "  \n",
        "  # Repeat the dataset with num_epochs\n",
        "  train_dataset = train_dataset.repeat(num_epochs)\n",
        "  \n",
        "  # Prefetch 10 batches \n",
        "  train_dataset = train_dataset.prefetch(10)\n",
        "  \n",
        "  return train_dataset\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3A9NdJVe3ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_testDatasets(src_data,src_maxlen,word2int,batch_size):\n",
        "  \n",
        "  # Create the Dataset\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices(src_data)\n",
        "\n",
        "  # Split sents into words\n",
        "  test_dataset = test_dataset.map(lambda src: tf.string_split([src]).values)\n",
        "\n",
        "  # Truncate src_data and tgt_data to max length\n",
        "  if src_maxlen:\n",
        "    test_dataset = test_dataset.map(lambda src: src[:src_maxlen])\n",
        "\n",
        "  # Convert sequence of words to sequence of int\n",
        "  vocab_list =list(word2int.keys())\n",
        "  mapping_strings_tensor=tf.constant(vocab_list)\n",
        "  vocab_table=tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings_tensor, default_value=word2int[\"<unk>\"])\n",
        "  test_dataset = test_dataset.map(lambda src: tf.cast(vocab_table.lookup(src), tf.int32)) \n",
        "\n",
        "  # Adding the length of encoder's input \n",
        "  test_dataset = test_dataset.map(lambda src: (src,tf.size(src)))\n",
        "\n",
        "  # Shuffle the dataset\n",
        "  test_dataset = test_dataset.shuffle(len(src_data))\n",
        "  \n",
        "  # Using padded_batch to create batches\n",
        "  test_dataset = test_dataset.padded_batch(  batch_size,\n",
        "                                             padded_shapes=(  tf.TensorShape([None]),  # src                                                   \n",
        "                                                              tf.TensorShape([])),  # src_len                                                              \n",
        "                                             padding_values=( word2int[\"<pad>\"],  # src\n",
        "                                                              0),  # src_len -- unused\n",
        "                                             drop_remainder= True\n",
        "                                            )\n",
        "    \n",
        "  # Prefetch 10 batches \n",
        "  test_dataset = test_dataset.prefetch(10)\n",
        "  \n",
        "  return test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntcdzcc-OK4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "class Seq2SeqModel(object):\n",
        "  def __init__(self,iterator, params, batch_size, word_embedding,train=True):\n",
        "    # Get the vocab size and batch-size\n",
        "    self.vocab_size=len(word_embedding)\n",
        "    self.batch_size=batch_size    \n",
        "    \n",
        "    # Get hyper-parameters from params       \n",
        "    self.num_layers=params['num_layers']\n",
        "    self.num_hiddens=params['num_hiddens']    \n",
        "    self.learning_rate = params['learning_rate']\n",
        "    self.keep_prob = params['keep_prob']\n",
        "    self.beam_width = params['beam_width']\n",
        "    \n",
        "    # Using BasicLSTMCell as a cell unit\n",
        "    self.cell=tf.nn.rnn_cell.LSTMCell  \n",
        "    self.global_step = tf.Variable(0, trainable=False) # False means not adding the variable to the graph collection \n",
        "    \n",
        "    # Get value from iterator\n",
        "    if(train):\n",
        "      encoder_input,decoder_input,decoder_output,inputSeq_len,decoder_len= iterator.get_next()\n",
        "      \n",
        "      # Decoder variable\n",
        "      self.decoder_input=decoder_input\n",
        "      self.decoder_len=decoder_len\n",
        "      self.decoder_target=decoder_output    \n",
        "\n",
        "      # The length of output sequence \n",
        "      output_len=tf.to_int32(tf.shape(decoder_output)[1])\n",
        "    else:\n",
        "      encoder_input,inputSeq_len= iterator.get_next()\n",
        "       \n",
        "    # Encoder variables     \n",
        "    self.inputSeq=encoder_input\n",
        "    self.inputSeq_len=inputSeq_len         \n",
        "    \n",
        "    # Define projection_layer\n",
        "    self.projection_layer = tf.layers.Dense(self.vocab_size, use_bias=False)\n",
        "    \n",
        "    # Define the Embedding layer\n",
        "    with tf.name_scope(\"embedding\"):\n",
        "      self.embeddings=tf.get_variable(\"embeddings\",initializer=tf.constant(word_embedding,dtype=tf.float32))\n",
        "      \n",
        "      # map the int value with its embeddings\n",
        "      input_emb=tf.nn.embedding_lookup(self.embeddings,self.inputSeq)\n",
        "      \n",
        "      # Convert from batch_size*seq_len*embedding to seq_len*batch_size*embedding to feed data with timestep      \n",
        "      # But, we need to set time_major=True during Training      \n",
        "      self.encoder_inputEmb = tf.transpose(input_emb, perm=[1, 0, 2])\n",
        "      \n",
        "      if(train):\n",
        "        decoder_input_emb=tf.nn.embedding_lookup(self.embeddings,self.decoder_input)\n",
        "        self.decoder_inputEmb = tf.transpose(decoder_input_emb, perm=[1, 0, 2])\n",
        "      \n",
        "    # Define the Encoder\n",
        "    with tf.name_scope(\"encoder\"):      \n",
        "      # Create RNN Cell for forward and backward direction\n",
        "      fw_cells=list()\n",
        "      bw_cells=list()\n",
        "      for i in range(self.num_layers):\n",
        "        fw_cell= self.cell(self.num_hiddens)\n",
        "        bw_cell= self.cell(self.num_hiddens)\n",
        "        \n",
        "        # Add Dropout\n",
        "        fw_cell=rnn.DropoutWrapper(fw_cell,output_keep_prob=self.keep_prob)\n",
        "        bw_cell=rnn.DropoutWrapper(bw_cell,output_keep_prob=self.keep_prob)\n",
        "        \n",
        "        # Add cell to the list\n",
        "        fw_cells.append(fw_cell)\n",
        "        bw_cells.append(bw_cell)\n",
        "        \n",
        "        \n",
        "      # Build a multi bi-directional model from fw_cells and bw_cells\n",
        "      outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "          cells_fw=fw_cells, cells_bw=bw_cells,inputs=self.encoder_inputEmb,time_major=True, sequence_length=self.inputSeq_len, dtype=tf.float32)\n",
        "      \n",
        "      # The ouput of Encoder (time major)\n",
        "      self.encoder_outputs=outputs\n",
        "      \n",
        "      # Use the final state of the last layer as encoder_final_state \n",
        "      encoder_state_c = tf.concat((encoder_state_fw[-1].c, encoder_state_bw[-1].c), 1)\n",
        "      encoder_state_h = tf.concat((encoder_state_fw[-1].h, encoder_state_bw[-1].h), 1)\n",
        "      self.encoder_final_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "      \n",
        "    # Define the Decoder for training\n",
        "    with tf.name_scope(\"decoder\"):\n",
        "      # Define Decoder cell\n",
        "      decoder_num_hiddens =self.num_hiddens * 2 # As we use bi-directional RNN\n",
        "      decoder_cell=self.cell(decoder_num_hiddens)\n",
        "      \n",
        "      # Training mode \n",
        "      if(train):\n",
        "        # Convert from time major to batch major \n",
        "        attention_states = tf.transpose(self.encoder_outputs, [1, 0, 2])\n",
        "        \n",
        "         # Decoder with attention      \n",
        "        attention=tf.contrib.seq2seq.BahdanauAttention(num_units=decoder_num_hiddens, memory=attention_states, memory_sequence_length=self.inputSeq_len,normalize=True)\n",
        "        attention_decoder_cell= tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell,attention_mechanism=attention,attention_layer_size=decoder_num_hiddens)\n",
        "\n",
        "        # Use the final state of encoder as the initial state of the decoder\n",
        "        decoder_initial_state = attention_decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=self.encoder_final_state )\n",
        "\n",
        "        # Use TrainingHelper to train the Model \n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=self.decoder_inputEmb,sequence_length=self.decoder_len, time_major=True)\n",
        "        decoder = tf.contrib.seq2seq.BasicDecoder(cell=attention_decoder_cell,helper=training_helper,initial_state=decoder_initial_state,output_layer=self.projection_layer)\n",
        "        logits, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True,maximum_iterations=output_len)\n",
        "        \n",
        "        \n",
        "        # Convert from time major to batch major \n",
        "        self.training_logits = tf.transpose(logits.rnn_output, perm=[1, 0, 2])\n",
        "        \n",
        "        # Adding zero to make sure training_logits has shape: [batch_size, sequence_length, num_decoder_symbols]\n",
        "        self.training_logits = tf.concat([self.training_logits, tf.zeros([self.batch_size, output_len - tf.shape(self.training_logits)[1], self.vocab_size])], axis=1)\n",
        "     \n",
        "      # Inference mode \n",
        "      else:\n",
        "        # Using Beam search\n",
        "        tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(tf.transpose(self.encoder_outputs, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "        tiled_encoder_final_state=tf.contrib.seq2seq.tile_batch(self.encoder_final_state, multiplier=self.beam_width)\n",
        "        tiled_inputSeq_len=tf.contrib.seq2seq.tile_batch(self.inputSeq_len, multiplier=self.beam_width)\n",
        "\n",
        "        # Decoder with attention with Beam search\n",
        "        attention=tf.contrib.seq2seq.BahdanauAttention(num_units=decoder_num_hiddens, memory=tiled_encoder_outputs, memory_sequence_length=tiled_inputSeq_len,normalize=True)\n",
        "        attention_decoder_cell= tf.contrib.seq2seq.AttentionWrapper(cell=decoder_cell,attention_mechanism=attention,attention_layer_size=decoder_num_hiddens)\n",
        "\n",
        "        # Use the final state of encoder as the initial state of the decoder\n",
        "        decoder_initial_state = attention_decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "\n",
        "        # Build a Decoder with Beam Search\n",
        "        beamSearch_decoder=tf.contrib.seq2seq.BeamSearchDecoder(          \n",
        "            cell=attention_decoder_cell,\n",
        "            embedding=self.embeddings,\n",
        "            start_tokens=tf.fill([self.batch_size],tf.constant(2)),\n",
        "            end_token=tf.constant(3),\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=self.beam_width,\n",
        "            output_layer=self.projection_layer  \n",
        "        )\n",
        "\n",
        "        # Perform dynamic decoding with beamSearch_decoder\n",
        "        outputs, _ , _ =tf.contrib.seq2seq.dynamic_decode(decoder=beamSearch_decoder,output_time_major=True)\n",
        "        \n",
        "        # Convert from seq_len*batch_size*beam_width to batch_size*beam_width*seq_len\n",
        "        outputs=tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "        \n",
        "        # Take the first beam (best result) as Decoder ouput \n",
        "        self.decoder_outputs=outputs[:,0,:]\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "      # Used for Training mode only \n",
        "      if(train):\n",
        "        # Caculate loss value \n",
        "        masks = tf.sequence_mask(lengths=self.decoder_len,maxlen=output_len, dtype=tf.float32)         \n",
        "        self.loss = tf.contrib.seq2seq.sequence_loss(logits=self.training_logits,targets=self.decoder_target,weights=masks)\n",
        "\n",
        "        # Using AdamOptimizer\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        # Compute gradient \n",
        "        gradients = optimizer.compute_gradients(self.loss)\n",
        "        # Apply Gradient Clipping \n",
        "        gradients_clipping = [(tf.clip_by_value(grad, clip_value_min=-5., clip_value_max=5.), var) for grad, var in gradients if grad is not None]\n",
        "\n",
        "        # Apply gradients to variables\n",
        "        self.train_update = optimizer.apply_gradients(gradients_clipping, global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivWcpXVYG1Hc",
        "colab_type": "text"
      },
      "source": [
        "### **Traing the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4eQmk41Dr5E",
        "colab_type": "code",
        "outputId": "5b1b5c5a-ec0e-4a6c-b7e5-936336900d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266478
        }
      },
      "source": [
        "# Define hyper-parameters for the Model\n",
        "params=dict()\n",
        "params['num_layers']=2\n",
        "params['num_hiddens']=150\n",
        "params['learning_rate']=0.001\n",
        "params['keep_prob']=0.85\n",
        "params['beam_width']=10\n",
        "\n",
        "# Define the max length of article and title\n",
        "article_max_len = 45\n",
        "title_max_len = 15\n",
        "\n",
        "num_epochs=10\n",
        "batch_size=64\n",
        "tf.reset_default_graph() \n",
        "\n",
        "train_dataset=get_trainDatasets(train_article,train_title,article_max_len,title_max_len,word2int,batch_size,num_epochs)\n",
        "# Create an Initializable iterator\n",
        "iterator = train_dataset.make_initializable_iterator()\n",
        "\n",
        "model=Seq2SeqModel(iterator, params, batch_size, word_emb)\n",
        "\n",
        "num_batches_epoch = len(train_article)//batch_size\n",
        "early_stop=5 # Stop if there is no improvement after 5 epochs\n",
        "\n",
        "# Set paths to save the model\n",
        "checkpoint = \"Saved Models/Text Summarization/Using Datasets/TSModel.ckpt\"\n",
        "import time\n",
        "start_time=time.time()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  # Run initializer\n",
        "  sess.run(tf.tables_initializer())\n",
        "  sess.run(iterator.initializer)  \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  min_loss=1000 # To find the minimum loss during training   \n",
        "  no_impove_count=0 # Count the number of consecutive epoch having no improvement\n",
        "    \n",
        "  for epoch in range (num_epochs):  \n",
        "    # Reset epoch_loss after each epoch \n",
        "    epoch_loss=0\n",
        "    for batch_i in range(num_batches_epoch):\n",
        "      #print(sess.run(model.outputLength))\n",
        "      # Start training the model\n",
        "      _, step, loss,encoder_outputs = sess.run([model.train_update, model.global_step, model.loss,model.encoder_outputs])\n",
        "      epoch_loss+=loss\n",
        "      \n",
        "      # Display loss value of each step\n",
        "      print(\"step {0}: loss = {1}\".format(step, loss))      \n",
        "   \n",
        "    print(\"Finish epoch\",epoch+1 )\n",
        "    # Averaging the epoch_loss\n",
        "    epoch_loss=epoch_loss/(batch_i+1)\n",
        "    \n",
        "    # Save the model if the epoch_loss is at a new minimum,\n",
        "    if epoch_loss <= min_loss:\n",
        "      # Set new minimum loss\n",
        "      min_loss=epoch_loss\n",
        "      # Reset the no_impove_count\n",
        "      no_impove_count=0 \n",
        "      \n",
        "      # Save the new model\n",
        "      saver = tf.train.Saver(tf.global_variables()) \n",
        "      saver.save(sess, checkpoint)\n",
        "      \n",
        "      print('New model saved, minimum loss:',min_loss,'\\n') \n",
        "      \n",
        "    # Early stopping\n",
        "    else:\n",
        "      print(\"No Improvement!\",'\\n')\n",
        "      no_impove_count+=1\n",
        "      if(no_impove_count==early_stop):\n",
        "        print(\"Early stopping... Finish training\")\n",
        "        break\n",
        "\n",
        "end_time=time.time()\n",
        "training_time=(end_time-start_time)/60\n",
        "print(\"\\nTraining time (mins): \",training_time )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-4-c57637e5c5c4>:31: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-c57637e5c5c4>:63: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "step 1: loss = 10.828153610229492\n",
            "step 2: loss = 10.78711223602295\n",
            "step 3: loss = 10.670001983642578\n",
            "step 4: loss = 10.364364624023438\n",
            "step 5: loss = 9.87707233428955\n",
            "step 6: loss = 9.137256622314453\n",
            "step 7: loss = 8.413293838500977\n",
            "step 8: loss = 8.200343132019043\n",
            "step 9: loss = 8.542448997497559\n",
            "step 10: loss = 8.741886138916016\n",
            "step 11: loss = 8.175909996032715\n",
            "step 12: loss = 8.019152641296387\n",
            "step 13: loss = 8.037979125976562\n",
            "step 14: loss = 7.9365105628967285\n",
            "step 15: loss = 8.029086112976074\n",
            "step 16: loss = 7.952972412109375\n",
            "step 17: loss = 7.9726176261901855\n",
            "step 18: loss = 7.936006546020508\n",
            "step 19: loss = 7.820207595825195\n",
            "step 20: loss = 7.819289207458496\n",
            "step 21: loss = 7.823500156402588\n",
            "step 22: loss = 7.6482977867126465\n",
            "step 23: loss = 7.807972431182861\n",
            "step 24: loss = 7.588498115539551\n",
            "step 25: loss = 7.343892574310303\n",
            "step 26: loss = 7.633932590484619\n",
            "step 27: loss = 7.306135177612305\n",
            "step 28: loss = 7.5589118003845215\n",
            "step 29: loss = 7.648594379425049\n",
            "step 30: loss = 7.551044940948486\n",
            "step 31: loss = 7.670339107513428\n",
            "step 32: loss = 7.370504856109619\n",
            "step 33: loss = 7.538900375366211\n",
            "step 34: loss = 7.427168369293213\n",
            "step 35: loss = 7.43320369720459\n",
            "step 36: loss = 7.367464542388916\n",
            "step 37: loss = 7.4232707023620605\n",
            "step 38: loss = 7.512678623199463\n",
            "step 39: loss = 7.263216495513916\n",
            "step 40: loss = 7.30046272277832\n",
            "step 41: loss = 7.338757038116455\n",
            "step 42: loss = 7.335381031036377\n",
            "step 43: loss = 7.3699164390563965\n",
            "step 44: loss = 7.351626396179199\n",
            "step 45: loss = 7.190844535827637\n",
            "step 46: loss = 7.441862106323242\n",
            "step 47: loss = 7.250638484954834\n",
            "step 48: loss = 7.394228458404541\n",
            "step 49: loss = 7.251418113708496\n",
            "step 50: loss = 7.276036262512207\n",
            "step 51: loss = 7.390485763549805\n",
            "step 52: loss = 7.306144714355469\n",
            "step 53: loss = 7.239382743835449\n",
            "step 54: loss = 7.3458452224731445\n",
            "step 55: loss = 7.320145606994629\n",
            "step 56: loss = 7.247364521026611\n",
            "step 57: loss = 7.488752365112305\n",
            "step 58: loss = 7.382269859313965\n",
            "step 59: loss = 7.1309614181518555\n",
            "step 60: loss = 7.2489519119262695\n",
            "step 61: loss = 7.165346622467041\n",
            "step 62: loss = 7.102282524108887\n",
            "step 63: loss = 7.249270439147949\n",
            "step 64: loss = 7.528170108795166\n",
            "step 65: loss = 7.2893524169921875\n",
            "step 66: loss = 7.21506404876709\n",
            "step 67: loss = 7.182989597320557\n",
            "step 68: loss = 7.33731746673584\n",
            "step 69: loss = 7.2266645431518555\n",
            "step 70: loss = 6.959499835968018\n",
            "step 71: loss = 7.226359844207764\n",
            "step 72: loss = 7.131712436676025\n",
            "step 73: loss = 7.3096923828125\n",
            "step 74: loss = 7.165069103240967\n",
            "step 75: loss = 7.233547687530518\n",
            "step 76: loss = 7.2726030349731445\n",
            "step 77: loss = 6.961171627044678\n",
            "step 78: loss = 7.107442855834961\n",
            "step 79: loss = 7.095484733581543\n",
            "step 80: loss = 7.1285481452941895\n",
            "step 81: loss = 7.327387809753418\n",
            "step 82: loss = 7.061086654663086\n",
            "step 83: loss = 6.940051555633545\n",
            "step 84: loss = 7.108299255371094\n",
            "step 85: loss = 6.889686584472656\n",
            "step 86: loss = 7.0061821937561035\n",
            "step 87: loss = 7.329067707061768\n",
            "step 88: loss = 7.131777286529541\n",
            "step 89: loss = 7.192180633544922\n",
            "step 90: loss = 6.997020721435547\n",
            "step 91: loss = 6.9771728515625\n",
            "step 92: loss = 7.189606189727783\n",
            "step 93: loss = 6.9851484298706055\n",
            "step 94: loss = 7.174161434173584\n",
            "step 95: loss = 7.008616924285889\n",
            "step 96: loss = 7.039741516113281\n",
            "step 97: loss = 7.306768417358398\n",
            "step 98: loss = 7.215236186981201\n",
            "step 99: loss = 7.043611526489258\n",
            "step 100: loss = 6.892566204071045\n",
            "step 101: loss = 7.108737468719482\n",
            "step 102: loss = 6.931558609008789\n",
            "step 103: loss = 7.019952297210693\n",
            "step 104: loss = 7.078960418701172\n",
            "step 105: loss = 7.089266777038574\n",
            "step 106: loss = 6.960063934326172\n",
            "step 107: loss = 7.153469562530518\n",
            "step 108: loss = 7.193032264709473\n",
            "step 109: loss = 6.794655799865723\n",
            "step 110: loss = 6.952682971954346\n",
            "step 111: loss = 6.719447135925293\n",
            "step 112: loss = 7.049509048461914\n",
            "step 113: loss = 6.87589693069458\n",
            "step 114: loss = 6.96609354019165\n",
            "step 115: loss = 7.0973381996154785\n",
            "step 116: loss = 7.0383830070495605\n",
            "step 117: loss = 7.0256805419921875\n",
            "step 118: loss = 6.811886310577393\n",
            "step 119: loss = 6.920797348022461\n",
            "step 120: loss = 7.105753421783447\n",
            "step 121: loss = 7.044586181640625\n",
            "step 122: loss = 7.095329761505127\n",
            "step 123: loss = 7.012935161590576\n",
            "step 124: loss = 6.974984645843506\n",
            "step 125: loss = 6.895604610443115\n",
            "step 126: loss = 6.961042404174805\n",
            "step 127: loss = 6.919922351837158\n",
            "step 128: loss = 6.966436862945557\n",
            "step 129: loss = 6.967036724090576\n",
            "step 130: loss = 6.958012580871582\n",
            "step 131: loss = 7.171712398529053\n",
            "step 132: loss = 7.061833381652832\n",
            "step 133: loss = 6.947738170623779\n",
            "step 134: loss = 7.00030517578125\n",
            "step 135: loss = 6.965098857879639\n",
            "step 136: loss = 6.915998458862305\n",
            "step 137: loss = 6.836596488952637\n",
            "step 138: loss = 6.921365261077881\n",
            "step 139: loss = 7.11110258102417\n",
            "step 140: loss = 6.912057876586914\n",
            "step 141: loss = 6.841256618499756\n",
            "step 142: loss = 6.676412582397461\n",
            "step 143: loss = 6.745909214019775\n",
            "step 144: loss = 6.85274076461792\n",
            "step 145: loss = 6.878859519958496\n",
            "step 146: loss = 7.048635482788086\n",
            "step 147: loss = 6.705186367034912\n",
            "step 148: loss = 6.961763858795166\n",
            "step 149: loss = 6.774046897888184\n",
            "step 150: loss = 6.7993550300598145\n",
            "step 151: loss = 6.861849784851074\n",
            "step 152: loss = 7.2127227783203125\n",
            "step 153: loss = 6.824862480163574\n",
            "step 154: loss = 6.927088260650635\n",
            "step 155: loss = 6.841157913208008\n",
            "step 156: loss = 6.829176902770996\n",
            "step 157: loss = 6.882823467254639\n",
            "step 158: loss = 6.845002174377441\n",
            "step 159: loss = 6.806424140930176\n",
            "step 160: loss = 6.688374996185303\n",
            "step 161: loss = 6.806785583496094\n",
            "step 162: loss = 6.913647174835205\n",
            "step 163: loss = 6.68740177154541\n",
            "step 164: loss = 6.9426445960998535\n",
            "step 165: loss = 6.899900913238525\n",
            "step 166: loss = 7.023783206939697\n",
            "step 167: loss = 6.7665934562683105\n",
            "step 168: loss = 6.9318718910217285\n",
            "step 169: loss = 6.7085747718811035\n",
            "step 170: loss = 6.923356056213379\n",
            "step 171: loss = 6.9209370613098145\n",
            "step 172: loss = 6.787299633026123\n",
            "step 173: loss = 6.718020915985107\n",
            "step 174: loss = 6.7394819259643555\n",
            "step 175: loss = 6.8767924308776855\n",
            "step 176: loss = 6.6962761878967285\n",
            "step 177: loss = 6.980846881866455\n",
            "step 178: loss = 6.796555519104004\n",
            "step 179: loss = 6.630386829376221\n",
            "step 180: loss = 6.54197883605957\n",
            "step 181: loss = 6.987338066101074\n",
            "step 182: loss = 6.830801963806152\n",
            "step 183: loss = 6.75890588760376\n",
            "step 184: loss = 7.010867595672607\n",
            "step 185: loss = 6.783276081085205\n",
            "step 186: loss = 6.8737406730651855\n",
            "step 187: loss = 6.820589065551758\n",
            "step 188: loss = 6.831010341644287\n",
            "step 189: loss = 7.012844085693359\n",
            "step 190: loss = 6.956686973571777\n",
            "step 191: loss = 6.79510498046875\n",
            "step 192: loss = 6.8496503829956055\n",
            "step 193: loss = 6.45751953125\n",
            "step 194: loss = 6.889145851135254\n",
            "step 195: loss = 6.726823806762695\n",
            "step 196: loss = 6.7359700202941895\n",
            "step 197: loss = 6.9345622062683105\n",
            "step 198: loss = 6.737030029296875\n",
            "step 199: loss = 6.447507381439209\n",
            "step 200: loss = 6.580327987670898\n",
            "step 201: loss = 6.651482582092285\n",
            "step 202: loss = 6.445025444030762\n",
            "step 203: loss = 6.639576435089111\n",
            "step 204: loss = 6.841753005981445\n",
            "step 205: loss = 6.793783664703369\n",
            "step 206: loss = 6.613773345947266\n",
            "step 207: loss = 6.841836452484131\n",
            "step 208: loss = 6.666950702667236\n",
            "step 209: loss = 6.769906997680664\n",
            "step 210: loss = 6.921844482421875\n",
            "step 211: loss = 7.043205261230469\n",
            "step 212: loss = 6.562514305114746\n",
            "step 213: loss = 6.65849494934082\n",
            "step 214: loss = 6.548184394836426\n",
            "step 215: loss = 6.746029853820801\n",
            "step 216: loss = 6.613032817840576\n",
            "step 217: loss = 6.3832173347473145\n",
            "step 218: loss = 6.901705265045166\n",
            "step 219: loss = 6.442650318145752\n",
            "step 220: loss = 6.611641883850098\n",
            "step 221: loss = 6.59556770324707\n",
            "step 222: loss = 6.745930194854736\n",
            "step 223: loss = 6.690807342529297\n",
            "step 224: loss = 6.4939117431640625\n",
            "step 225: loss = 6.402010917663574\n",
            "step 226: loss = 6.6392292976379395\n",
            "step 227: loss = 6.562596797943115\n",
            "step 228: loss = 6.5172648429870605\n",
            "step 229: loss = 6.730671405792236\n",
            "step 230: loss = 6.552038669586182\n",
            "step 231: loss = 6.755578994750977\n",
            "step 232: loss = 6.664841651916504\n",
            "step 233: loss = 6.598180294036865\n",
            "step 234: loss = 6.390834808349609\n",
            "step 235: loss = 6.728118419647217\n",
            "step 236: loss = 6.317875385284424\n",
            "step 237: loss = 6.907752513885498\n",
            "step 238: loss = 6.678214073181152\n",
            "step 239: loss = 6.7852888107299805\n",
            "step 240: loss = 6.5950093269348145\n",
            "step 241: loss = 6.484137058258057\n",
            "step 242: loss = 6.409607887268066\n",
            "step 243: loss = 6.5226969718933105\n",
            "step 244: loss = 6.553229808807373\n",
            "step 245: loss = 6.539243698120117\n",
            "step 246: loss = 6.756593704223633\n",
            "step 247: loss = 6.406777381896973\n",
            "step 248: loss = 6.530220031738281\n",
            "step 249: loss = 6.6356706619262695\n",
            "step 250: loss = 6.36210298538208\n",
            "step 251: loss = 6.37652587890625\n",
            "step 252: loss = 6.32568359375\n",
            "step 253: loss = 6.703062534332275\n",
            "step 254: loss = 6.485950946807861\n",
            "step 255: loss = 6.918105125427246\n",
            "step 256: loss = 6.7298078536987305\n",
            "step 257: loss = 6.3384928703308105\n",
            "step 258: loss = 6.443045616149902\n",
            "step 259: loss = 6.677590847015381\n",
            "step 260: loss = 6.380059719085693\n",
            "step 261: loss = 6.493092060089111\n",
            "step 262: loss = 6.4398884773254395\n",
            "step 263: loss = 6.2713398933410645\n",
            "step 264: loss = 6.398609638214111\n",
            "step 265: loss = 6.436881065368652\n",
            "step 266: loss = 6.679135322570801\n",
            "step 267: loss = 6.530962944030762\n",
            "step 268: loss = 6.267240047454834\n",
            "step 269: loss = 6.431620121002197\n",
            "step 270: loss = 6.440873622894287\n",
            "step 271: loss = 6.469119548797607\n",
            "step 272: loss = 6.282782554626465\n",
            "step 273: loss = 6.2931742668151855\n",
            "step 274: loss = 6.354631423950195\n",
            "step 275: loss = 6.711333751678467\n",
            "step 276: loss = 6.409912109375\n",
            "step 277: loss = 6.357571125030518\n",
            "step 278: loss = 6.444675922393799\n",
            "step 279: loss = 6.4020304679870605\n",
            "step 280: loss = 6.405142784118652\n",
            "step 281: loss = 6.5493388175964355\n",
            "step 282: loss = 6.287452220916748\n",
            "step 283: loss = 6.6643829345703125\n",
            "step 284: loss = 6.482618808746338\n",
            "step 285: loss = 6.376169681549072\n",
            "step 286: loss = 6.423402786254883\n",
            "step 287: loss = 6.578053951263428\n",
            "step 288: loss = 6.386443138122559\n",
            "step 289: loss = 6.518970966339111\n",
            "step 290: loss = 6.438558578491211\n",
            "step 291: loss = 6.115289688110352\n",
            "step 292: loss = 6.083226203918457\n",
            "step 293: loss = 6.365502834320068\n",
            "step 294: loss = 6.330231666564941\n",
            "step 295: loss = 6.362748146057129\n",
            "step 296: loss = 6.464994430541992\n",
            "step 297: loss = 6.430450916290283\n",
            "step 298: loss = 6.372416973114014\n",
            "step 299: loss = 6.386704444885254\n",
            "step 300: loss = 6.252824783325195\n",
            "step 301: loss = 6.149713516235352\n",
            "step 302: loss = 6.3926544189453125\n",
            "step 303: loss = 6.438570976257324\n",
            "step 304: loss = 6.4194512367248535\n",
            "step 305: loss = 6.2797675132751465\n",
            "step 306: loss = 6.36863374710083\n",
            "step 307: loss = 6.707667350769043\n",
            "step 308: loss = 6.186532974243164\n",
            "step 309: loss = 6.427037715911865\n",
            "step 310: loss = 6.451976299285889\n",
            "step 311: loss = 6.369653224945068\n",
            "step 312: loss = 6.22343635559082\n",
            "step 313: loss = 6.007763385772705\n",
            "step 314: loss = 6.19091796875\n",
            "step 315: loss = 6.2549357414245605\n",
            "step 316: loss = 6.259761333465576\n",
            "step 317: loss = 6.522510051727295\n",
            "step 318: loss = 6.12081241607666\n",
            "step 319: loss = 6.394604682922363\n",
            "step 320: loss = 6.2550435066223145\n",
            "step 321: loss = 6.145539283752441\n",
            "step 322: loss = 6.398738384246826\n",
            "step 323: loss = 6.190741539001465\n",
            "step 324: loss = 5.850002288818359\n",
            "step 325: loss = 6.334041595458984\n",
            "step 326: loss = 6.383288860321045\n",
            "step 327: loss = 6.316421031951904\n",
            "step 328: loss = 6.208580493927002\n",
            "step 329: loss = 6.119156837463379\n",
            "step 330: loss = 6.571907997131348\n",
            "step 331: loss = 6.179287433624268\n",
            "step 332: loss = 6.20024299621582\n",
            "step 333: loss = 6.2470502853393555\n",
            "step 334: loss = 6.190589904785156\n",
            "step 335: loss = 6.204475402832031\n",
            "step 336: loss = 6.169344902038574\n",
            "step 337: loss = 6.142999172210693\n",
            "step 338: loss = 6.41960334777832\n",
            "step 339: loss = 6.155673980712891\n",
            "step 340: loss = 6.395914077758789\n",
            "step 341: loss = 6.292206287384033\n",
            "step 342: loss = 6.437770366668701\n",
            "step 343: loss = 6.271363735198975\n",
            "step 344: loss = 6.2702507972717285\n",
            "step 345: loss = 6.505859375\n",
            "step 346: loss = 6.282942771911621\n",
            "step 347: loss = 6.3586249351501465\n",
            "step 348: loss = 6.172020435333252\n",
            "step 349: loss = 6.404569625854492\n",
            "step 350: loss = 6.270623207092285\n",
            "step 351: loss = 6.112722873687744\n",
            "step 352: loss = 5.904819965362549\n",
            "step 353: loss = 6.118910312652588\n",
            "step 354: loss = 6.151282787322998\n",
            "step 355: loss = 6.283835411071777\n",
            "step 356: loss = 6.3060784339904785\n",
            "step 357: loss = 6.154268741607666\n",
            "step 358: loss = 6.174749374389648\n",
            "step 359: loss = 6.19361686706543\n",
            "step 360: loss = 6.256218910217285\n",
            "step 361: loss = 6.336737632751465\n",
            "step 362: loss = 6.130523681640625\n",
            "step 363: loss = 6.301920413970947\n",
            "step 364: loss = 6.340959548950195\n",
            "step 365: loss = 6.272770404815674\n",
            "step 366: loss = 6.548728942871094\n",
            "step 367: loss = 6.338000297546387\n",
            "step 368: loss = 6.218672752380371\n",
            "step 369: loss = 6.578044891357422\n",
            "step 370: loss = 6.26817512512207\n",
            "step 371: loss = 6.305247783660889\n",
            "step 372: loss = 6.195539474487305\n",
            "step 373: loss = 6.025211811065674\n",
            "step 374: loss = 6.157451629638672\n",
            "step 375: loss = 6.123507022857666\n",
            "step 376: loss = 6.2151947021484375\n",
            "step 377: loss = 6.197775840759277\n",
            "step 378: loss = 6.009867191314697\n",
            "step 379: loss = 6.032355308532715\n",
            "step 380: loss = 5.986741065979004\n",
            "step 381: loss = 5.756989479064941\n",
            "step 382: loss = 6.237088203430176\n",
            "step 383: loss = 6.3427228927612305\n",
            "step 384: loss = 6.11729621887207\n",
            "step 385: loss = 6.036216735839844\n",
            "step 386: loss = 6.022358417510986\n",
            "step 387: loss = 6.086396217346191\n",
            "step 388: loss = 6.202513694763184\n",
            "step 389: loss = 6.184687614440918\n",
            "step 390: loss = 5.988086700439453\n",
            "step 391: loss = 5.9586567878723145\n",
            "step 392: loss = 6.20346736907959\n",
            "step 393: loss = 6.289450645446777\n",
            "step 394: loss = 6.276171684265137\n",
            "step 395: loss = 6.303493976593018\n",
            "step 396: loss = 6.267655849456787\n",
            "step 397: loss = 5.940786838531494\n",
            "step 398: loss = 6.089719295501709\n",
            "step 399: loss = 6.1169891357421875\n",
            "step 400: loss = 5.979740619659424\n",
            "step 401: loss = 6.347586154937744\n",
            "step 402: loss = 6.025540351867676\n",
            "step 403: loss = 6.1688127517700195\n",
            "step 404: loss = 6.239965915679932\n",
            "step 405: loss = 6.211352825164795\n",
            "step 406: loss = 6.153824806213379\n",
            "step 407: loss = 5.97215461730957\n",
            "step 408: loss = 6.292802810668945\n",
            "step 409: loss = 6.378586769104004\n",
            "step 410: loss = 6.270484924316406\n",
            "step 411: loss = 6.099644184112549\n",
            "step 412: loss = 6.396052360534668\n",
            "step 413: loss = 6.3107404708862305\n",
            "step 414: loss = 6.0586676597595215\n",
            "step 415: loss = 6.083515644073486\n",
            "step 416: loss = 6.0248703956604\n",
            "step 417: loss = 6.072668552398682\n",
            "step 418: loss = 6.104578018188477\n",
            "step 419: loss = 5.867288589477539\n",
            "step 420: loss = 6.0018696784973145\n",
            "step 421: loss = 6.1681623458862305\n",
            "step 422: loss = 5.962236404418945\n",
            "step 423: loss = 5.93137788772583\n",
            "step 424: loss = 6.037004470825195\n",
            "step 425: loss = 6.208175182342529\n",
            "step 426: loss = 5.921225070953369\n",
            "step 427: loss = 6.141057014465332\n",
            "step 428: loss = 5.849681377410889\n",
            "step 429: loss = 6.084828853607178\n",
            "step 430: loss = 5.875955581665039\n",
            "step 431: loss = 6.0646257400512695\n",
            "step 432: loss = 5.928380489349365\n",
            "step 433: loss = 6.223513603210449\n",
            "step 434: loss = 5.98564338684082\n",
            "step 435: loss = 6.024199962615967\n",
            "step 436: loss = 5.968886375427246\n",
            "step 437: loss = 5.970431804656982\n",
            "step 438: loss = 5.923305511474609\n",
            "step 439: loss = 5.998212814331055\n",
            "step 440: loss = 6.114551067352295\n",
            "step 441: loss = 5.971322059631348\n",
            "step 442: loss = 5.983851432800293\n",
            "step 443: loss = 6.077223300933838\n",
            "step 444: loss = 5.918766021728516\n",
            "step 445: loss = 6.0894622802734375\n",
            "step 446: loss = 5.985001087188721\n",
            "step 447: loss = 5.896087169647217\n",
            "step 448: loss = 5.836374282836914\n",
            "step 449: loss = 5.918384552001953\n",
            "step 450: loss = 6.135479927062988\n",
            "step 451: loss = 6.245200157165527\n",
            "step 452: loss = 6.203103065490723\n",
            "step 453: loss = 5.920571804046631\n",
            "step 454: loss = 5.935660362243652\n",
            "step 455: loss = 5.837012767791748\n",
            "step 456: loss = 6.130386829376221\n",
            "step 457: loss = 6.174815654754639\n",
            "step 458: loss = 6.191275596618652\n",
            "step 459: loss = 6.100262641906738\n",
            "step 460: loss = 5.990455150604248\n",
            "step 461: loss = 6.018048286437988\n",
            "step 462: loss = 6.02394437789917\n",
            "step 463: loss = 6.14882230758667\n",
            "step 464: loss = 5.9487175941467285\n",
            "step 465: loss = 6.074452877044678\n",
            "step 466: loss = 5.858949184417725\n",
            "step 467: loss = 5.809910774230957\n",
            "step 468: loss = 5.869091033935547\n",
            "step 469: loss = 5.824403762817383\n",
            "step 470: loss = 5.903303623199463\n",
            "step 471: loss = 6.332224369049072\n",
            "step 472: loss = 6.127051830291748\n",
            "step 473: loss = 6.061561107635498\n",
            "step 474: loss = 5.82732629776001\n",
            "step 475: loss = 5.962501049041748\n",
            "step 476: loss = 5.738874435424805\n",
            "step 477: loss = 5.9279327392578125\n",
            "step 478: loss = 5.877211570739746\n",
            "step 479: loss = 5.7559332847595215\n",
            "step 480: loss = 6.103663444519043\n",
            "step 481: loss = 5.985581874847412\n",
            "step 482: loss = 6.046781063079834\n",
            "step 483: loss = 6.08010196685791\n",
            "step 484: loss = 5.777328014373779\n",
            "step 485: loss = 5.926803112030029\n",
            "step 486: loss = 6.296402454376221\n",
            "step 487: loss = 5.699973106384277\n",
            "step 488: loss = 6.1982831954956055\n",
            "step 489: loss = 5.756042957305908\n",
            "step 490: loss = 6.031440258026123\n",
            "step 491: loss = 5.582140922546387\n",
            "step 492: loss = 5.976618766784668\n",
            "step 493: loss = 6.261255741119385\n",
            "step 494: loss = 6.092146396636963\n",
            "step 495: loss = 5.803001880645752\n",
            "step 496: loss = 5.778371810913086\n",
            "step 497: loss = 6.071244716644287\n",
            "step 498: loss = 6.032686710357666\n",
            "step 499: loss = 5.75771427154541\n",
            "step 500: loss = 5.92666482925415\n",
            "step 501: loss = 5.9838762283325195\n",
            "step 502: loss = 6.178380966186523\n",
            "step 503: loss = 5.708417892456055\n",
            "step 504: loss = 5.726812839508057\n",
            "step 505: loss = 6.03138542175293\n",
            "step 506: loss = 6.0355000495910645\n",
            "step 507: loss = 5.757626533508301\n",
            "step 508: loss = 5.508313179016113\n",
            "step 509: loss = 6.045953273773193\n",
            "step 510: loss = 6.016850471496582\n",
            "step 511: loss = 6.086573600769043\n",
            "step 512: loss = 5.886190414428711\n",
            "step 513: loss = 5.795604228973389\n",
            "step 514: loss = 6.034170150756836\n",
            "step 515: loss = 5.7745490074157715\n",
            "step 516: loss = 5.919428825378418\n",
            "step 517: loss = 5.870765209197998\n",
            "step 518: loss = 5.711509704589844\n",
            "step 519: loss = 5.819523334503174\n",
            "step 520: loss = 5.694156646728516\n",
            "step 521: loss = 5.91785192489624\n",
            "step 522: loss = 5.75881290435791\n",
            "step 523: loss = 5.785224914550781\n",
            "step 524: loss = 5.872631072998047\n",
            "step 525: loss = 5.297562599182129\n",
            "step 526: loss = 5.799184799194336\n",
            "step 527: loss = 6.227002143859863\n",
            "step 528: loss = 5.981512069702148\n",
            "step 529: loss = 5.925906658172607\n",
            "step 530: loss = 5.592353343963623\n",
            "step 531: loss = 5.765430927276611\n",
            "step 532: loss = 5.98005485534668\n",
            "step 533: loss = 5.921158790588379\n",
            "step 534: loss = 5.550576686859131\n",
            "step 535: loss = 5.894347667694092\n",
            "step 536: loss = 5.566904067993164\n",
            "step 537: loss = 6.005442142486572\n",
            "step 538: loss = 5.907961368560791\n",
            "step 539: loss = 5.771441459655762\n",
            "step 540: loss = 5.851869106292725\n",
            "step 541: loss = 6.003564357757568\n",
            "step 542: loss = 5.805318832397461\n",
            "step 543: loss = 6.023839473724365\n",
            "step 544: loss = 6.04295539855957\n",
            "step 545: loss = 5.620963096618652\n",
            "step 546: loss = 5.759033679962158\n",
            "step 547: loss = 5.9668755531311035\n",
            "step 548: loss = 5.933111667633057\n",
            "step 549: loss = 5.493700981140137\n",
            "step 550: loss = 5.665584087371826\n",
            "step 551: loss = 5.891348838806152\n",
            "step 552: loss = 5.654289245605469\n",
            "step 553: loss = 5.841489791870117\n",
            "step 554: loss = 6.029173851013184\n",
            "step 555: loss = 5.555702209472656\n",
            "step 556: loss = 5.447666168212891\n",
            "step 557: loss = 5.919987678527832\n",
            "step 558: loss = 5.84665584564209\n",
            "step 559: loss = 5.703665733337402\n",
            "step 560: loss = 5.55656623840332\n",
            "step 561: loss = 5.83626651763916\n",
            "step 562: loss = 5.762465000152588\n",
            "step 563: loss = 5.9660749435424805\n",
            "step 564: loss = 5.602842807769775\n",
            "step 565: loss = 5.737915515899658\n",
            "step 566: loss = 5.753628253936768\n",
            "step 567: loss = 5.781188011169434\n",
            "step 568: loss = 5.564011096954346\n",
            "step 569: loss = 5.890110492706299\n",
            "step 570: loss = 5.746952056884766\n",
            "step 571: loss = 5.819123268127441\n",
            "step 572: loss = 5.963550090789795\n",
            "step 573: loss = 5.766027450561523\n",
            "step 574: loss = 5.906872272491455\n",
            "step 575: loss = 5.970373630523682\n",
            "step 576: loss = 5.753533363342285\n",
            "step 577: loss = 5.459031105041504\n",
            "step 578: loss = 5.663984775543213\n",
            "step 579: loss = 5.850051403045654\n",
            "step 580: loss = 6.0378289222717285\n",
            "step 581: loss = 5.735694885253906\n",
            "step 582: loss = 5.665769100189209\n",
            "step 583: loss = 5.978606700897217\n",
            "step 584: loss = 5.408056259155273\n",
            "step 585: loss = 5.695510387420654\n",
            "step 586: loss = 5.6449127197265625\n",
            "step 587: loss = 5.745485305786133\n",
            "step 588: loss = 5.774931907653809\n",
            "step 589: loss = 6.404445171356201\n",
            "step 590: loss = 5.968775749206543\n",
            "step 591: loss = 5.969566345214844\n",
            "step 592: loss = 5.781499862670898\n",
            "step 593: loss = 5.675711631774902\n",
            "step 594: loss = 5.800231456756592\n",
            "step 595: loss = 5.896693706512451\n",
            "step 596: loss = 5.937241554260254\n",
            "step 597: loss = 5.609168529510498\n",
            "step 598: loss = 5.964942932128906\n",
            "step 599: loss = 5.658759117126465\n",
            "step 600: loss = 5.808920383453369\n",
            "step 601: loss = 5.402246475219727\n",
            "step 602: loss = 5.7282023429870605\n",
            "step 603: loss = 5.698752403259277\n",
            "step 604: loss = 5.738160610198975\n",
            "step 605: loss = 5.556676864624023\n",
            "step 606: loss = 5.680215358734131\n",
            "step 607: loss = 5.88177490234375\n",
            "step 608: loss = 5.645681381225586\n",
            "step 609: loss = 5.696189880371094\n",
            "step 610: loss = 5.717361927032471\n",
            "step 611: loss = 5.735912799835205\n",
            "step 612: loss = 5.7291364669799805\n",
            "step 613: loss = 5.3112945556640625\n",
            "step 614: loss = 5.977206230163574\n",
            "step 615: loss = 5.893470764160156\n",
            "step 616: loss = 6.029101371765137\n",
            "step 617: loss = 5.854428291320801\n",
            "step 618: loss = 5.5793633460998535\n",
            "step 619: loss = 5.9689412117004395\n",
            "step 620: loss = 5.5826239585876465\n",
            "step 621: loss = 5.745528697967529\n",
            "step 622: loss = 5.633016586303711\n",
            "step 623: loss = 5.574548244476318\n",
            "step 624: loss = 5.9318671226501465\n",
            "step 625: loss = 5.912213325500488\n",
            "step 626: loss = 5.550405979156494\n",
            "step 627: loss = 5.563884735107422\n",
            "step 628: loss = 5.655433177947998\n",
            "step 629: loss = 5.978018760681152\n",
            "step 630: loss = 5.792500019073486\n",
            "step 631: loss = 5.785589694976807\n",
            "step 632: loss = 5.8409624099731445\n",
            "step 633: loss = 5.44676399230957\n",
            "step 634: loss = 5.730956554412842\n",
            "step 635: loss = 5.685820579528809\n",
            "step 636: loss = 5.731517791748047\n",
            "step 637: loss = 5.504103660583496\n",
            "step 638: loss = 5.616555213928223\n",
            "step 639: loss = 5.840632915496826\n",
            "step 640: loss = 6.040891647338867\n",
            "step 641: loss = 5.377368450164795\n",
            "step 642: loss = 5.8527045249938965\n",
            "step 643: loss = 5.655388832092285\n",
            "step 644: loss = 5.625066757202148\n",
            "step 645: loss = 5.525728225708008\n",
            "step 646: loss = 5.9659104347229\n",
            "step 647: loss = 5.77967643737793\n",
            "step 648: loss = 5.412888050079346\n",
            "step 649: loss = 5.595885276794434\n",
            "step 650: loss = 5.281680107116699\n",
            "step 651: loss = 5.732325553894043\n",
            "step 652: loss = 5.533509731292725\n",
            "step 653: loss = 5.428380966186523\n",
            "step 654: loss = 5.843149662017822\n",
            "step 655: loss = 5.4292497634887695\n",
            "step 656: loss = 5.749209880828857\n",
            "step 657: loss = 5.591587543487549\n",
            "step 658: loss = 5.543319225311279\n",
            "step 659: loss = 5.95224142074585\n",
            "step 660: loss = 5.582243919372559\n",
            "step 661: loss = 5.753647804260254\n",
            "step 662: loss = 5.413185119628906\n",
            "step 663: loss = 5.8115668296813965\n",
            "step 664: loss = 5.705942153930664\n",
            "step 665: loss = 5.403952121734619\n",
            "step 666: loss = 5.678008079528809\n",
            "step 667: loss = 5.958404064178467\n",
            "step 668: loss = 5.529641151428223\n",
            "step 669: loss = 5.711353302001953\n",
            "step 670: loss = 5.639379501342773\n",
            "step 671: loss = 5.633802890777588\n",
            "step 672: loss = 5.517550468444824\n",
            "step 673: loss = 5.91989803314209\n",
            "step 674: loss = 5.757194995880127\n",
            "step 675: loss = 5.6410298347473145\n",
            "step 676: loss = 5.398818016052246\n",
            "step 677: loss = 5.816572189331055\n",
            "step 678: loss = 5.621642589569092\n",
            "step 679: loss = 5.525409698486328\n",
            "step 680: loss = 5.424258232116699\n",
            "step 681: loss = 5.850402355194092\n",
            "step 682: loss = 5.951973915100098\n",
            "step 683: loss = 5.6245927810668945\n",
            "step 684: loss = 5.464597225189209\n",
            "step 685: loss = 5.502408027648926\n",
            "step 686: loss = 5.714161396026611\n",
            "step 687: loss = 5.8465800285339355\n",
            "step 688: loss = 5.443053245544434\n",
            "step 689: loss = 5.599392414093018\n",
            "step 690: loss = 5.561643600463867\n",
            "step 691: loss = 5.5702033042907715\n",
            "step 692: loss = 5.440289497375488\n",
            "step 693: loss = 5.589133262634277\n",
            "step 694: loss = 5.409328937530518\n",
            "step 695: loss = 5.56140661239624\n",
            "step 696: loss = 5.245423793792725\n",
            "step 697: loss = 5.456937313079834\n",
            "step 698: loss = 5.706718921661377\n",
            "step 699: loss = 5.156275272369385\n",
            "step 700: loss = 5.93859338760376\n",
            "step 701: loss = 5.48010778427124\n",
            "step 702: loss = 5.703188419342041\n",
            "step 703: loss = 5.280948638916016\n",
            "step 704: loss = 5.543652534484863\n",
            "step 705: loss = 5.705682277679443\n",
            "step 706: loss = 5.370116233825684\n",
            "step 707: loss = 5.472701549530029\n",
            "step 708: loss = 5.603425979614258\n",
            "step 709: loss = 5.6461501121521\n",
            "step 710: loss = 5.6418232917785645\n",
            "step 711: loss = 5.707513809204102\n",
            "step 712: loss = 5.160151958465576\n",
            "step 713: loss = 5.602428913116455\n",
            "step 714: loss = 5.840660095214844\n",
            "step 715: loss = 5.7460103034973145\n",
            "step 716: loss = 5.4997711181640625\n",
            "step 717: loss = 5.313248157501221\n",
            "step 718: loss = 5.670750617980957\n",
            "step 719: loss = 5.7417426109313965\n",
            "step 720: loss = 5.761372089385986\n",
            "step 721: loss = 5.650679111480713\n",
            "step 722: loss = 5.5005717277526855\n",
            "step 723: loss = 5.6026716232299805\n",
            "step 724: loss = 5.657008647918701\n",
            "step 725: loss = 5.530342102050781\n",
            "step 726: loss = 5.39598274230957\n",
            "step 727: loss = 5.304500579833984\n",
            "step 728: loss = 5.619981288909912\n",
            "step 729: loss = 5.369263648986816\n",
            "step 730: loss = 5.500387191772461\n",
            "step 731: loss = 5.7194061279296875\n",
            "step 732: loss = 5.316734313964844\n",
            "step 733: loss = 5.809629917144775\n",
            "step 734: loss = 5.522747039794922\n",
            "step 735: loss = 5.599057674407959\n",
            "step 736: loss = 5.812075614929199\n",
            "step 737: loss = 5.846012592315674\n",
            "step 738: loss = 5.20046329498291\n",
            "step 739: loss = 5.535400390625\n",
            "step 740: loss = 5.2492499351501465\n",
            "step 741: loss = 5.37998628616333\n",
            "step 742: loss = 5.751462936401367\n",
            "step 743: loss = 5.570751190185547\n",
            "step 744: loss = 5.701221942901611\n",
            "step 745: loss = 5.778283596038818\n",
            "step 746: loss = 5.7464985847473145\n",
            "step 747: loss = 5.459148406982422\n",
            "step 748: loss = 5.606052875518799\n",
            "step 749: loss = 5.789563179016113\n",
            "step 750: loss = 5.179405212402344\n",
            "step 751: loss = 5.11959981918335\n",
            "step 752: loss = 5.582469463348389\n",
            "step 753: loss = 5.499788761138916\n",
            "step 754: loss = 5.35297966003418\n",
            "step 755: loss = 5.231846332550049\n",
            "step 756: loss = 5.652620792388916\n",
            "step 757: loss = 5.331516265869141\n",
            "step 758: loss = 5.490419864654541\n",
            "step 759: loss = 5.254257678985596\n",
            "step 760: loss = 5.187351703643799\n",
            "step 761: loss = 5.68619966506958\n",
            "step 762: loss = 5.321438789367676\n",
            "step 763: loss = 5.442463397979736\n",
            "step 764: loss = 5.5669941902160645\n",
            "step 765: loss = 5.591540813446045\n",
            "step 766: loss = 5.583019256591797\n",
            "step 767: loss = 5.552971839904785\n",
            "step 768: loss = 5.634673595428467\n",
            "step 769: loss = 5.525496482849121\n",
            "step 770: loss = 5.476897239685059\n",
            "step 771: loss = 5.526596546173096\n",
            "step 772: loss = 5.4922990798950195\n",
            "step 773: loss = 5.443594455718994\n",
            "step 774: loss = 5.469388008117676\n",
            "step 775: loss = 5.3681559562683105\n",
            "step 776: loss = 5.243373394012451\n",
            "step 777: loss = 5.172963619232178\n",
            "step 778: loss = 5.39530086517334\n",
            "step 779: loss = 5.308093070983887\n",
            "step 780: loss = 5.071807384490967\n",
            "step 781: loss = 5.494355201721191\n",
            "step 782: loss = 5.288571834564209\n",
            "step 783: loss = 5.358121395111084\n",
            "step 784: loss = 5.352051258087158\n",
            "step 785: loss = 5.650102138519287\n",
            "step 786: loss = 5.3940887451171875\n",
            "step 787: loss = 5.380236625671387\n",
            "step 788: loss = 5.154631614685059\n",
            "step 789: loss = 5.804041385650635\n",
            "step 790: loss = 5.764082431793213\n",
            "step 791: loss = 5.460731029510498\n",
            "step 792: loss = 5.440818786621094\n",
            "step 793: loss = 5.1370062828063965\n",
            "step 794: loss = 5.4046430587768555\n",
            "step 795: loss = 5.539190769195557\n",
            "step 796: loss = 5.618183612823486\n",
            "step 797: loss = 5.1816277503967285\n",
            "step 798: loss = 5.144162178039551\n",
            "step 799: loss = 5.480823040008545\n",
            "step 800: loss = 5.243183135986328\n",
            "step 801: loss = 5.179331302642822\n",
            "step 802: loss = 5.582038402557373\n",
            "step 803: loss = 5.6190924644470215\n",
            "step 804: loss = 5.363924026489258\n",
            "step 805: loss = 5.174127578735352\n",
            "step 806: loss = 5.429892063140869\n",
            "step 807: loss = 5.509511470794678\n",
            "step 808: loss = 5.242811679840088\n",
            "step 809: loss = 5.669221878051758\n",
            "step 810: loss = 5.463996410369873\n",
            "step 811: loss = 5.620591163635254\n",
            "step 812: loss = 5.378082752227783\n",
            "step 813: loss = 5.296306610107422\n",
            "step 814: loss = 5.081011772155762\n",
            "step 815: loss = 5.316081523895264\n",
            "step 816: loss = 5.549290657043457\n",
            "step 817: loss = 5.357942581176758\n",
            "step 818: loss = 5.719565391540527\n",
            "step 819: loss = 5.259172439575195\n",
            "step 820: loss = 5.414512634277344\n",
            "step 821: loss = 5.37952184677124\n",
            "step 822: loss = 5.500778675079346\n",
            "step 823: loss = 5.558538436889648\n",
            "step 824: loss = 5.61712121963501\n",
            "step 825: loss = 5.152830600738525\n",
            "step 826: loss = 5.551843643188477\n",
            "step 827: loss = 5.535778999328613\n",
            "step 828: loss = 4.876716613769531\n",
            "step 829: loss = 5.738210678100586\n",
            "step 830: loss = 5.667163848876953\n",
            "step 831: loss = 5.368974685668945\n",
            "step 832: loss = 5.524784088134766\n",
            "step 833: loss = 5.6371965408325195\n",
            "step 834: loss = 5.661239147186279\n",
            "step 835: loss = 5.331121444702148\n",
            "step 836: loss = 5.289678573608398\n",
            "step 837: loss = 5.056436061859131\n",
            "step 838: loss = 5.463092803955078\n",
            "step 839: loss = 5.496788024902344\n",
            "step 840: loss = 5.448152542114258\n",
            "step 841: loss = 5.321934700012207\n",
            "step 842: loss = 5.666781902313232\n",
            "step 843: loss = 5.437180519104004\n",
            "step 844: loss = 5.484078884124756\n",
            "step 845: loss = 5.302972793579102\n",
            "step 846: loss = 5.506222724914551\n",
            "step 847: loss = 5.380706310272217\n",
            "step 848: loss = 5.447803020477295\n",
            "step 849: loss = 5.327018737792969\n",
            "step 850: loss = 5.432687282562256\n",
            "step 851: loss = 5.039093494415283\n",
            "step 852: loss = 5.195112228393555\n",
            "step 853: loss = 5.424943923950195\n",
            "step 854: loss = 5.470756530761719\n",
            "step 855: loss = 5.218019962310791\n",
            "step 856: loss = 5.300988674163818\n",
            "step 857: loss = 5.401703357696533\n",
            "step 858: loss = 5.508301734924316\n",
            "step 859: loss = 5.207996368408203\n",
            "step 860: loss = 5.488492012023926\n",
            "step 861: loss = 5.179330825805664\n",
            "step 862: loss = 5.161257743835449\n",
            "step 863: loss = 5.300017833709717\n",
            "step 864: loss = 5.364553451538086\n",
            "step 865: loss = 5.314587593078613\n",
            "step 866: loss = 5.1447978019714355\n",
            "step 867: loss = 5.355791091918945\n",
            "step 868: loss = 5.359254837036133\n",
            "step 869: loss = 5.268184661865234\n",
            "step 870: loss = 5.099422931671143\n",
            "step 871: loss = 5.395130157470703\n",
            "step 872: loss = 5.244151592254639\n",
            "step 873: loss = 5.302838325500488\n",
            "step 874: loss = 5.337749004364014\n",
            "step 875: loss = 4.924924373626709\n",
            "step 876: loss = 4.982004165649414\n",
            "step 877: loss = 5.5696563720703125\n",
            "step 878: loss = 5.2967753410339355\n",
            "step 879: loss = 5.409392356872559\n",
            "step 880: loss = 5.453887462615967\n",
            "step 881: loss = 5.5730061531066895\n",
            "step 882: loss = 5.471588611602783\n",
            "step 883: loss = 5.234461307525635\n",
            "step 884: loss = 5.725085735321045\n",
            "step 885: loss = 5.394759178161621\n",
            "step 886: loss = 5.131119728088379\n",
            "step 887: loss = 5.21049690246582\n",
            "step 888: loss = 5.352449417114258\n",
            "step 889: loss = 5.482643127441406\n",
            "step 890: loss = 5.3876633644104\n",
            "step 891: loss = 5.3230509757995605\n",
            "step 892: loss = 5.394723892211914\n",
            "step 893: loss = 5.206404685974121\n",
            "step 894: loss = 5.306606769561768\n",
            "step 895: loss = 5.4323930740356445\n",
            "step 896: loss = 5.397785663604736\n",
            "step 897: loss = 5.024994373321533\n",
            "step 898: loss = 5.265504837036133\n",
            "step 899: loss = 5.183729648590088\n",
            "step 900: loss = 5.170262336730957\n",
            "step 901: loss = 5.143553256988525\n",
            "step 902: loss = 5.410848617553711\n",
            "step 903: loss = 5.20118522644043\n",
            "step 904: loss = 5.350553035736084\n",
            "step 905: loss = 5.645431041717529\n",
            "step 906: loss = 5.143572807312012\n",
            "step 907: loss = 5.335474967956543\n",
            "step 908: loss = 5.563511848449707\n",
            "step 909: loss = 5.19223165512085\n",
            "step 910: loss = 5.250518321990967\n",
            "step 911: loss = 5.290676593780518\n",
            "step 912: loss = 5.121005535125732\n",
            "step 913: loss = 5.262324810028076\n",
            "step 914: loss = 4.9592742919921875\n",
            "step 915: loss = 5.014896869659424\n",
            "step 916: loss = 5.471189975738525\n",
            "step 917: loss = 5.503891944885254\n",
            "step 918: loss = 5.119583606719971\n",
            "step 919: loss = 5.445178508758545\n",
            "step 920: loss = 5.297936916351318\n",
            "step 921: loss = 5.335132122039795\n",
            "step 922: loss = 5.265408515930176\n",
            "step 923: loss = 5.269407749176025\n",
            "step 924: loss = 5.035876750946045\n",
            "step 925: loss = 5.023501873016357\n",
            "step 926: loss = 5.12394380569458\n",
            "step 927: loss = 5.505682945251465\n",
            "step 928: loss = 5.216196060180664\n",
            "step 929: loss = 5.280760765075684\n",
            "step 930: loss = 5.379870414733887\n",
            "step 931: loss = 5.4124836921691895\n",
            "step 932: loss = 5.259685516357422\n",
            "step 933: loss = 5.20291805267334\n",
            "step 934: loss = 5.055408000946045\n",
            "step 935: loss = 5.508782863616943\n",
            "step 936: loss = 5.393942356109619\n",
            "step 937: loss = 4.917587757110596\n",
            "step 938: loss = 5.412329196929932\n",
            "step 939: loss = 5.059426784515381\n",
            "step 940: loss = 5.260686874389648\n",
            "step 941: loss = 5.144806861877441\n",
            "step 942: loss = 5.167572498321533\n",
            "step 943: loss = 5.154246807098389\n",
            "step 944: loss = 5.089471340179443\n",
            "step 945: loss = 4.947992324829102\n",
            "step 946: loss = 4.908069610595703\n",
            "step 947: loss = 5.107361793518066\n",
            "step 948: loss = 5.474661827087402\n",
            "step 949: loss = 5.233585834503174\n",
            "step 950: loss = 5.221016883850098\n",
            "step 951: loss = 5.1569061279296875\n",
            "step 952: loss = 5.258786678314209\n",
            "step 953: loss = 4.700108051300049\n",
            "step 954: loss = 5.6552205085754395\n",
            "step 955: loss = 5.216341018676758\n",
            "step 956: loss = 5.195076942443848\n",
            "step 957: loss = 5.06644344329834\n",
            "step 958: loss = 5.499046802520752\n",
            "step 959: loss = 5.456355094909668\n",
            "step 960: loss = 5.47497034072876\n",
            "step 961: loss = 5.136414527893066\n",
            "step 962: loss = 5.199793815612793\n",
            "step 963: loss = 5.253566741943359\n",
            "step 964: loss = 5.454382419586182\n",
            "step 965: loss = 5.228612899780273\n",
            "step 966: loss = 4.9394850730896\n",
            "step 967: loss = 5.1604204177856445\n",
            "step 968: loss = 5.177633762359619\n",
            "step 969: loss = 5.503329753875732\n",
            "step 970: loss = 5.292806625366211\n",
            "step 971: loss = 5.054406642913818\n",
            "step 972: loss = 5.148224353790283\n",
            "step 973: loss = 5.226651191711426\n",
            "step 974: loss = 4.980678081512451\n",
            "step 975: loss = 5.474706172943115\n",
            "step 976: loss = 5.192502498626709\n",
            "step 977: loss = 5.393538475036621\n",
            "step 978: loss = 4.864147186279297\n",
            "step 979: loss = 5.202595233917236\n",
            "step 980: loss = 5.129406929016113\n",
            "step 981: loss = 4.871101379394531\n",
            "step 982: loss = 5.180770397186279\n",
            "step 983: loss = 5.7022223472595215\n",
            "step 984: loss = 5.1927809715271\n",
            "step 985: loss = 5.196908950805664\n",
            "step 986: loss = 5.328181266784668\n",
            "step 987: loss = 5.525411128997803\n",
            "step 988: loss = 5.472066402435303\n",
            "step 989: loss = 5.164013862609863\n",
            "step 990: loss = 4.98489236831665\n",
            "step 991: loss = 4.996342658996582\n",
            "step 992: loss = 5.269629001617432\n",
            "step 993: loss = 5.264531135559082\n",
            "step 994: loss = 5.1336164474487305\n",
            "step 995: loss = 5.320621967315674\n",
            "step 996: loss = 4.936771869659424\n",
            "step 997: loss = 5.3410844802856445\n",
            "step 998: loss = 5.236861228942871\n",
            "step 999: loss = 5.047609806060791\n",
            "step 1000: loss = 5.275834083557129\n",
            "step 1001: loss = 5.13957405090332\n",
            "step 1002: loss = 5.166193962097168\n",
            "step 1003: loss = 4.761341094970703\n",
            "step 1004: loss = 5.2962646484375\n",
            "step 1005: loss = 4.829128265380859\n",
            "step 1006: loss = 4.92658805847168\n",
            "step 1007: loss = 5.087543964385986\n",
            "step 1008: loss = 5.307788372039795\n",
            "step 1009: loss = 5.287065505981445\n",
            "step 1010: loss = 5.010501861572266\n",
            "step 1011: loss = 5.139089107513428\n",
            "step 1012: loss = 5.1831464767456055\n",
            "step 1013: loss = 5.078249454498291\n",
            "step 1014: loss = 4.952774524688721\n",
            "step 1015: loss = 4.773787975311279\n",
            "step 1016: loss = 5.01612663269043\n",
            "step 1017: loss = 5.19641637802124\n",
            "step 1018: loss = 4.929531097412109\n",
            "step 1019: loss = 5.255906105041504\n",
            "step 1020: loss = 5.248295307159424\n",
            "step 1021: loss = 5.131464958190918\n",
            "step 1022: loss = 5.414926528930664\n",
            "step 1023: loss = 5.207287311553955\n",
            "step 1024: loss = 5.391634464263916\n",
            "step 1025: loss = 5.122983455657959\n",
            "step 1026: loss = 5.182785511016846\n",
            "step 1027: loss = 4.799715042114258\n",
            "step 1028: loss = 4.8579630851745605\n",
            "step 1029: loss = 5.3040595054626465\n",
            "step 1030: loss = 5.107075214385986\n",
            "step 1031: loss = 5.239683151245117\n",
            "step 1032: loss = 5.161896705627441\n",
            "step 1033: loss = 5.19669771194458\n",
            "step 1034: loss = 5.484753131866455\n",
            "step 1035: loss = 5.13861083984375\n",
            "step 1036: loss = 4.99870491027832\n",
            "step 1037: loss = 5.033306121826172\n",
            "step 1038: loss = 5.1680006980896\n",
            "step 1039: loss = 4.932219982147217\n",
            "step 1040: loss = 5.023288726806641\n",
            "step 1041: loss = 5.45712423324585\n",
            "step 1042: loss = 4.93398904800415\n",
            "step 1043: loss = 5.2306227684021\n",
            "step 1044: loss = 5.218977451324463\n",
            "step 1045: loss = 4.874050617218018\n",
            "step 1046: loss = 5.340850830078125\n",
            "step 1047: loss = 5.082159042358398\n",
            "step 1048: loss = 5.169277191162109\n",
            "step 1049: loss = 5.171403408050537\n",
            "step 1050: loss = 5.258057594299316\n",
            "step 1051: loss = 5.049692153930664\n",
            "step 1052: loss = 4.9180684089660645\n",
            "step 1053: loss = 5.240228652954102\n",
            "step 1054: loss = 4.91729736328125\n",
            "step 1055: loss = 4.772397518157959\n",
            "step 1056: loss = 5.006850719451904\n",
            "step 1057: loss = 4.944797039031982\n",
            "step 1058: loss = 5.297093391418457\n",
            "step 1059: loss = 5.436488151550293\n",
            "step 1060: loss = 5.289894104003906\n",
            "step 1061: loss = 5.257047653198242\n",
            "step 1062: loss = 5.158671855926514\n",
            "step 1063: loss = 4.998055934906006\n",
            "step 1064: loss = 4.838742256164551\n",
            "step 1065: loss = 5.010658264160156\n",
            "step 1066: loss = 5.474205017089844\n",
            "step 1067: loss = 5.09313440322876\n",
            "step 1068: loss = 5.392430305480957\n",
            "step 1069: loss = 5.238846302032471\n",
            "step 1070: loss = 4.96537446975708\n",
            "step 1071: loss = 4.792063236236572\n",
            "step 1072: loss = 5.254945755004883\n",
            "step 1073: loss = 4.991856098175049\n",
            "step 1074: loss = 4.828828811645508\n",
            "step 1075: loss = 5.053018569946289\n",
            "step 1076: loss = 5.250882625579834\n",
            "step 1077: loss = 5.047492980957031\n",
            "step 1078: loss = 4.804965972900391\n",
            "step 1079: loss = 5.1400980949401855\n",
            "step 1080: loss = 5.090151786804199\n",
            "step 1081: loss = 4.982085704803467\n",
            "step 1082: loss = 5.016862869262695\n",
            "step 1083: loss = 5.084554195404053\n",
            "step 1084: loss = 5.343637943267822\n",
            "step 1085: loss = 5.4638895988464355\n",
            "step 1086: loss = 5.20240592956543\n",
            "step 1087: loss = 5.181797027587891\n",
            "step 1088: loss = 5.1704912185668945\n",
            "step 1089: loss = 5.061041831970215\n",
            "step 1090: loss = 5.009073257446289\n",
            "step 1091: loss = 5.408331871032715\n",
            "step 1092: loss = 5.30518102645874\n",
            "step 1093: loss = 4.996805191040039\n",
            "step 1094: loss = 5.124386310577393\n",
            "step 1095: loss = 5.219411373138428\n",
            "step 1096: loss = 5.169880390167236\n",
            "step 1097: loss = 4.833895206451416\n",
            "step 1098: loss = 4.987270355224609\n",
            "step 1099: loss = 5.021976470947266\n",
            "step 1100: loss = 5.436469554901123\n",
            "step 1101: loss = 5.194865703582764\n",
            "step 1102: loss = 5.083906173706055\n",
            "step 1103: loss = 5.215692520141602\n",
            "step 1104: loss = 4.427298545837402\n",
            "step 1105: loss = 5.093863487243652\n",
            "step 1106: loss = 5.057969093322754\n",
            "step 1107: loss = 4.624019622802734\n",
            "step 1108: loss = 5.068765640258789\n",
            "step 1109: loss = 4.82219123840332\n",
            "step 1110: loss = 5.089604377746582\n",
            "step 1111: loss = 5.2189717292785645\n",
            "step 1112: loss = 4.778171539306641\n",
            "step 1113: loss = 5.191256046295166\n",
            "step 1114: loss = 4.9124979972839355\n",
            "step 1115: loss = 5.189789295196533\n",
            "step 1116: loss = 4.881338596343994\n",
            "step 1117: loss = 4.735130786895752\n",
            "step 1118: loss = 4.854002475738525\n",
            "step 1119: loss = 4.961556434631348\n",
            "step 1120: loss = 4.915716648101807\n",
            "step 1121: loss = 5.206439018249512\n",
            "step 1122: loss = 5.157530784606934\n",
            "step 1123: loss = 5.140046119689941\n",
            "step 1124: loss = 5.503004550933838\n",
            "step 1125: loss = 4.787539005279541\n",
            "step 1126: loss = 4.786377906799316\n",
            "step 1127: loss = 5.106977939605713\n",
            "step 1128: loss = 4.794121742248535\n",
            "step 1129: loss = 5.155601978302002\n",
            "step 1130: loss = 5.238156318664551\n",
            "step 1131: loss = 5.050819396972656\n",
            "step 1132: loss = 5.102145195007324\n",
            "step 1133: loss = 5.18865442276001\n",
            "step 1134: loss = 4.8596343994140625\n",
            "step 1135: loss = 4.994364261627197\n",
            "step 1136: loss = 5.397340297698975\n",
            "step 1137: loss = 4.652799129486084\n",
            "step 1138: loss = 5.342721939086914\n",
            "step 1139: loss = 4.888944625854492\n",
            "step 1140: loss = 4.898622512817383\n",
            "step 1141: loss = 4.727406978607178\n",
            "step 1142: loss = 4.915144443511963\n",
            "step 1143: loss = 4.829906463623047\n",
            "step 1144: loss = 5.13359260559082\n",
            "step 1145: loss = 5.218118190765381\n",
            "step 1146: loss = 5.231812000274658\n",
            "step 1147: loss = 5.062072277069092\n",
            "step 1148: loss = 4.981507778167725\n",
            "step 1149: loss = 5.193451404571533\n",
            "step 1150: loss = 5.096489429473877\n",
            "step 1151: loss = 4.91799783706665\n",
            "step 1152: loss = 5.132587909698486\n",
            "step 1153: loss = 4.8931097984313965\n",
            "step 1154: loss = 5.118289947509766\n",
            "step 1155: loss = 5.225090980529785\n",
            "step 1156: loss = 5.060188293457031\n",
            "step 1157: loss = 4.973471641540527\n",
            "step 1158: loss = 5.188531875610352\n",
            "step 1159: loss = 4.919295787811279\n",
            "step 1160: loss = 4.806662082672119\n",
            "step 1161: loss = 4.982819080352783\n",
            "step 1162: loss = 4.883386135101318\n",
            "step 1163: loss = 5.075969219207764\n",
            "step 1164: loss = 5.18480920791626\n",
            "step 1165: loss = 4.856634140014648\n",
            "step 1166: loss = 5.509124279022217\n",
            "step 1167: loss = 5.112009525299072\n",
            "step 1168: loss = 5.313989162445068\n",
            "step 1169: loss = 5.262176990509033\n",
            "step 1170: loss = 4.779163837432861\n",
            "step 1171: loss = 5.185079097747803\n",
            "step 1172: loss = 4.939233779907227\n",
            "step 1173: loss = 5.027775764465332\n",
            "step 1174: loss = 4.776407241821289\n",
            "step 1175: loss = 4.9355082511901855\n",
            "step 1176: loss = 5.07282829284668\n",
            "step 1177: loss = 4.905437469482422\n",
            "step 1178: loss = 5.074512958526611\n",
            "step 1179: loss = 4.476987838745117\n",
            "step 1180: loss = 5.417142868041992\n",
            "step 1181: loss = 4.743736267089844\n",
            "step 1182: loss = 4.779031276702881\n",
            "step 1183: loss = 5.268347263336182\n",
            "step 1184: loss = 4.952949047088623\n",
            "step 1185: loss = 4.8731160163879395\n",
            "step 1186: loss = 4.660497188568115\n",
            "step 1187: loss = 4.979143142700195\n",
            "step 1188: loss = 5.0852131843566895\n",
            "step 1189: loss = 4.9231672286987305\n",
            "step 1190: loss = 4.973888397216797\n",
            "step 1191: loss = 5.106067180633545\n",
            "step 1192: loss = 5.031513214111328\n",
            "step 1193: loss = 5.019454479217529\n",
            "step 1194: loss = 5.124551296234131\n",
            "step 1195: loss = 4.766880035400391\n",
            "step 1196: loss = 5.05134916305542\n",
            "step 1197: loss = 5.085741996765137\n",
            "step 1198: loss = 4.613419532775879\n",
            "step 1199: loss = 4.71731424331665\n",
            "step 1200: loss = 4.917412281036377\n",
            "step 1201: loss = 5.117961883544922\n",
            "step 1202: loss = 5.0790205001831055\n",
            "step 1203: loss = 5.0777587890625\n",
            "step 1204: loss = 5.2566914558410645\n",
            "step 1205: loss = 4.891436576843262\n",
            "step 1206: loss = 5.012812614440918\n",
            "step 1207: loss = 4.885483741760254\n",
            "step 1208: loss = 5.102044582366943\n",
            "step 1209: loss = 5.208310127258301\n",
            "step 1210: loss = 4.989924907684326\n",
            "step 1211: loss = 4.928524017333984\n",
            "step 1212: loss = 4.794075965881348\n",
            "step 1213: loss = 5.228542804718018\n",
            "step 1214: loss = 4.948129653930664\n",
            "step 1215: loss = 5.089379787445068\n",
            "step 1216: loss = 5.130230903625488\n",
            "step 1217: loss = 4.990999698638916\n",
            "step 1218: loss = 4.908697605133057\n",
            "step 1219: loss = 4.931941509246826\n",
            "step 1220: loss = 4.834400653839111\n",
            "step 1221: loss = 4.773728370666504\n",
            "step 1222: loss = 4.915346145629883\n",
            "step 1223: loss = 4.971955299377441\n",
            "step 1224: loss = 4.846437931060791\n",
            "step 1225: loss = 4.781959533691406\n",
            "step 1226: loss = 5.203925609588623\n",
            "step 1227: loss = 5.08991003036499\n",
            "step 1228: loss = 4.886141777038574\n",
            "step 1229: loss = 5.280782222747803\n",
            "step 1230: loss = 5.069042682647705\n",
            "step 1231: loss = 4.973918914794922\n",
            "step 1232: loss = 5.068602085113525\n",
            "step 1233: loss = 4.6694231033325195\n",
            "step 1234: loss = 5.234929084777832\n",
            "step 1235: loss = 4.8754777908325195\n",
            "step 1236: loss = 4.838380813598633\n",
            "step 1237: loss = 4.840804100036621\n",
            "step 1238: loss = 5.118105888366699\n",
            "step 1239: loss = 4.980170249938965\n",
            "step 1240: loss = 4.855772495269775\n",
            "step 1241: loss = 5.3841705322265625\n",
            "step 1242: loss = 4.744246959686279\n",
            "step 1243: loss = 4.702826499938965\n",
            "step 1244: loss = 4.956231594085693\n",
            "step 1245: loss = 4.73317813873291\n",
            "step 1246: loss = 5.2185750007629395\n",
            "step 1247: loss = 4.9043707847595215\n",
            "step 1248: loss = 4.77476692199707\n",
            "step 1249: loss = 4.852655410766602\n",
            "step 1250: loss = 5.146367073059082\n",
            "step 1251: loss = 5.019399642944336\n",
            "step 1252: loss = 4.989755153656006\n",
            "step 1253: loss = 5.134808540344238\n",
            "step 1254: loss = 4.997204303741455\n",
            "step 1255: loss = 4.808231830596924\n",
            "step 1256: loss = 5.166723728179932\n",
            "step 1257: loss = 4.869326591491699\n",
            "step 1258: loss = 4.768405437469482\n",
            "step 1259: loss = 5.0373921394348145\n",
            "step 1260: loss = 5.164774417877197\n",
            "step 1261: loss = 4.767687797546387\n",
            "step 1262: loss = 4.869850158691406\n",
            "step 1263: loss = 4.591463565826416\n",
            "step 1264: loss = 4.881235599517822\n",
            "step 1265: loss = 4.495763778686523\n",
            "step 1266: loss = 4.899150848388672\n",
            "step 1267: loss = 5.007254600524902\n",
            "step 1268: loss = 5.058821678161621\n",
            "step 1269: loss = 4.590722560882568\n",
            "step 1270: loss = 5.0886430740356445\n",
            "step 1271: loss = 4.80874490737915\n",
            "step 1272: loss = 5.045072078704834\n",
            "step 1273: loss = 4.790022373199463\n",
            "step 1274: loss = 4.864325523376465\n",
            "step 1275: loss = 5.231464385986328\n",
            "step 1276: loss = 4.6655426025390625\n",
            "step 1277: loss = 4.888720989227295\n",
            "step 1278: loss = 5.066990375518799\n",
            "step 1279: loss = 5.068661212921143\n",
            "step 1280: loss = 4.652321815490723\n",
            "step 1281: loss = 4.741057872772217\n",
            "step 1282: loss = 4.889357566833496\n",
            "step 1283: loss = 4.910012245178223\n",
            "step 1284: loss = 4.762901782989502\n",
            "step 1285: loss = 4.986489295959473\n",
            "step 1286: loss = 4.745394229888916\n",
            "step 1287: loss = 4.920881748199463\n",
            "step 1288: loss = 4.471890926361084\n",
            "step 1289: loss = 4.789305210113525\n",
            "step 1290: loss = 4.580442428588867\n",
            "step 1291: loss = 4.7772064208984375\n",
            "step 1292: loss = 4.930562973022461\n",
            "step 1293: loss = 5.0498576164245605\n",
            "step 1294: loss = 4.4766526222229\n",
            "step 1295: loss = 4.784664630889893\n",
            "step 1296: loss = 4.864314079284668\n",
            "step 1297: loss = 4.968003749847412\n",
            "step 1298: loss = 4.890291213989258\n",
            "step 1299: loss = 4.623409271240234\n",
            "step 1300: loss = 4.834421634674072\n",
            "step 1301: loss = 4.7020769119262695\n",
            "step 1302: loss = 4.846621990203857\n",
            "step 1303: loss = 4.895962715148926\n",
            "step 1304: loss = 5.023523330688477\n",
            "step 1305: loss = 4.974358081817627\n",
            "step 1306: loss = 4.784251689910889\n",
            "step 1307: loss = 4.844500541687012\n",
            "step 1308: loss = 4.59828519821167\n",
            "step 1309: loss = 5.02227783203125\n",
            "step 1310: loss = 4.891480922698975\n",
            "step 1311: loss = 4.7485880851745605\n",
            "step 1312: loss = 4.843282222747803\n",
            "step 1313: loss = 5.076174736022949\n",
            "step 1314: loss = 5.0201802253723145\n",
            "step 1315: loss = 5.0081071853637695\n",
            "step 1316: loss = 4.762871742248535\n",
            "step 1317: loss = 5.062890529632568\n",
            "step 1318: loss = 4.733001708984375\n",
            "step 1319: loss = 5.0408453941345215\n",
            "step 1320: loss = 4.797101974487305\n",
            "step 1321: loss = 4.843582630157471\n",
            "step 1322: loss = 4.889305591583252\n",
            "step 1323: loss = 4.856919765472412\n",
            "step 1324: loss = 5.160150051116943\n",
            "step 1325: loss = 4.971073627471924\n",
            "step 1326: loss = 4.914816379547119\n",
            "step 1327: loss = 4.706387996673584\n",
            "step 1328: loss = 4.805792331695557\n",
            "step 1329: loss = 4.8776679039001465\n",
            "step 1330: loss = 4.4574151039123535\n",
            "step 1331: loss = 4.6986212730407715\n",
            "step 1332: loss = 5.132864952087402\n",
            "step 1333: loss = 5.002294540405273\n",
            "step 1334: loss = 4.834249496459961\n",
            "step 1335: loss = 4.649662971496582\n",
            "step 1336: loss = 5.079399585723877\n",
            "step 1337: loss = 4.74820613861084\n",
            "step 1338: loss = 5.150197505950928\n",
            "step 1339: loss = 5.040828704833984\n",
            "step 1340: loss = 4.609494209289551\n",
            "step 1341: loss = 4.747378826141357\n",
            "step 1342: loss = 4.733029842376709\n",
            "step 1343: loss = 5.2481689453125\n",
            "step 1344: loss = 4.581610202789307\n",
            "step 1345: loss = 5.006300926208496\n",
            "step 1346: loss = 4.913854122161865\n",
            "step 1347: loss = 5.121541500091553\n",
            "step 1348: loss = 4.625776290893555\n",
            "step 1349: loss = 4.573457717895508\n",
            "step 1350: loss = 4.81980037689209\n",
            "step 1351: loss = 5.010387897491455\n",
            "step 1352: loss = 4.786111831665039\n",
            "step 1353: loss = 4.54286527633667\n",
            "step 1354: loss = 4.896780967712402\n",
            "step 1355: loss = 4.875029563903809\n",
            "step 1356: loss = 4.826761722564697\n",
            "step 1357: loss = 4.541909217834473\n",
            "step 1358: loss = 4.833036422729492\n",
            "step 1359: loss = 4.891616344451904\n",
            "step 1360: loss = 5.058752536773682\n",
            "step 1361: loss = 4.973172187805176\n",
            "step 1362: loss = 4.907961845397949\n",
            "step 1363: loss = 4.515649795532227\n",
            "step 1364: loss = 5.001857280731201\n",
            "step 1365: loss = 4.404034614562988\n",
            "step 1366: loss = 4.980650901794434\n",
            "step 1367: loss = 5.107958793640137\n",
            "step 1368: loss = 4.877640247344971\n",
            "step 1369: loss = 4.618350028991699\n",
            "step 1370: loss = 4.9190521240234375\n",
            "step 1371: loss = 4.758586406707764\n",
            "step 1372: loss = 4.86781644821167\n",
            "step 1373: loss = 4.588050365447998\n",
            "step 1374: loss = 5.052842617034912\n",
            "step 1375: loss = 4.859396934509277\n",
            "step 1376: loss = 4.8769145011901855\n",
            "step 1377: loss = 4.787521839141846\n",
            "step 1378: loss = 4.579349994659424\n",
            "step 1379: loss = 5.031900405883789\n",
            "step 1380: loss = 4.69807243347168\n",
            "step 1381: loss = 4.698733806610107\n",
            "step 1382: loss = 4.645535469055176\n",
            "step 1383: loss = 4.888514041900635\n",
            "step 1384: loss = 4.94697380065918\n",
            "step 1385: loss = 4.853104591369629\n",
            "step 1386: loss = 4.641457557678223\n",
            "step 1387: loss = 4.507162570953369\n",
            "step 1388: loss = 4.5496625900268555\n",
            "step 1389: loss = 5.1723432540893555\n",
            "step 1390: loss = 4.850274562835693\n",
            "step 1391: loss = 4.878755569458008\n",
            "step 1392: loss = 4.564388275146484\n",
            "step 1393: loss = 4.512482643127441\n",
            "step 1394: loss = 5.090593338012695\n",
            "step 1395: loss = 4.901345729827881\n",
            "step 1396: loss = 4.975269794464111\n",
            "step 1397: loss = 4.823182106018066\n",
            "step 1398: loss = 5.142364978790283\n",
            "step 1399: loss = 5.0054144859313965\n",
            "step 1400: loss = 5.221778869628906\n",
            "step 1401: loss = 4.6854023933410645\n",
            "step 1402: loss = 4.605430603027344\n",
            "step 1403: loss = 4.33128023147583\n",
            "step 1404: loss = 4.70759391784668\n",
            "step 1405: loss = 4.997393608093262\n",
            "step 1406: loss = 4.5304131507873535\n",
            "step 1407: loss = 4.80136251449585\n",
            "step 1408: loss = 4.594252586364746\n",
            "step 1409: loss = 4.443493366241455\n",
            "step 1410: loss = 4.71975040435791\n",
            "step 1411: loss = 4.658475399017334\n",
            "step 1412: loss = 4.734930515289307\n",
            "step 1413: loss = 4.636419296264648\n",
            "step 1414: loss = 4.498862266540527\n",
            "step 1415: loss = 5.005791187286377\n",
            "step 1416: loss = 4.881062030792236\n",
            "step 1417: loss = 5.001216888427734\n",
            "step 1418: loss = 4.663174629211426\n",
            "step 1419: loss = 5.096340179443359\n",
            "step 1420: loss = 4.7392377853393555\n",
            "step 1421: loss = 4.877079963684082\n",
            "step 1422: loss = 4.758769989013672\n",
            "step 1423: loss = 4.999326229095459\n",
            "step 1424: loss = 4.667530059814453\n",
            "step 1425: loss = 4.675654411315918\n",
            "step 1426: loss = 4.977720260620117\n",
            "step 1427: loss = 4.732870101928711\n",
            "step 1428: loss = 4.6955180168151855\n",
            "step 1429: loss = 4.7114386558532715\n",
            "step 1430: loss = 5.283880233764648\n",
            "step 1431: loss = 4.870241165161133\n",
            "step 1432: loss = 5.130118370056152\n",
            "step 1433: loss = 5.039267063140869\n",
            "step 1434: loss = 5.21457052230835\n",
            "step 1435: loss = 4.774488925933838\n",
            "step 1436: loss = 4.970767974853516\n",
            "step 1437: loss = 4.6172990798950195\n",
            "step 1438: loss = 4.936020851135254\n",
            "step 1439: loss = 4.901042938232422\n",
            "step 1440: loss = 4.9092631340026855\n",
            "step 1441: loss = 4.67566442489624\n",
            "step 1442: loss = 4.713540077209473\n",
            "step 1443: loss = 4.4445481300354\n",
            "step 1444: loss = 5.0688042640686035\n",
            "step 1445: loss = 4.80875301361084\n",
            "step 1446: loss = 4.555962085723877\n",
            "step 1447: loss = 4.504560947418213\n",
            "step 1448: loss = 4.817187309265137\n",
            "step 1449: loss = 4.49798059463501\n",
            "step 1450: loss = 4.718021392822266\n",
            "step 1451: loss = 4.756328105926514\n",
            "step 1452: loss = 4.666627407073975\n",
            "step 1453: loss = 4.479966640472412\n",
            "step 1454: loss = 4.860718250274658\n",
            "step 1455: loss = 4.747967720031738\n",
            "step 1456: loss = 4.927215099334717\n",
            "step 1457: loss = 4.276450157165527\n",
            "step 1458: loss = 4.485067367553711\n",
            "step 1459: loss = 4.484913349151611\n",
            "step 1460: loss = 4.550747871398926\n",
            "step 1461: loss = 5.053498268127441\n",
            "step 1462: loss = 4.707120895385742\n",
            "step 1463: loss = 4.792133808135986\n",
            "step 1464: loss = 4.751062393188477\n",
            "step 1465: loss = 4.242749214172363\n",
            "step 1466: loss = 4.906279563903809\n",
            "step 1467: loss = 4.722474575042725\n",
            "step 1468: loss = 4.648662090301514\n",
            "step 1469: loss = 4.469230651855469\n",
            "step 1470: loss = 4.94555139541626\n",
            "step 1471: loss = 4.736571788787842\n",
            "step 1472: loss = 4.780416965484619\n",
            "step 1473: loss = 4.809558391571045\n",
            "step 1474: loss = 4.720321178436279\n",
            "step 1475: loss = 4.6284260749816895\n",
            "step 1476: loss = 4.32919979095459\n",
            "step 1477: loss = 4.416862487792969\n",
            "step 1478: loss = 4.247245788574219\n",
            "step 1479: loss = 5.007696628570557\n",
            "step 1480: loss = 4.59598445892334\n",
            "step 1481: loss = 4.669864177703857\n",
            "step 1482: loss = 4.710285663604736\n",
            "step 1483: loss = 4.60672664642334\n",
            "step 1484: loss = 4.624878883361816\n",
            "step 1485: loss = 4.542744159698486\n",
            "step 1486: loss = 4.585535526275635\n",
            "step 1487: loss = 4.821407794952393\n",
            "step 1488: loss = 4.770580768585205\n",
            "step 1489: loss = 4.5128045082092285\n",
            "step 1490: loss = 4.8589701652526855\n",
            "step 1491: loss = 4.4175238609313965\n",
            "step 1492: loss = 4.855352401733398\n",
            "step 1493: loss = 4.550122261047363\n",
            "step 1494: loss = 4.740046977996826\n",
            "step 1495: loss = 4.402881145477295\n",
            "step 1496: loss = 4.787996292114258\n",
            "step 1497: loss = 4.698322296142578\n",
            "step 1498: loss = 4.531669616699219\n",
            "step 1499: loss = 4.56797456741333\n",
            "step 1500: loss = 4.649569511413574\n",
            "step 1501: loss = 4.351791858673096\n",
            "step 1502: loss = 4.73961067199707\n",
            "step 1503: loss = 4.5501604080200195\n",
            "step 1504: loss = 4.557097911834717\n",
            "step 1505: loss = 4.538365364074707\n",
            "step 1506: loss = 4.809521675109863\n",
            "step 1507: loss = 4.957698822021484\n",
            "step 1508: loss = 4.568679332733154\n",
            "step 1509: loss = 4.650417804718018\n",
            "step 1510: loss = 4.502574443817139\n",
            "step 1511: loss = 4.397463798522949\n",
            "step 1512: loss = 4.446267604827881\n",
            "step 1513: loss = 4.853708267211914\n",
            "step 1514: loss = 4.593942165374756\n",
            "step 1515: loss = 4.782955646514893\n",
            "step 1516: loss = 4.521092414855957\n",
            "step 1517: loss = 4.614851951599121\n",
            "step 1518: loss = 4.7147603034973145\n",
            "step 1519: loss = 4.667232036590576\n",
            "step 1520: loss = 4.37306022644043\n",
            "step 1521: loss = 4.870945453643799\n",
            "step 1522: loss = 4.737433433532715\n",
            "step 1523: loss = 4.683302879333496\n",
            "step 1524: loss = 4.952630043029785\n",
            "step 1525: loss = 4.821666717529297\n",
            "step 1526: loss = 5.052328109741211\n",
            "step 1527: loss = 4.428563594818115\n",
            "step 1528: loss = 4.749324798583984\n",
            "step 1529: loss = 4.39313268661499\n",
            "step 1530: loss = 4.890447616577148\n",
            "step 1531: loss = 4.737313270568848\n",
            "step 1532: loss = 4.840758800506592\n",
            "step 1533: loss = 4.711700916290283\n",
            "step 1534: loss = 4.545721530914307\n",
            "step 1535: loss = 4.6947431564331055\n",
            "step 1536: loss = 4.852084159851074\n",
            "step 1537: loss = 5.132226467132568\n",
            "step 1538: loss = 4.8728556632995605\n",
            "step 1539: loss = 4.808426856994629\n",
            "step 1540: loss = 5.174437999725342\n",
            "step 1541: loss = 4.509643077850342\n",
            "step 1542: loss = 4.632431507110596\n",
            "step 1543: loss = 4.807560920715332\n",
            "step 1544: loss = 4.80460786819458\n",
            "step 1545: loss = 4.600610256195068\n",
            "step 1546: loss = 4.703589916229248\n",
            "step 1547: loss = 4.77978515625\n",
            "step 1548: loss = 4.4828782081604\n",
            "step 1549: loss = 4.785804748535156\n",
            "step 1550: loss = 5.007811546325684\n",
            "step 1551: loss = 4.466180324554443\n",
            "step 1552: loss = 4.72783088684082\n",
            "step 1553: loss = 4.873708248138428\n",
            "step 1554: loss = 4.661212921142578\n",
            "step 1555: loss = 4.667646884918213\n",
            "step 1556: loss = 4.5088605880737305\n",
            "step 1557: loss = 4.53875732421875\n",
            "step 1558: loss = 4.742640972137451\n",
            "step 1559: loss = 4.894859313964844\n",
            "step 1560: loss = 4.795434474945068\n",
            "step 1561: loss = 4.83182430267334\n",
            "step 1562: loss = 4.656017780303955\n",
            "Finish epoch 1\n",
            "New model saved, minimum loss: 5.655840799536809 \n",
            "\n",
            "step 1563: loss = 4.340001583099365\n",
            "step 1564: loss = 4.329592704772949\n",
            "step 1565: loss = 4.403767108917236\n",
            "step 1566: loss = 4.222950458526611\n",
            "step 1567: loss = 4.525922775268555\n",
            "step 1568: loss = 4.442435264587402\n",
            "step 1569: loss = 4.437538146972656\n",
            "step 1570: loss = 4.318437576293945\n",
            "step 1571: loss = 4.404369354248047\n",
            "step 1572: loss = 4.580307960510254\n",
            "step 1573: loss = 4.3412346839904785\n",
            "step 1574: loss = 4.4375715255737305\n",
            "step 1575: loss = 4.562565803527832\n",
            "step 1576: loss = 4.272546768188477\n",
            "step 1577: loss = 4.630125522613525\n",
            "step 1578: loss = 4.200438022613525\n",
            "step 1579: loss = 4.378126621246338\n",
            "step 1580: loss = 4.286634922027588\n",
            "step 1581: loss = 4.619967460632324\n",
            "step 1582: loss = 4.516707420349121\n",
            "step 1583: loss = 4.482044696807861\n",
            "step 1584: loss = 4.766554355621338\n",
            "step 1585: loss = 4.226461887359619\n",
            "step 1586: loss = 4.429376125335693\n",
            "step 1587: loss = 4.40299129486084\n",
            "step 1588: loss = 4.377633094787598\n",
            "step 1589: loss = 4.292148590087891\n",
            "step 1590: loss = 4.167463779449463\n",
            "step 1591: loss = 4.235081672668457\n",
            "step 1592: loss = 4.343306064605713\n",
            "step 1593: loss = 4.514852046966553\n",
            "step 1594: loss = 4.154494762420654\n",
            "step 1595: loss = 4.481388568878174\n",
            "step 1596: loss = 4.608365535736084\n",
            "step 1597: loss = 4.068238735198975\n",
            "step 1598: loss = 4.524268627166748\n",
            "step 1599: loss = 4.523171901702881\n",
            "step 1600: loss = 4.065102577209473\n",
            "step 1601: loss = 4.1194329261779785\n",
            "step 1602: loss = 4.096233367919922\n",
            "step 1603: loss = 4.257187843322754\n",
            "step 1604: loss = 4.128188133239746\n",
            "step 1605: loss = 4.132211208343506\n",
            "step 1606: loss = 4.301518440246582\n",
            "step 1607: loss = 4.141510963439941\n",
            "step 1608: loss = 4.514726161956787\n",
            "step 1609: loss = 4.3836541175842285\n",
            "step 1610: loss = 4.326661109924316\n",
            "step 1611: loss = 4.455295562744141\n",
            "step 1612: loss = 4.081368923187256\n",
            "step 1613: loss = 4.556783676147461\n",
            "step 1614: loss = 4.2453413009643555\n",
            "step 1615: loss = 4.23270845413208\n",
            "step 1616: loss = 4.166195392608643\n",
            "step 1617: loss = 4.526846885681152\n",
            "step 1618: loss = 4.11013126373291\n",
            "step 1619: loss = 4.38519811630249\n",
            "step 1620: loss = 4.5267252922058105\n",
            "step 1621: loss = 4.046366214752197\n",
            "step 1622: loss = 4.538553237915039\n",
            "step 1623: loss = 4.264132499694824\n",
            "step 1624: loss = 4.082400798797607\n",
            "step 1625: loss = 4.536145210266113\n",
            "step 1626: loss = 4.428317546844482\n",
            "step 1627: loss = 4.213293552398682\n",
            "step 1628: loss = 4.137527942657471\n",
            "step 1629: loss = 4.5157856941223145\n",
            "step 1630: loss = 4.267681121826172\n",
            "step 1631: loss = 4.56204080581665\n",
            "step 1632: loss = 4.520845890045166\n",
            "step 1633: loss = 4.077309608459473\n",
            "step 1634: loss = 4.354290962219238\n",
            "step 1635: loss = 4.431744575500488\n",
            "step 1636: loss = 4.376423358917236\n",
            "step 1637: loss = 4.486570358276367\n",
            "step 1638: loss = 4.0541911125183105\n",
            "step 1639: loss = 4.147409915924072\n",
            "step 1640: loss = 4.551835060119629\n",
            "step 1641: loss = 4.457592010498047\n",
            "step 1642: loss = 4.289560317993164\n",
            "step 1643: loss = 4.333834648132324\n",
            "step 1644: loss = 4.32096004486084\n",
            "step 1645: loss = 4.651552677154541\n",
            "step 1646: loss = 4.429020881652832\n",
            "step 1647: loss = 4.4634857177734375\n",
            "step 1648: loss = 4.225882530212402\n",
            "step 1649: loss = 4.216062545776367\n",
            "step 1650: loss = 4.273863315582275\n",
            "step 1651: loss = 4.531055450439453\n",
            "step 1652: loss = 4.439275741577148\n",
            "step 1653: loss = 4.478878974914551\n",
            "step 1654: loss = 4.137642860412598\n",
            "step 1655: loss = 4.211599349975586\n",
            "step 1656: loss = 4.390377521514893\n",
            "step 1657: loss = 4.352798938751221\n",
            "step 1658: loss = 4.102502346038818\n",
            "step 1659: loss = 4.377792835235596\n",
            "step 1660: loss = 4.15432071685791\n",
            "step 1661: loss = 4.334052085876465\n",
            "step 1662: loss = 4.823315620422363\n",
            "step 1663: loss = 4.323918342590332\n",
            "step 1664: loss = 4.269154071807861\n",
            "step 1665: loss = 4.449605941772461\n",
            "step 1666: loss = 4.3442206382751465\n",
            "step 1667: loss = 4.300588130950928\n",
            "step 1668: loss = 4.424100875854492\n",
            "step 1669: loss = 4.139216423034668\n",
            "step 1670: loss = 4.212414264678955\n",
            "step 1671: loss = 4.267989158630371\n",
            "step 1672: loss = 4.382859706878662\n",
            "step 1673: loss = 4.524818420410156\n",
            "step 1674: loss = 4.603480815887451\n",
            "step 1675: loss = 4.393621444702148\n",
            "step 1676: loss = 4.351391792297363\n",
            "step 1677: loss = 4.325014114379883\n",
            "step 1678: loss = 4.619504928588867\n",
            "step 1679: loss = 4.4168009757995605\n",
            "step 1680: loss = 4.247130870819092\n",
            "step 1681: loss = 4.192953109741211\n",
            "step 1682: loss = 4.206079483032227\n",
            "step 1683: loss = 4.281650066375732\n",
            "step 1684: loss = 4.439599990844727\n",
            "step 1685: loss = 4.5224223136901855\n",
            "step 1686: loss = 4.341754913330078\n",
            "step 1687: loss = 4.494016647338867\n",
            "step 1688: loss = 4.304657936096191\n",
            "step 1689: loss = 4.248752593994141\n",
            "step 1690: loss = 4.21560525894165\n",
            "step 1691: loss = 4.276343822479248\n",
            "step 1692: loss = 4.378849029541016\n",
            "step 1693: loss = 4.289346694946289\n",
            "step 1694: loss = 4.369133472442627\n",
            "step 1695: loss = 4.251044273376465\n",
            "step 1696: loss = 4.3922119140625\n",
            "step 1697: loss = 4.505318641662598\n",
            "step 1698: loss = 4.5061187744140625\n",
            "step 1699: loss = 4.418766021728516\n",
            "step 1700: loss = 4.221249103546143\n",
            "step 1701: loss = 4.258056640625\n",
            "step 1702: loss = 4.469736576080322\n",
            "step 1703: loss = 4.5373358726501465\n",
            "step 1704: loss = 4.227845191955566\n",
            "step 1705: loss = 4.627185821533203\n",
            "step 1706: loss = 4.517587661743164\n",
            "step 1707: loss = 4.236922740936279\n",
            "step 1708: loss = 4.468299865722656\n",
            "step 1709: loss = 4.439321041107178\n",
            "step 1710: loss = 4.463205337524414\n",
            "step 1711: loss = 4.378216743469238\n",
            "step 1712: loss = 4.358150959014893\n",
            "step 1713: loss = 4.40873384475708\n",
            "step 1714: loss = 4.6207966804504395\n",
            "step 1715: loss = 4.161076545715332\n",
            "step 1716: loss = 3.996952533721924\n",
            "step 1717: loss = 4.58500862121582\n",
            "step 1718: loss = 4.412737846374512\n",
            "step 1719: loss = 4.469884395599365\n",
            "step 1720: loss = 4.282200813293457\n",
            "step 1721: loss = 4.426915645599365\n",
            "step 1722: loss = 4.200102806091309\n",
            "step 1723: loss = 4.3943328857421875\n",
            "step 1724: loss = 4.230400085449219\n",
            "step 1725: loss = 4.362244129180908\n",
            "step 1726: loss = 4.435525894165039\n",
            "step 1727: loss = 4.4866862297058105\n",
            "step 1728: loss = 4.4025187492370605\n",
            "step 1729: loss = 4.327192306518555\n",
            "step 1730: loss = 4.28748893737793\n",
            "step 1731: loss = 4.183906078338623\n",
            "step 1732: loss = 4.572930335998535\n",
            "step 1733: loss = 4.178689479827881\n",
            "step 1734: loss = 4.559455394744873\n",
            "step 1735: loss = 3.847191095352173\n",
            "step 1736: loss = 4.433094024658203\n",
            "step 1737: loss = 4.230569362640381\n",
            "step 1738: loss = 4.268126010894775\n",
            "step 1739: loss = 4.450619220733643\n",
            "step 1740: loss = 4.34303617477417\n",
            "step 1741: loss = 4.075304985046387\n",
            "step 1742: loss = 4.5211029052734375\n",
            "step 1743: loss = 4.590947151184082\n",
            "step 1744: loss = 4.48939323425293\n",
            "step 1745: loss = 4.109134674072266\n",
            "step 1746: loss = 4.343973159790039\n",
            "step 1747: loss = 4.312328815460205\n",
            "step 1748: loss = 4.243038654327393\n",
            "step 1749: loss = 4.468624114990234\n",
            "step 1750: loss = 4.443951606750488\n",
            "step 1751: loss = 4.137921333312988\n",
            "step 1752: loss = 4.27073335647583\n",
            "step 1753: loss = 4.620900630950928\n",
            "step 1754: loss = 4.441299915313721\n",
            "step 1755: loss = 4.3709516525268555\n",
            "step 1756: loss = 4.090982437133789\n",
            "step 1757: loss = 4.172069549560547\n",
            "step 1758: loss = 4.415040969848633\n",
            "step 1759: loss = 4.366031169891357\n",
            "step 1760: loss = 4.151360034942627\n",
            "step 1761: loss = 4.320377826690674\n",
            "step 1762: loss = 4.409671306610107\n",
            "step 1763: loss = 4.098714351654053\n",
            "step 1764: loss = 4.30342435836792\n",
            "step 1765: loss = 4.381359100341797\n",
            "step 1766: loss = 4.444695949554443\n",
            "step 1767: loss = 4.255169868469238\n",
            "step 1768: loss = 4.165353298187256\n",
            "step 1769: loss = 4.218991279602051\n",
            "step 1770: loss = 4.21729040145874\n",
            "step 1771: loss = 4.121531009674072\n",
            "step 1772: loss = 4.72675895690918\n",
            "step 1773: loss = 4.0434489250183105\n",
            "step 1774: loss = 4.215195178985596\n",
            "step 1775: loss = 4.127018928527832\n",
            "step 1776: loss = 4.6245317459106445\n",
            "step 1777: loss = 4.0868330001831055\n",
            "step 1778: loss = 4.261099338531494\n",
            "step 1779: loss = 4.235715389251709\n",
            "step 1780: loss = 4.308907508850098\n",
            "step 1781: loss = 4.202180862426758\n",
            "step 1782: loss = 4.1321702003479\n",
            "step 1783: loss = 4.351589202880859\n",
            "step 1784: loss = 4.336672306060791\n",
            "step 1785: loss = 4.393304347991943\n",
            "step 1786: loss = 4.525411128997803\n",
            "step 1787: loss = 4.2908711433410645\n",
            "step 1788: loss = 4.375635147094727\n",
            "step 1789: loss = 4.582936763763428\n",
            "step 1790: loss = 4.415688514709473\n",
            "step 1791: loss = 4.122981548309326\n",
            "step 1792: loss = 4.085577964782715\n",
            "step 1793: loss = 4.3615031242370605\n",
            "step 1794: loss = 4.5720648765563965\n",
            "step 1795: loss = 4.427410125732422\n",
            "step 1796: loss = 4.2722063064575195\n",
            "step 1797: loss = 4.257103443145752\n",
            "step 1798: loss = 4.424766540527344\n",
            "step 1799: loss = 4.054172039031982\n",
            "step 1800: loss = 4.45540189743042\n",
            "step 1801: loss = 4.320383071899414\n",
            "step 1802: loss = 4.395480632781982\n",
            "step 1803: loss = 4.308078765869141\n",
            "step 1804: loss = 4.152358055114746\n",
            "step 1805: loss = 4.4902472496032715\n",
            "step 1806: loss = 3.957566261291504\n",
            "step 1807: loss = 3.9063074588775635\n",
            "step 1808: loss = 4.235730171203613\n",
            "step 1809: loss = 4.005635738372803\n",
            "step 1810: loss = 4.175030708312988\n",
            "step 1811: loss = 4.010097503662109\n",
            "step 1812: loss = 4.35460090637207\n",
            "step 1813: loss = 4.550899028778076\n",
            "step 1814: loss = 4.583616256713867\n",
            "step 1815: loss = 4.281379699707031\n",
            "step 1816: loss = 4.277132987976074\n",
            "step 1817: loss = 3.9675581455230713\n",
            "step 1818: loss = 4.414553165435791\n",
            "step 1819: loss = 4.283751010894775\n",
            "step 1820: loss = 4.365270137786865\n",
            "step 1821: loss = 4.5706095695495605\n",
            "step 1822: loss = 4.033687114715576\n",
            "step 1823: loss = 4.185206413269043\n",
            "step 1824: loss = 4.191745281219482\n",
            "step 1825: loss = 3.863058567047119\n",
            "step 1826: loss = 4.666469573974609\n",
            "step 1827: loss = 4.189087390899658\n",
            "step 1828: loss = 4.367846488952637\n",
            "step 1829: loss = 4.561482906341553\n",
            "step 1830: loss = 4.195929527282715\n",
            "step 1831: loss = 4.021202087402344\n",
            "step 1832: loss = 4.232689380645752\n",
            "step 1833: loss = 4.259486675262451\n",
            "step 1834: loss = 4.241853713989258\n",
            "step 1835: loss = 4.395419120788574\n",
            "step 1836: loss = 4.349047660827637\n",
            "step 1837: loss = 4.492134094238281\n",
            "step 1838: loss = 4.6405348777771\n",
            "step 1839: loss = 4.185328483581543\n",
            "step 1840: loss = 4.685162544250488\n",
            "step 1841: loss = 4.136373043060303\n",
            "step 1842: loss = 4.147152423858643\n",
            "step 1843: loss = 4.212952613830566\n",
            "step 1844: loss = 4.113641262054443\n",
            "step 1845: loss = 4.393501281738281\n",
            "step 1846: loss = 4.2129106521606445\n",
            "step 1847: loss = 4.430384635925293\n",
            "step 1848: loss = 4.051863193511963\n",
            "step 1849: loss = 4.2226033210754395\n",
            "step 1850: loss = 4.206453800201416\n",
            "step 1851: loss = 4.106349468231201\n",
            "step 1852: loss = 4.34688663482666\n",
            "step 1853: loss = 3.849630117416382\n",
            "step 1854: loss = 4.496455192565918\n",
            "step 1855: loss = 4.430368900299072\n",
            "step 1856: loss = 4.497361660003662\n",
            "step 1857: loss = 4.248204708099365\n",
            "step 1858: loss = 4.216733455657959\n",
            "step 1859: loss = 4.3928093910217285\n",
            "step 1860: loss = 4.3438801765441895\n",
            "step 1861: loss = 4.3053741455078125\n",
            "step 1862: loss = 4.349217414855957\n",
            "step 1863: loss = 4.328103065490723\n",
            "step 1864: loss = 4.268545150756836\n",
            "step 1865: loss = 3.8294198513031006\n",
            "step 1866: loss = 4.372949600219727\n",
            "step 1867: loss = 4.4827423095703125\n",
            "step 1868: loss = 4.328912258148193\n",
            "step 1869: loss = 4.021551132202148\n",
            "step 1870: loss = 4.099076747894287\n",
            "step 1871: loss = 4.061525344848633\n",
            "step 1872: loss = 4.550786018371582\n",
            "step 1873: loss = 4.216423034667969\n",
            "step 1874: loss = 4.665740489959717\n",
            "step 1875: loss = 4.313014984130859\n",
            "step 1876: loss = 4.355344772338867\n",
            "step 1877: loss = 3.9713709354400635\n",
            "step 1878: loss = 4.41952657699585\n",
            "step 1879: loss = 4.406362056732178\n",
            "step 1880: loss = 4.406183242797852\n",
            "step 1881: loss = 4.176939964294434\n",
            "step 1882: loss = 4.255887508392334\n",
            "step 1883: loss = 4.531068801879883\n",
            "step 1884: loss = 4.132413387298584\n",
            "step 1885: loss = 4.215317726135254\n",
            "step 1886: loss = 4.151082992553711\n",
            "step 1887: loss = 4.157063961029053\n",
            "step 1888: loss = 4.306747913360596\n",
            "step 1889: loss = 4.187429904937744\n",
            "step 1890: loss = 3.8864126205444336\n",
            "step 1891: loss = 4.222809314727783\n",
            "step 1892: loss = 4.203389644622803\n",
            "step 1893: loss = 4.472969055175781\n",
            "step 1894: loss = 4.305343151092529\n",
            "step 1895: loss = 4.152359962463379\n",
            "step 1896: loss = 4.213187217712402\n",
            "step 1897: loss = 4.5836663246154785\n",
            "step 1898: loss = 4.380810737609863\n",
            "step 1899: loss = 4.346677780151367\n",
            "step 1900: loss = 4.537975311279297\n",
            "step 1901: loss = 4.465194225311279\n",
            "step 1902: loss = 4.220095157623291\n",
            "step 1903: loss = 4.08343505859375\n",
            "step 1904: loss = 4.247566223144531\n",
            "step 1905: loss = 4.089081287384033\n",
            "step 1906: loss = 4.302323341369629\n",
            "step 1907: loss = 4.162390232086182\n",
            "step 1908: loss = 4.2473907470703125\n",
            "step 1909: loss = 4.341037750244141\n",
            "step 1910: loss = 4.457799911499023\n",
            "step 1911: loss = 4.146122455596924\n",
            "step 1912: loss = 4.214536190032959\n",
            "step 1913: loss = 4.5251383781433105\n",
            "step 1914: loss = 4.30280876159668\n",
            "step 1915: loss = 4.416476726531982\n",
            "step 1916: loss = 4.5416388511657715\n",
            "step 1917: loss = 4.0755085945129395\n",
            "step 1918: loss = 4.488358974456787\n",
            "step 1919: loss = 4.11973762512207\n",
            "step 1920: loss = 3.9927632808685303\n",
            "step 1921: loss = 4.1688947677612305\n",
            "step 1922: loss = 4.1871161460876465\n",
            "step 1923: loss = 3.818830966949463\n",
            "step 1924: loss = 4.041904926300049\n",
            "step 1925: loss = 4.612781047821045\n",
            "step 1926: loss = 4.282923221588135\n",
            "step 1927: loss = 4.298306465148926\n",
            "step 1928: loss = 3.9693663120269775\n",
            "step 1929: loss = 4.278482437133789\n",
            "step 1930: loss = 4.412660598754883\n",
            "step 1931: loss = 4.089137077331543\n",
            "step 1932: loss = 4.521861553192139\n",
            "step 1933: loss = 4.2498555183410645\n",
            "step 1934: loss = 4.490493297576904\n",
            "step 1935: loss = 4.205307960510254\n",
            "step 1936: loss = 4.521098613739014\n",
            "step 1937: loss = 4.254979610443115\n",
            "step 1938: loss = 4.174811363220215\n",
            "step 1939: loss = 4.152876853942871\n",
            "step 1940: loss = 4.073410987854004\n",
            "step 1941: loss = 4.188403606414795\n",
            "step 1942: loss = 4.158500671386719\n",
            "step 1943: loss = 4.275369644165039\n",
            "step 1944: loss = 3.952676296234131\n",
            "step 1945: loss = 4.47636079788208\n",
            "step 1946: loss = 3.8909292221069336\n",
            "step 1947: loss = 4.110287189483643\n",
            "step 1948: loss = 4.313976764678955\n",
            "step 1949: loss = 4.178062915802002\n",
            "step 1950: loss = 4.256988048553467\n",
            "step 1951: loss = 4.343632221221924\n",
            "step 1952: loss = 4.235016345977783\n",
            "step 1953: loss = 4.221420764923096\n",
            "step 1954: loss = 4.117422103881836\n",
            "step 1955: loss = 4.426497936248779\n",
            "step 1956: loss = 4.082489490509033\n",
            "step 1957: loss = 4.033608913421631\n",
            "step 1958: loss = 4.124885559082031\n",
            "step 1959: loss = 4.0714430809021\n",
            "step 1960: loss = 4.120993137359619\n",
            "step 1961: loss = 3.7703511714935303\n",
            "step 1962: loss = 4.337646007537842\n",
            "step 1963: loss = 3.8218438625335693\n",
            "step 1964: loss = 4.146090984344482\n",
            "step 1965: loss = 4.309327602386475\n",
            "step 1966: loss = 4.25813627243042\n",
            "step 1967: loss = 4.018095016479492\n",
            "step 1968: loss = 4.0853424072265625\n",
            "step 1969: loss = 4.109699249267578\n",
            "step 1970: loss = 4.378335952758789\n",
            "step 1971: loss = 3.918149948120117\n",
            "step 1972: loss = 4.173956871032715\n",
            "step 1973: loss = 4.417331218719482\n",
            "step 1974: loss = 4.284745216369629\n",
            "step 1975: loss = 4.266882419586182\n",
            "step 1976: loss = 4.162214756011963\n",
            "step 1977: loss = 4.26943302154541\n",
            "step 1978: loss = 3.942089796066284\n",
            "step 1979: loss = 4.595266342163086\n",
            "step 1980: loss = 4.554049491882324\n",
            "step 1981: loss = 4.2068986892700195\n",
            "step 1982: loss = 4.234225273132324\n",
            "step 1983: loss = 4.120884895324707\n",
            "step 1984: loss = 4.135833740234375\n",
            "step 1985: loss = 4.1289591789245605\n",
            "step 1986: loss = 4.202728748321533\n",
            "step 1987: loss = 4.436990737915039\n",
            "step 1988: loss = 4.001618385314941\n",
            "step 1989: loss = 4.050023078918457\n",
            "step 1990: loss = 4.454160213470459\n",
            "step 1991: loss = 3.9869000911712646\n",
            "step 1992: loss = 4.195125102996826\n",
            "step 1993: loss = 4.127781391143799\n",
            "step 1994: loss = 4.27354621887207\n",
            "step 1995: loss = 4.648568153381348\n",
            "step 1996: loss = 4.6928019523620605\n",
            "step 1997: loss = 4.052019119262695\n",
            "step 1998: loss = 3.9768073558807373\n",
            "step 1999: loss = 4.419260501861572\n",
            "step 2000: loss = 4.230288505554199\n",
            "step 2001: loss = 4.300877094268799\n",
            "step 2002: loss = 4.198628902435303\n",
            "step 2003: loss = 4.55336856842041\n",
            "step 2004: loss = 4.272518157958984\n",
            "step 2005: loss = 4.039276123046875\n",
            "step 2006: loss = 3.906795024871826\n",
            "step 2007: loss = 4.302668571472168\n",
            "step 2008: loss = 4.171323776245117\n",
            "step 2009: loss = 4.1776442527771\n",
            "step 2010: loss = 4.158119201660156\n",
            "step 2011: loss = 4.195186614990234\n",
            "step 2012: loss = 4.136096000671387\n",
            "step 2013: loss = 4.129223346710205\n",
            "step 2014: loss = 4.334747791290283\n",
            "step 2015: loss = 4.173186779022217\n",
            "step 2016: loss = 4.053895473480225\n",
            "step 2017: loss = 4.037686824798584\n",
            "step 2018: loss = 4.470895290374756\n",
            "step 2019: loss = 3.9616472721099854\n",
            "step 2020: loss = 3.973374366760254\n",
            "step 2021: loss = 4.418774604797363\n",
            "step 2022: loss = 4.27898645401001\n",
            "step 2023: loss = 4.200482368469238\n",
            "step 2024: loss = 4.2431416511535645\n",
            "step 2025: loss = 4.329868316650391\n",
            "step 2026: loss = 4.255282878875732\n",
            "step 2027: loss = 4.24269437789917\n",
            "step 2028: loss = 3.789536237716675\n",
            "step 2029: loss = 4.243317127227783\n",
            "step 2030: loss = 4.273167133331299\n",
            "step 2031: loss = 3.9471850395202637\n",
            "step 2032: loss = 4.173722267150879\n",
            "step 2033: loss = 4.302688121795654\n",
            "step 2034: loss = 4.440290927886963\n",
            "step 2035: loss = 4.177815914154053\n",
            "step 2036: loss = 4.162074565887451\n",
            "step 2037: loss = 4.329686164855957\n",
            "step 2038: loss = 4.078516960144043\n",
            "step 2039: loss = 4.24766731262207\n",
            "step 2040: loss = 3.7712457180023193\n",
            "step 2041: loss = 3.999769449234009\n",
            "step 2042: loss = 4.506718635559082\n",
            "step 2043: loss = 4.094341278076172\n",
            "step 2044: loss = 4.162858486175537\n",
            "step 2045: loss = 4.123982906341553\n",
            "step 2046: loss = 4.079526424407959\n",
            "step 2047: loss = 4.208096027374268\n",
            "step 2048: loss = 4.14646577835083\n",
            "step 2049: loss = 4.085204124450684\n",
            "step 2050: loss = 4.290966033935547\n",
            "step 2051: loss = 3.9287984371185303\n",
            "step 2052: loss = 4.3977580070495605\n",
            "step 2053: loss = 4.210931777954102\n",
            "step 2054: loss = 3.9814298152923584\n",
            "step 2055: loss = 4.272299289703369\n",
            "step 2056: loss = 4.312102317810059\n",
            "step 2057: loss = 4.324934005737305\n",
            "step 2058: loss = 4.521693706512451\n",
            "step 2059: loss = 4.225162029266357\n",
            "step 2060: loss = 4.217679977416992\n",
            "step 2061: loss = 4.119078636169434\n",
            "step 2062: loss = 4.0493550300598145\n",
            "step 2063: loss = 4.111730575561523\n",
            "step 2064: loss = 4.42007303237915\n",
            "step 2065: loss = 3.9841644763946533\n",
            "step 2066: loss = 4.306065559387207\n",
            "step 2067: loss = 4.207403659820557\n",
            "step 2068: loss = 3.6444170475006104\n",
            "step 2069: loss = 4.04215669631958\n",
            "step 2070: loss = 4.07990026473999\n",
            "step 2071: loss = 4.189309597015381\n",
            "step 2072: loss = 4.200894832611084\n",
            "step 2073: loss = 4.130484580993652\n",
            "step 2074: loss = 4.3779616355896\n",
            "step 2075: loss = 4.296300888061523\n",
            "step 2076: loss = 4.397737503051758\n",
            "step 2077: loss = 4.182862281799316\n",
            "step 2078: loss = 3.927880048751831\n",
            "step 2079: loss = 4.191372394561768\n",
            "step 2080: loss = 4.599280834197998\n",
            "step 2081: loss = 4.149688243865967\n",
            "step 2082: loss = 4.2305498123168945\n",
            "step 2083: loss = 4.142786502838135\n",
            "step 2084: loss = 4.2672438621521\n",
            "step 2085: loss = 4.137678623199463\n",
            "step 2086: loss = 4.329030513763428\n",
            "step 2087: loss = 4.314168930053711\n",
            "step 2088: loss = 4.402153968811035\n",
            "step 2089: loss = 4.063684940338135\n",
            "step 2090: loss = 4.180058002471924\n",
            "step 2091: loss = 4.3061676025390625\n",
            "step 2092: loss = 4.151846885681152\n",
            "step 2093: loss = 4.099957466125488\n",
            "step 2094: loss = 4.145220756530762\n",
            "step 2095: loss = 4.030607223510742\n",
            "step 2096: loss = 4.404601573944092\n",
            "step 2097: loss = 3.942229747772217\n",
            "step 2098: loss = 4.266005992889404\n",
            "step 2099: loss = 4.161033630371094\n",
            "step 2100: loss = 4.401919364929199\n",
            "step 2101: loss = 4.01901912689209\n",
            "step 2102: loss = 4.160042762756348\n",
            "step 2103: loss = 4.155464172363281\n",
            "step 2104: loss = 4.488935470581055\n",
            "step 2105: loss = 4.027089595794678\n",
            "step 2106: loss = 3.951228618621826\n",
            "step 2107: loss = 4.166147232055664\n",
            "step 2108: loss = 4.287452220916748\n",
            "step 2109: loss = 4.458085060119629\n",
            "step 2110: loss = 4.187097549438477\n",
            "step 2111: loss = 4.324231147766113\n",
            "step 2112: loss = 4.11419153213501\n",
            "step 2113: loss = 4.375110626220703\n",
            "step 2114: loss = 4.461886405944824\n",
            "step 2115: loss = 4.097477436065674\n",
            "step 2116: loss = 4.370028018951416\n",
            "step 2117: loss = 3.9571194648742676\n",
            "step 2118: loss = 4.1065802574157715\n",
            "step 2119: loss = 4.255753993988037\n",
            "step 2120: loss = 4.056748867034912\n",
            "step 2121: loss = 4.1212992668151855\n",
            "step 2122: loss = 3.932756185531616\n",
            "step 2123: loss = 4.011015892028809\n",
            "step 2124: loss = 4.044317245483398\n",
            "step 2125: loss = 4.127602577209473\n",
            "step 2126: loss = 3.9990904331207275\n",
            "step 2127: loss = 4.641133785247803\n",
            "step 2128: loss = 4.137752056121826\n",
            "step 2129: loss = 4.131437301635742\n",
            "step 2130: loss = 4.162137031555176\n",
            "step 2131: loss = 4.210325717926025\n",
            "step 2132: loss = 4.136765003204346\n",
            "step 2133: loss = 4.497049331665039\n",
            "step 2134: loss = 4.281340599060059\n",
            "step 2135: loss = 4.048418998718262\n",
            "step 2136: loss = 4.1613006591796875\n",
            "step 2137: loss = 4.338090896606445\n",
            "step 2138: loss = 4.201498031616211\n",
            "step 2139: loss = 4.264937877655029\n",
            "step 2140: loss = 4.397322654724121\n",
            "step 2141: loss = 3.8327395915985107\n",
            "step 2142: loss = 3.9639358520507812\n",
            "step 2143: loss = 3.980098247528076\n",
            "step 2144: loss = 4.358824729919434\n",
            "step 2145: loss = 4.0140767097473145\n",
            "step 2146: loss = 4.277921199798584\n",
            "step 2147: loss = 4.1537370681762695\n",
            "step 2148: loss = 4.152737617492676\n",
            "step 2149: loss = 3.824037551879883\n",
            "step 2150: loss = 4.1089606285095215\n",
            "step 2151: loss = 4.018320560455322\n",
            "step 2152: loss = 4.3360514640808105\n",
            "step 2153: loss = 4.396613597869873\n",
            "step 2154: loss = 4.522119045257568\n",
            "step 2155: loss = 4.0728044509887695\n",
            "step 2156: loss = 3.980703115463257\n",
            "step 2157: loss = 4.589416027069092\n",
            "step 2158: loss = 4.221921920776367\n",
            "step 2159: loss = 4.189703464508057\n",
            "step 2160: loss = 3.966113567352295\n",
            "step 2161: loss = 4.069714069366455\n",
            "step 2162: loss = 4.055795669555664\n",
            "step 2163: loss = 3.9993157386779785\n",
            "step 2164: loss = 4.1384429931640625\n",
            "step 2165: loss = 3.9237382411956787\n",
            "step 2166: loss = 3.828153371810913\n",
            "step 2167: loss = 4.221761226654053\n",
            "step 2168: loss = 4.164691925048828\n",
            "step 2169: loss = 3.9599945545196533\n",
            "step 2170: loss = 4.200193405151367\n",
            "step 2171: loss = 4.315735340118408\n",
            "step 2172: loss = 4.100193500518799\n",
            "step 2173: loss = 3.9168829917907715\n",
            "step 2174: loss = 4.163806915283203\n",
            "step 2175: loss = 4.145113945007324\n",
            "step 2176: loss = 3.989382743835449\n",
            "step 2177: loss = 4.22733211517334\n",
            "step 2178: loss = 4.131021022796631\n",
            "step 2179: loss = 4.1092753410339355\n",
            "step 2180: loss = 4.147686004638672\n",
            "step 2181: loss = 4.364353179931641\n",
            "step 2182: loss = 3.8304903507232666\n",
            "step 2183: loss = 4.242537975311279\n",
            "step 2184: loss = 3.950735330581665\n",
            "step 2185: loss = 3.9082868099212646\n",
            "step 2186: loss = 4.185124397277832\n",
            "step 2187: loss = 4.221224308013916\n",
            "step 2188: loss = 4.308632850646973\n",
            "step 2189: loss = 4.334322452545166\n",
            "step 2190: loss = 3.977905511856079\n",
            "step 2191: loss = 3.9873363971710205\n",
            "step 2192: loss = 4.246929168701172\n",
            "step 2193: loss = 4.275515079498291\n",
            "step 2194: loss = 4.410678863525391\n",
            "step 2195: loss = 4.179838180541992\n",
            "step 2196: loss = 4.360865592956543\n",
            "step 2197: loss = 4.245794773101807\n",
            "step 2198: loss = 3.6865851879119873\n",
            "step 2199: loss = 4.394614219665527\n",
            "step 2200: loss = 4.191580295562744\n",
            "step 2201: loss = 4.265806674957275\n",
            "step 2202: loss = 4.027976989746094\n",
            "step 2203: loss = 4.116668701171875\n",
            "step 2204: loss = 4.459329605102539\n",
            "step 2205: loss = 4.283211708068848\n",
            "step 2206: loss = 4.115262031555176\n",
            "step 2207: loss = 4.358977317810059\n",
            "step 2208: loss = 3.9297235012054443\n",
            "step 2209: loss = 4.363669395446777\n",
            "step 2210: loss = 4.6915082931518555\n",
            "step 2211: loss = 4.267754077911377\n",
            "step 2212: loss = 4.339278697967529\n",
            "step 2213: loss = 4.304955005645752\n",
            "step 2214: loss = 4.028163909912109\n",
            "step 2215: loss = 4.319129943847656\n",
            "step 2216: loss = 3.990053653717041\n",
            "step 2217: loss = 4.17462158203125\n",
            "step 2218: loss = 3.910620927810669\n",
            "step 2219: loss = 4.235236644744873\n",
            "step 2220: loss = 4.149269104003906\n",
            "step 2221: loss = 4.1687092781066895\n",
            "step 2222: loss = 3.9412639141082764\n",
            "step 2223: loss = 4.085943222045898\n",
            "step 2224: loss = 4.20384407043457\n",
            "step 2225: loss = 3.901960611343384\n",
            "step 2226: loss = 4.109004497528076\n",
            "step 2227: loss = 3.8610706329345703\n",
            "step 2228: loss = 4.076827049255371\n",
            "step 2229: loss = 4.210962772369385\n",
            "step 2230: loss = 3.9316864013671875\n",
            "step 2231: loss = 3.9380369186401367\n",
            "step 2232: loss = 4.386941909790039\n",
            "step 2233: loss = 4.310790538787842\n",
            "step 2234: loss = 4.282686233520508\n",
            "step 2235: loss = 4.2212629318237305\n",
            "step 2236: loss = 4.006128311157227\n",
            "step 2237: loss = 4.1443023681640625\n",
            "step 2238: loss = 4.195980072021484\n",
            "step 2239: loss = 4.130307197570801\n",
            "step 2240: loss = 4.067506790161133\n",
            "step 2241: loss = 4.215373992919922\n",
            "step 2242: loss = 4.310274124145508\n",
            "step 2243: loss = 3.8974316120147705\n",
            "step 2244: loss = 4.344874382019043\n",
            "step 2245: loss = 4.2880377769470215\n",
            "step 2246: loss = 3.949705123901367\n",
            "step 2247: loss = 4.589674949645996\n",
            "step 2248: loss = 3.9318602085113525\n",
            "step 2249: loss = 3.9965107440948486\n",
            "step 2250: loss = 3.842724323272705\n",
            "step 2251: loss = 3.925065755844116\n",
            "step 2252: loss = 3.9709765911102295\n",
            "step 2253: loss = 3.992432117462158\n",
            "step 2254: loss = 3.754310131072998\n",
            "step 2255: loss = 4.3305559158325195\n",
            "step 2256: loss = 4.317471981048584\n",
            "step 2257: loss = 3.5548877716064453\n",
            "step 2258: loss = 4.601500988006592\n",
            "step 2259: loss = 4.325819492340088\n",
            "step 2260: loss = 4.344853401184082\n",
            "step 2261: loss = 4.060459613800049\n",
            "step 2262: loss = 4.211889743804932\n",
            "step 2263: loss = 4.038149356842041\n",
            "step 2264: loss = 4.389675140380859\n",
            "step 2265: loss = 4.316862106323242\n",
            "step 2266: loss = 4.341525554656982\n",
            "step 2267: loss = 3.804231882095337\n",
            "step 2268: loss = 3.855940103530884\n",
            "step 2269: loss = 4.047704219818115\n",
            "step 2270: loss = 4.1697678565979\n",
            "step 2271: loss = 3.8520219326019287\n",
            "step 2272: loss = 4.021443843841553\n",
            "step 2273: loss = 4.1614580154418945\n",
            "step 2274: loss = 4.532221794128418\n",
            "step 2275: loss = 4.002147674560547\n",
            "step 2276: loss = 4.036005020141602\n",
            "step 2277: loss = 4.027670860290527\n",
            "step 2278: loss = 4.068713665008545\n",
            "step 2279: loss = 4.3625640869140625\n",
            "step 2280: loss = 4.1919074058532715\n",
            "step 2281: loss = 4.113707065582275\n",
            "step 2282: loss = 3.7534091472625732\n",
            "step 2283: loss = 3.8180437088012695\n",
            "step 2284: loss = 3.997622489929199\n",
            "step 2285: loss = 3.791618824005127\n",
            "step 2286: loss = 4.11049222946167\n",
            "step 2287: loss = 4.293548107147217\n",
            "step 2288: loss = 3.8113443851470947\n",
            "step 2289: loss = 4.285992622375488\n",
            "step 2290: loss = 4.253098964691162\n",
            "step 2291: loss = 4.041779041290283\n",
            "step 2292: loss = 4.144999980926514\n",
            "step 2293: loss = 4.025181293487549\n",
            "step 2294: loss = 4.261127471923828\n",
            "step 2295: loss = 3.934844970703125\n",
            "step 2296: loss = 4.28389310836792\n",
            "step 2297: loss = 3.737459421157837\n",
            "step 2298: loss = 4.288995265960693\n",
            "step 2299: loss = 4.063211441040039\n",
            "step 2300: loss = 4.068530082702637\n",
            "step 2301: loss = 4.13626766204834\n",
            "step 2302: loss = 4.229294776916504\n",
            "step 2303: loss = 3.7870659828186035\n",
            "step 2304: loss = 4.49854850769043\n",
            "step 2305: loss = 4.102346897125244\n",
            "step 2306: loss = 3.9825990200042725\n",
            "step 2307: loss = 4.359110355377197\n",
            "step 2308: loss = 4.143167495727539\n",
            "step 2309: loss = 4.003848075866699\n",
            "step 2310: loss = 4.021371841430664\n",
            "step 2311: loss = 4.4297709465026855\n",
            "step 2312: loss = 4.3721394538879395\n",
            "step 2313: loss = 4.310610294342041\n",
            "step 2314: loss = 3.8563456535339355\n",
            "step 2315: loss = 3.840127468109131\n",
            "step 2316: loss = 4.430954933166504\n",
            "step 2317: loss = 3.953031539916992\n",
            "step 2318: loss = 4.271022319793701\n",
            "step 2319: loss = 4.030369281768799\n",
            "step 2320: loss = 3.8991172313690186\n",
            "step 2321: loss = 4.062450408935547\n",
            "step 2322: loss = 4.364145755767822\n",
            "step 2323: loss = 3.891218662261963\n",
            "step 2324: loss = 4.311069965362549\n",
            "step 2325: loss = 3.942945718765259\n",
            "step 2326: loss = 4.017655849456787\n",
            "step 2327: loss = 3.878351926803589\n",
            "step 2328: loss = 4.3122735023498535\n",
            "step 2329: loss = 4.197362422943115\n",
            "step 2330: loss = 3.9931297302246094\n",
            "step 2331: loss = 4.014217376708984\n",
            "step 2332: loss = 4.300187587738037\n",
            "step 2333: loss = 4.273522853851318\n",
            "step 2334: loss = 4.345601558685303\n",
            "step 2335: loss = 4.291169166564941\n",
            "step 2336: loss = 4.076448440551758\n",
            "step 2337: loss = 4.0522637367248535\n",
            "step 2338: loss = 4.354581832885742\n",
            "step 2339: loss = 3.925922155380249\n",
            "step 2340: loss = 4.245339393615723\n",
            "step 2341: loss = 4.223582744598389\n",
            "step 2342: loss = 3.9316565990448\n",
            "step 2343: loss = 4.276084899902344\n",
            "step 2344: loss = 3.8017795085906982\n",
            "step 2345: loss = 4.086964130401611\n",
            "step 2346: loss = 4.0760698318481445\n",
            "step 2347: loss = 4.020557403564453\n",
            "step 2348: loss = 3.947676658630371\n",
            "step 2349: loss = 3.8724424839019775\n",
            "step 2350: loss = 4.21911096572876\n",
            "step 2351: loss = 4.048262596130371\n",
            "step 2352: loss = 3.6772842407226562\n",
            "step 2353: loss = 4.27277135848999\n",
            "step 2354: loss = 3.9564476013183594\n",
            "step 2355: loss = 4.002439498901367\n",
            "step 2356: loss = 3.8110451698303223\n",
            "step 2357: loss = 3.746347665786743\n",
            "step 2358: loss = 3.9442250728607178\n",
            "step 2359: loss = 4.3697614669799805\n",
            "step 2360: loss = 4.208950042724609\n",
            "step 2361: loss = 3.910142183303833\n",
            "step 2362: loss = 4.273931503295898\n",
            "step 2363: loss = 3.924166202545166\n",
            "step 2364: loss = 3.8486216068267822\n",
            "step 2365: loss = 4.195952892303467\n",
            "step 2366: loss = 4.149123668670654\n",
            "step 2367: loss = 4.037468433380127\n",
            "step 2368: loss = 3.7259116172790527\n",
            "step 2369: loss = 4.054415702819824\n",
            "step 2370: loss = 4.022738456726074\n",
            "step 2371: loss = 3.9024157524108887\n",
            "step 2372: loss = 3.992034673690796\n",
            "step 2373: loss = 4.042243957519531\n",
            "step 2374: loss = 3.725285053253174\n",
            "step 2375: loss = 4.193546295166016\n",
            "step 2376: loss = 3.9567832946777344\n",
            "step 2377: loss = 4.309119701385498\n",
            "step 2378: loss = 3.99589204788208\n",
            "step 2379: loss = 3.9360103607177734\n",
            "step 2380: loss = 3.866438388824463\n",
            "step 2381: loss = 3.980053186416626\n",
            "step 2382: loss = 4.308650016784668\n",
            "step 2383: loss = 4.0007734298706055\n",
            "step 2384: loss = 4.152914047241211\n",
            "step 2385: loss = 4.285966873168945\n",
            "step 2386: loss = 4.143355846405029\n",
            "step 2387: loss = 4.239896774291992\n",
            "step 2388: loss = 3.9226460456848145\n",
            "step 2389: loss = 4.0891828536987305\n",
            "step 2390: loss = 3.6936793327331543\n",
            "step 2391: loss = 4.2705206871032715\n",
            "step 2392: loss = 4.1125030517578125\n",
            "step 2393: loss = 3.922877550125122\n",
            "step 2394: loss = 4.072309494018555\n",
            "step 2395: loss = 3.619715690612793\n",
            "step 2396: loss = 4.0066914558410645\n",
            "step 2397: loss = 4.5004167556762695\n",
            "step 2398: loss = 4.266454219818115\n",
            "step 2399: loss = 4.125973224639893\n",
            "step 2400: loss = 4.350334167480469\n",
            "step 2401: loss = 4.357468128204346\n",
            "step 2402: loss = 3.9813904762268066\n",
            "step 2403: loss = 4.31356954574585\n",
            "step 2404: loss = 3.8080146312713623\n",
            "step 2405: loss = 3.8646769523620605\n",
            "step 2406: loss = 4.121686935424805\n",
            "step 2407: loss = 4.018890857696533\n",
            "step 2408: loss = 4.071059703826904\n",
            "step 2409: loss = 4.073019504547119\n",
            "step 2410: loss = 3.9510936737060547\n",
            "step 2411: loss = 3.845432758331299\n",
            "step 2412: loss = 4.247278213500977\n",
            "step 2413: loss = 4.400389671325684\n",
            "step 2414: loss = 3.9866743087768555\n",
            "step 2415: loss = 3.868077516555786\n",
            "step 2416: loss = 3.9956088066101074\n",
            "step 2417: loss = 3.840771436691284\n",
            "step 2418: loss = 4.080201625823975\n",
            "step 2419: loss = 4.174310207366943\n",
            "step 2420: loss = 3.853015899658203\n",
            "step 2421: loss = 4.2309675216674805\n",
            "step 2422: loss = 4.168878555297852\n",
            "step 2423: loss = 4.177146911621094\n",
            "step 2424: loss = 3.720541000366211\n",
            "step 2425: loss = 3.9822206497192383\n",
            "step 2426: loss = 3.91294264793396\n",
            "step 2427: loss = 4.193914890289307\n",
            "step 2428: loss = 4.3642168045043945\n",
            "step 2429: loss = 4.2384772300720215\n",
            "step 2430: loss = 4.18136739730835\n",
            "step 2431: loss = 3.729703187942505\n",
            "step 2432: loss = 4.375952243804932\n",
            "step 2433: loss = 3.8054018020629883\n",
            "step 2434: loss = 4.121949672698975\n",
            "step 2435: loss = 4.685942649841309\n",
            "step 2436: loss = 4.064930438995361\n",
            "step 2437: loss = 4.376284599304199\n",
            "step 2438: loss = 4.193194389343262\n",
            "step 2439: loss = 3.824693441390991\n",
            "step 2440: loss = 3.9366650581359863\n",
            "step 2441: loss = 4.23367166519165\n",
            "step 2442: loss = 4.26837158203125\n",
            "step 2443: loss = 3.907057046890259\n",
            "step 2444: loss = 3.9992966651916504\n",
            "step 2445: loss = 4.118788242340088\n",
            "step 2446: loss = 3.951916217803955\n",
            "step 2447: loss = 3.990471601486206\n",
            "step 2448: loss = 4.095842361450195\n",
            "step 2449: loss = 4.103146553039551\n",
            "step 2450: loss = 3.840331554412842\n",
            "step 2451: loss = 3.706913471221924\n",
            "step 2452: loss = 3.9783294200897217\n",
            "step 2453: loss = 3.993659019470215\n",
            "step 2454: loss = 4.004484176635742\n",
            "step 2455: loss = 3.9668240547180176\n",
            "step 2456: loss = 4.342000484466553\n",
            "step 2457: loss = 3.805959463119507\n",
            "step 2458: loss = 4.154447555541992\n",
            "step 2459: loss = 3.873666286468506\n",
            "step 2460: loss = 3.962752103805542\n",
            "step 2461: loss = 4.265742301940918\n",
            "step 2462: loss = 3.850813865661621\n",
            "step 2463: loss = 3.9244771003723145\n",
            "step 2464: loss = 4.118593692779541\n",
            "step 2465: loss = 3.9568865299224854\n",
            "step 2466: loss = 4.10875940322876\n",
            "step 2467: loss = 4.331159591674805\n",
            "step 2468: loss = 3.9428601264953613\n",
            "step 2469: loss = 4.1231303215026855\n",
            "step 2470: loss = 4.166102409362793\n",
            "step 2471: loss = 4.169969081878662\n",
            "step 2472: loss = 3.872304677963257\n",
            "step 2473: loss = 4.052809238433838\n",
            "step 2474: loss = 4.1620283126831055\n",
            "step 2475: loss = 4.09372615814209\n",
            "step 2476: loss = 3.9149529933929443\n",
            "step 2477: loss = 3.9249074459075928\n",
            "step 2478: loss = 3.995976209640503\n",
            "step 2479: loss = 3.7693028450012207\n",
            "step 2480: loss = 4.052386283874512\n",
            "step 2481: loss = 3.6907901763916016\n",
            "step 2482: loss = 4.103951454162598\n",
            "step 2483: loss = 4.057239055633545\n",
            "step 2484: loss = 3.7687571048736572\n",
            "step 2485: loss = 3.944188117980957\n",
            "step 2486: loss = 3.9112751483917236\n",
            "step 2487: loss = 4.179967880249023\n",
            "step 2488: loss = 4.097620964050293\n",
            "step 2489: loss = 3.7152154445648193\n",
            "step 2490: loss = 4.095682144165039\n",
            "step 2491: loss = 4.325587272644043\n",
            "step 2492: loss = 3.860276222229004\n",
            "step 2493: loss = 3.8576436042785645\n",
            "step 2494: loss = 4.12720251083374\n",
            "step 2495: loss = 3.77716064453125\n",
            "step 2496: loss = 4.0695953369140625\n",
            "step 2497: loss = 4.050095558166504\n",
            "step 2498: loss = 4.070027828216553\n",
            "step 2499: loss = 4.457236289978027\n",
            "step 2500: loss = 4.121713161468506\n",
            "step 2501: loss = 4.402345180511475\n",
            "step 2502: loss = 4.071565628051758\n",
            "step 2503: loss = 4.004440784454346\n",
            "step 2504: loss = 4.113039016723633\n",
            "step 2505: loss = 4.2149858474731445\n",
            "step 2506: loss = 4.067990303039551\n",
            "step 2507: loss = 4.10626220703125\n",
            "step 2508: loss = 3.7113089561462402\n",
            "step 2509: loss = 4.130229949951172\n",
            "step 2510: loss = 3.963954210281372\n",
            "step 2511: loss = 4.21621561050415\n",
            "step 2512: loss = 3.9230079650878906\n",
            "step 2513: loss = 3.829458475112915\n",
            "step 2514: loss = 3.9911978244781494\n",
            "step 2515: loss = 3.8166303634643555\n",
            "step 2516: loss = 3.8774325847625732\n",
            "step 2517: loss = 4.03506326675415\n",
            "step 2518: loss = 4.153675556182861\n",
            "step 2519: loss = 3.904067277908325\n",
            "step 2520: loss = 3.9543206691741943\n",
            "step 2521: loss = 4.259579181671143\n",
            "step 2522: loss = 3.9398531913757324\n",
            "step 2523: loss = 3.607034921646118\n",
            "step 2524: loss = 3.8138318061828613\n",
            "step 2525: loss = 4.166164398193359\n",
            "step 2526: loss = 3.8496594429016113\n",
            "step 2527: loss = 3.915527820587158\n",
            "step 2528: loss = 4.0782151222229\n",
            "step 2529: loss = 4.203261852264404\n",
            "step 2530: loss = 4.232666969299316\n",
            "step 2531: loss = 3.984837055206299\n",
            "step 2532: loss = 4.1187944412231445\n",
            "step 2533: loss = 4.115499019622803\n",
            "step 2534: loss = 4.16238260269165\n",
            "step 2535: loss = 3.757669687271118\n",
            "step 2536: loss = 4.127887725830078\n",
            "step 2537: loss = 4.2689924240112305\n",
            "step 2538: loss = 3.9672248363494873\n",
            "step 2539: loss = 4.403336048126221\n",
            "step 2540: loss = 3.9399373531341553\n",
            "step 2541: loss = 3.9624602794647217\n",
            "step 2542: loss = 3.7645201683044434\n",
            "step 2543: loss = 3.844648838043213\n",
            "step 2544: loss = 3.6816372871398926\n",
            "step 2545: loss = 4.053267002105713\n",
            "step 2546: loss = 4.133111476898193\n",
            "step 2547: loss = 3.931516170501709\n",
            "step 2548: loss = 3.8902342319488525\n",
            "step 2549: loss = 3.7529280185699463\n",
            "step 2550: loss = 3.982923746109009\n",
            "step 2551: loss = 4.13860559463501\n",
            "step 2552: loss = 3.774810314178467\n",
            "step 2553: loss = 3.9314844608306885\n",
            "step 2554: loss = 3.7419049739837646\n",
            "step 2555: loss = 3.959369421005249\n",
            "step 2556: loss = 3.744849443435669\n",
            "step 2557: loss = 3.788320779800415\n",
            "step 2558: loss = 4.028382778167725\n",
            "step 2559: loss = 4.044554710388184\n",
            "step 2560: loss = 3.9457859992980957\n",
            "step 2561: loss = 3.996286630630493\n",
            "step 2562: loss = 3.780644655227661\n",
            "step 2563: loss = 4.034668922424316\n",
            "step 2564: loss = 4.16859769821167\n",
            "step 2565: loss = 3.8695924282073975\n",
            "step 2566: loss = 4.189821243286133\n",
            "step 2567: loss = 4.1280364990234375\n",
            "step 2568: loss = 4.239813804626465\n",
            "step 2569: loss = 3.608586072921753\n",
            "step 2570: loss = 4.033588886260986\n",
            "step 2571: loss = 4.293087959289551\n",
            "step 2572: loss = 4.000869274139404\n",
            "step 2573: loss = 4.4077582359313965\n",
            "step 2574: loss = 3.9485485553741455\n",
            "step 2575: loss = 4.307601451873779\n",
            "step 2576: loss = 3.726379632949829\n",
            "step 2577: loss = 4.0558061599731445\n",
            "step 2578: loss = 3.8165066242218018\n",
            "step 2579: loss = 3.8480124473571777\n",
            "step 2580: loss = 3.871208429336548\n",
            "step 2581: loss = 4.2168803215026855\n",
            "step 2582: loss = 3.90291428565979\n",
            "step 2583: loss = 3.934823513031006\n",
            "step 2584: loss = 4.01787805557251\n",
            "step 2585: loss = 3.884441375732422\n",
            "step 2586: loss = 4.245183944702148\n",
            "step 2587: loss = 4.244703769683838\n",
            "step 2588: loss = 4.075222969055176\n",
            "step 2589: loss = 3.6831133365631104\n",
            "step 2590: loss = 3.7397055625915527\n",
            "step 2591: loss = 4.0781569480896\n",
            "step 2592: loss = 3.521482229232788\n",
            "step 2593: loss = 4.038748264312744\n",
            "step 2594: loss = 4.053483963012695\n",
            "step 2595: loss = 4.122411727905273\n",
            "step 2596: loss = 3.8789830207824707\n",
            "step 2597: loss = 4.2331743240356445\n",
            "step 2598: loss = 4.054388046264648\n",
            "step 2599: loss = 4.017082691192627\n",
            "step 2600: loss = 4.012000560760498\n",
            "step 2601: loss = 4.04403018951416\n",
            "step 2602: loss = 3.9151227474212646\n",
            "step 2603: loss = 4.065791606903076\n",
            "step 2604: loss = 4.168624401092529\n",
            "step 2605: loss = 4.099483966827393\n",
            "step 2606: loss = 4.056300640106201\n",
            "step 2607: loss = 4.282320499420166\n",
            "step 2608: loss = 4.18192720413208\n",
            "step 2609: loss = 3.835995674133301\n",
            "step 2610: loss = 3.8410768508911133\n",
            "step 2611: loss = 3.845625162124634\n",
            "step 2612: loss = 4.023526668548584\n",
            "step 2613: loss = 4.0214385986328125\n",
            "step 2614: loss = 4.474913120269775\n",
            "step 2615: loss = 4.0201921463012695\n",
            "step 2616: loss = 3.7882540225982666\n",
            "step 2617: loss = 3.792637586593628\n",
            "step 2618: loss = 4.061380386352539\n",
            "step 2619: loss = 3.940932035446167\n",
            "step 2620: loss = 4.148045063018799\n",
            "step 2621: loss = 4.1730523109436035\n",
            "step 2622: loss = 4.0034308433532715\n",
            "step 2623: loss = 4.061861515045166\n",
            "step 2624: loss = 4.279025554656982\n",
            "step 2625: loss = 4.063792705535889\n",
            "step 2626: loss = 3.917584180831909\n",
            "step 2627: loss = 3.8826451301574707\n",
            "step 2628: loss = 4.079966068267822\n",
            "step 2629: loss = 4.356786251068115\n",
            "step 2630: loss = 4.046541690826416\n",
            "step 2631: loss = 4.134339809417725\n",
            "step 2632: loss = 3.9984028339385986\n",
            "step 2633: loss = 3.9983184337615967\n",
            "step 2634: loss = 3.8836164474487305\n",
            "step 2635: loss = 4.318855285644531\n",
            "step 2636: loss = 3.777275562286377\n",
            "step 2637: loss = 4.076511383056641\n",
            "step 2638: loss = 3.7538304328918457\n",
            "step 2639: loss = 3.680363893508911\n",
            "step 2640: loss = 3.991237163543701\n",
            "step 2641: loss = 4.030221462249756\n",
            "step 2642: loss = 4.034469127655029\n",
            "step 2643: loss = 3.9240355491638184\n",
            "step 2644: loss = 3.929009199142456\n",
            "step 2645: loss = 4.227083683013916\n",
            "step 2646: loss = 4.14678430557251\n",
            "step 2647: loss = 3.929297685623169\n",
            "step 2648: loss = 4.049528121948242\n",
            "step 2649: loss = 4.280105113983154\n",
            "step 2650: loss = 3.8536152839660645\n",
            "step 2651: loss = 3.9196279048919678\n",
            "step 2652: loss = 3.7305612564086914\n",
            "step 2653: loss = 3.7962100505828857\n",
            "step 2654: loss = 3.8169941902160645\n",
            "step 2655: loss = 3.853761911392212\n",
            "step 2656: loss = 3.9584898948669434\n",
            "step 2657: loss = 3.772226572036743\n",
            "step 2658: loss = 3.9028890132904053\n",
            "step 2659: loss = 4.1844048500061035\n",
            "step 2660: loss = 4.011258602142334\n",
            "step 2661: loss = 4.196086883544922\n",
            "step 2662: loss = 3.9150209426879883\n",
            "step 2663: loss = 3.97692608833313\n",
            "step 2664: loss = 4.210862636566162\n",
            "step 2665: loss = 3.890687942504883\n",
            "step 2666: loss = 3.9306554794311523\n",
            "step 2667: loss = 4.158422946929932\n",
            "step 2668: loss = 3.7770187854766846\n",
            "step 2669: loss = 3.6627087593078613\n",
            "step 2670: loss = 3.7216544151306152\n",
            "step 2671: loss = 3.9498088359832764\n",
            "step 2672: loss = 4.129255294799805\n",
            "step 2673: loss = 4.124210357666016\n",
            "step 2674: loss = 3.8059933185577393\n",
            "step 2675: loss = 3.8180198669433594\n",
            "step 2676: loss = 4.208810806274414\n",
            "step 2677: loss = 4.193697452545166\n",
            "step 2678: loss = 3.9526708126068115\n",
            "step 2679: loss = 4.161150932312012\n",
            "step 2680: loss = 4.051885604858398\n",
            "step 2681: loss = 3.991849184036255\n",
            "step 2682: loss = 3.933917999267578\n",
            "step 2683: loss = 4.034326553344727\n",
            "step 2684: loss = 3.824329376220703\n",
            "step 2685: loss = 4.015028476715088\n",
            "step 2686: loss = 3.92488431930542\n",
            "step 2687: loss = 3.952188730239868\n",
            "step 2688: loss = 4.02383279800415\n",
            "step 2689: loss = 4.120107650756836\n",
            "step 2690: loss = 3.8837273120880127\n",
            "step 2691: loss = 3.849973201751709\n",
            "step 2692: loss = 3.962561845779419\n",
            "step 2693: loss = 3.873701572418213\n",
            "step 2694: loss = 3.8858368396759033\n",
            "step 2695: loss = 3.825162887573242\n",
            "step 2696: loss = 3.8039615154266357\n",
            "step 2697: loss = 3.95924711227417\n",
            "step 2698: loss = 4.054208278656006\n",
            "step 2699: loss = 4.205702781677246\n",
            "step 2700: loss = 3.989891290664673\n",
            "step 2701: loss = 3.859560966491699\n",
            "step 2702: loss = 3.65364670753479\n",
            "step 2703: loss = 4.046772480010986\n",
            "step 2704: loss = 3.751370429992676\n",
            "step 2705: loss = 4.105074882507324\n",
            "step 2706: loss = 3.894087076187134\n",
            "step 2707: loss = 4.109071731567383\n",
            "step 2708: loss = 3.798612594604492\n",
            "step 2709: loss = 3.7855119705200195\n",
            "step 2710: loss = 3.663445234298706\n",
            "step 2711: loss = 3.8700664043426514\n",
            "step 2712: loss = 4.071556091308594\n",
            "step 2713: loss = 4.356063365936279\n",
            "step 2714: loss = 3.6801955699920654\n",
            "step 2715: loss = 4.231795310974121\n",
            "step 2716: loss = 4.166381359100342\n",
            "step 2717: loss = 3.8679521083831787\n",
            "step 2718: loss = 3.8721671104431152\n",
            "step 2719: loss = 3.96419620513916\n",
            "step 2720: loss = 3.6959009170532227\n",
            "step 2721: loss = 3.613177537918091\n",
            "step 2722: loss = 3.9839816093444824\n",
            "step 2723: loss = 3.7238242626190186\n",
            "step 2724: loss = 3.9074456691741943\n",
            "step 2725: loss = 4.301182270050049\n",
            "step 2726: loss = 3.9975290298461914\n",
            "step 2727: loss = 3.6927456855773926\n",
            "step 2728: loss = 3.89886474609375\n",
            "step 2729: loss = 3.8449153900146484\n",
            "step 2730: loss = 3.883746385574341\n",
            "step 2731: loss = 3.91819167137146\n",
            "step 2732: loss = 4.084010601043701\n",
            "step 2733: loss = 4.236968517303467\n",
            "step 2734: loss = 4.050040245056152\n",
            "step 2735: loss = 4.207648277282715\n",
            "step 2736: loss = 3.7502973079681396\n",
            "step 2737: loss = 3.9352989196777344\n",
            "step 2738: loss = 3.720184326171875\n",
            "step 2739: loss = 4.044037818908691\n",
            "step 2740: loss = 3.8895645141601562\n",
            "step 2741: loss = 4.042272090911865\n",
            "step 2742: loss = 3.9571101665496826\n",
            "step 2743: loss = 3.9230620861053467\n",
            "step 2744: loss = 3.7800450325012207\n",
            "step 2745: loss = 4.121618270874023\n",
            "step 2746: loss = 3.9773709774017334\n",
            "step 2747: loss = 3.915921449661255\n",
            "step 2748: loss = 3.675208806991577\n",
            "step 2749: loss = 3.8331005573272705\n",
            "step 2750: loss = 3.6556649208068848\n",
            "step 2751: loss = 3.883688449859619\n",
            "step 2752: loss = 3.9437716007232666\n",
            "step 2753: loss = 3.849714517593384\n",
            "step 2754: loss = 4.038219928741455\n",
            "step 2755: loss = 3.9720520973205566\n",
            "step 2756: loss = 4.106350898742676\n",
            "step 2757: loss = 3.9980647563934326\n",
            "step 2758: loss = 4.117401123046875\n",
            "step 2759: loss = 3.8459742069244385\n",
            "step 2760: loss = 4.181255340576172\n",
            "step 2761: loss = 3.96246337890625\n",
            "step 2762: loss = 4.23020601272583\n",
            "step 2763: loss = 3.921165704727173\n",
            "step 2764: loss = 3.804656505584717\n",
            "step 2765: loss = 4.124480724334717\n",
            "step 2766: loss = 3.961672067642212\n",
            "step 2767: loss = 3.95021390914917\n",
            "step 2768: loss = 4.087881088256836\n",
            "step 2769: loss = 4.066891193389893\n",
            "step 2770: loss = 3.58150053024292\n",
            "step 2771: loss = 4.452754497528076\n",
            "step 2772: loss = 3.9447524547576904\n",
            "step 2773: loss = 3.797064781188965\n",
            "step 2774: loss = 3.780005693435669\n",
            "step 2775: loss = 4.107193946838379\n",
            "step 2776: loss = 3.8866968154907227\n",
            "step 2777: loss = 3.941650390625\n",
            "step 2778: loss = 3.867202043533325\n",
            "step 2779: loss = 3.8135435581207275\n",
            "step 2780: loss = 4.295348644256592\n",
            "step 2781: loss = 3.7817835807800293\n",
            "step 2782: loss = 3.7287092208862305\n",
            "step 2783: loss = 3.917942523956299\n",
            "step 2784: loss = 3.738870620727539\n",
            "step 2785: loss = 3.6100826263427734\n",
            "step 2786: loss = 3.9353835582733154\n",
            "step 2787: loss = 3.9755990505218506\n",
            "step 2788: loss = 3.7427380084991455\n",
            "step 2789: loss = 3.824352264404297\n",
            "step 2790: loss = 3.6485416889190674\n",
            "step 2791: loss = 4.021682262420654\n",
            "step 2792: loss = 3.9130659103393555\n",
            "step 2793: loss = 4.074756622314453\n",
            "step 2794: loss = 3.6560049057006836\n",
            "step 2795: loss = 3.857088804244995\n",
            "step 2796: loss = 4.076086521148682\n",
            "step 2797: loss = 4.064109802246094\n",
            "step 2798: loss = 4.154949188232422\n",
            "step 2799: loss = 4.0437211990356445\n",
            "step 2800: loss = 3.9270694255828857\n",
            "step 2801: loss = 4.251903057098389\n",
            "step 2802: loss = 3.8944761753082275\n",
            "step 2803: loss = 3.7712435722351074\n",
            "step 2804: loss = 3.972243309020996\n",
            "step 2805: loss = 3.500178337097168\n",
            "step 2806: loss = 4.27059268951416\n",
            "step 2807: loss = 4.079972743988037\n",
            "step 2808: loss = 3.7919650077819824\n",
            "step 2809: loss = 4.0696001052856445\n",
            "step 2810: loss = 4.222431182861328\n",
            "step 2811: loss = 3.7536463737487793\n",
            "step 2812: loss = 3.887341022491455\n",
            "step 2813: loss = 4.123171329498291\n",
            "step 2814: loss = 3.9905247688293457\n",
            "step 2815: loss = 4.055405616760254\n",
            "step 2816: loss = 3.985775947570801\n",
            "step 2817: loss = 3.8067879676818848\n",
            "step 2818: loss = 3.8965721130371094\n",
            "step 2819: loss = 3.885424852371216\n",
            "step 2820: loss = 3.8290677070617676\n",
            "step 2821: loss = 4.023125171661377\n",
            "step 2822: loss = 4.154074668884277\n",
            "step 2823: loss = 4.080471992492676\n",
            "step 2824: loss = 3.971172332763672\n",
            "step 2825: loss = 3.9682843685150146\n",
            "step 2826: loss = 4.2603678703308105\n",
            "step 2827: loss = 3.960838556289673\n",
            "step 2828: loss = 3.8309545516967773\n",
            "step 2829: loss = 3.76664400100708\n",
            "step 2830: loss = 4.02532434463501\n",
            "step 2831: loss = 3.917647361755371\n",
            "step 2832: loss = 4.06486177444458\n",
            "step 2833: loss = 3.985776901245117\n",
            "step 2834: loss = 4.036652565002441\n",
            "step 2835: loss = 3.9921774864196777\n",
            "step 2836: loss = 3.6849420070648193\n",
            "step 2837: loss = 3.939012050628662\n",
            "step 2838: loss = 3.8408358097076416\n",
            "step 2839: loss = 3.9460551738739014\n",
            "step 2840: loss = 4.371407508850098\n",
            "step 2841: loss = 4.024757385253906\n",
            "step 2842: loss = 4.200403690338135\n",
            "step 2843: loss = 3.504930019378662\n",
            "step 2844: loss = 3.6470444202423096\n",
            "step 2845: loss = 4.00078821182251\n",
            "step 2846: loss = 4.0850653648376465\n",
            "step 2847: loss = 4.189074516296387\n",
            "step 2848: loss = 3.9793331623077393\n",
            "step 2849: loss = 3.922578811645508\n",
            "step 2850: loss = 3.833482027053833\n",
            "step 2851: loss = 3.793978214263916\n",
            "step 2852: loss = 3.9798505306243896\n",
            "step 2853: loss = 4.069514751434326\n",
            "step 2854: loss = 3.9221432209014893\n",
            "step 2855: loss = 3.771700620651245\n",
            "step 2856: loss = 3.982801675796509\n",
            "step 2857: loss = 3.6827211380004883\n",
            "step 2858: loss = 3.9458351135253906\n",
            "step 2859: loss = 3.960251808166504\n",
            "step 2860: loss = 3.861833095550537\n",
            "step 2861: loss = 3.799210548400879\n",
            "step 2862: loss = 3.6575465202331543\n",
            "step 2863: loss = 4.04922342300415\n",
            "step 2864: loss = 3.6890506744384766\n",
            "step 2865: loss = 3.8141605854034424\n",
            "step 2866: loss = 3.829843759536743\n",
            "step 2867: loss = 3.7489545345306396\n",
            "step 2868: loss = 3.7483675479888916\n",
            "step 2869: loss = 3.661313533782959\n",
            "step 2870: loss = 3.9081947803497314\n",
            "step 2871: loss = 3.9180283546447754\n",
            "step 2872: loss = 3.8833062648773193\n",
            "step 2873: loss = 4.153165340423584\n",
            "step 2874: loss = 3.873835802078247\n",
            "step 2875: loss = 4.058945178985596\n",
            "step 2876: loss = 3.8183789253234863\n",
            "step 2877: loss = 3.93503475189209\n",
            "step 2878: loss = 3.8717546463012695\n",
            "step 2879: loss = 3.8327245712280273\n",
            "step 2880: loss = 3.599825859069824\n",
            "step 2881: loss = 3.886554479598999\n",
            "step 2882: loss = 3.548135757446289\n",
            "step 2883: loss = 3.7494494915008545\n",
            "step 2884: loss = 4.087369441986084\n",
            "step 2885: loss = 3.541337013244629\n",
            "step 2886: loss = 3.6259429454803467\n",
            "step 2887: loss = 3.7332708835601807\n",
            "step 2888: loss = 3.6143224239349365\n",
            "step 2889: loss = 3.9570140838623047\n",
            "step 2890: loss = 3.7066538333892822\n",
            "step 2891: loss = 3.948509454727173\n",
            "step 2892: loss = 3.948694944381714\n",
            "step 2893: loss = 3.9995853900909424\n",
            "step 2894: loss = 3.6839354038238525\n",
            "step 2895: loss = 3.845200300216675\n",
            "step 2896: loss = 4.108193874359131\n",
            "step 2897: loss = 4.05242919921875\n",
            "step 2898: loss = 3.700052261352539\n",
            "step 2899: loss = 3.9465420246124268\n",
            "step 2900: loss = 3.755932092666626\n",
            "step 2901: loss = 4.027965068817139\n",
            "step 2902: loss = 4.218980312347412\n",
            "step 2903: loss = 3.8892557621002197\n",
            "step 2904: loss = 4.030776023864746\n",
            "step 2905: loss = 4.244235992431641\n",
            "step 2906: loss = 3.7404139041900635\n",
            "step 2907: loss = 3.800241470336914\n",
            "step 2908: loss = 3.794367551803589\n",
            "step 2909: loss = 3.9251675605773926\n",
            "step 2910: loss = 3.889741897583008\n",
            "step 2911: loss = 4.060810089111328\n",
            "step 2912: loss = 3.754749059677124\n",
            "step 2913: loss = 3.8862576484680176\n",
            "step 2914: loss = 3.8782248497009277\n",
            "step 2915: loss = 4.014392375946045\n",
            "step 2916: loss = 3.6771445274353027\n",
            "step 2917: loss = 4.176953315734863\n",
            "step 2918: loss = 3.6483490467071533\n",
            "step 2919: loss = 3.917337656021118\n",
            "step 2920: loss = 3.5525479316711426\n",
            "step 2921: loss = 3.7261650562286377\n",
            "step 2922: loss = 4.036072731018066\n",
            "step 2923: loss = 3.979046583175659\n",
            "step 2924: loss = 4.083963871002197\n",
            "step 2925: loss = 3.935304880142212\n",
            "step 2926: loss = 3.8937361240386963\n",
            "step 2927: loss = 3.474637269973755\n",
            "step 2928: loss = 3.7444570064544678\n",
            "step 2929: loss = 3.996986150741577\n",
            "step 2930: loss = 3.8016457557678223\n",
            "step 2931: loss = 3.78261137008667\n",
            "step 2932: loss = 3.9069385528564453\n",
            "step 2933: loss = 3.839648723602295\n",
            "step 2934: loss = 3.593599319458008\n",
            "step 2935: loss = 3.7602851390838623\n",
            "step 2936: loss = 3.9639270305633545\n",
            "step 2937: loss = 4.025300025939941\n",
            "step 2938: loss = 4.225255012512207\n",
            "step 2939: loss = 3.856743097305298\n",
            "step 2940: loss = 4.056262016296387\n",
            "step 2941: loss = 4.0709052085876465\n",
            "step 2942: loss = 3.9530484676361084\n",
            "step 2943: loss = 3.9797677993774414\n",
            "step 2944: loss = 4.149275779724121\n",
            "step 2945: loss = 3.6295247077941895\n",
            "step 2946: loss = 4.040270805358887\n",
            "step 2947: loss = 4.018972396850586\n",
            "step 2948: loss = 3.7323503494262695\n",
            "step 2949: loss = 3.6356916427612305\n",
            "step 2950: loss = 4.163580894470215\n",
            "step 2951: loss = 3.580781936645508\n",
            "step 2952: loss = 3.501552104949951\n",
            "step 2953: loss = 3.7203762531280518\n",
            "step 2954: loss = 3.8472068309783936\n",
            "step 2955: loss = 3.774153470993042\n",
            "step 2956: loss = 3.7271347045898438\n",
            "step 2957: loss = 3.7723312377929688\n",
            "step 2958: loss = 3.8811895847320557\n",
            "step 2959: loss = 3.629897117614746\n",
            "step 2960: loss = 3.979086399078369\n",
            "step 2961: loss = 3.9540278911590576\n",
            "step 2962: loss = 4.138507843017578\n",
            "step 2963: loss = 3.763296365737915\n",
            "step 2964: loss = 3.9899497032165527\n",
            "step 2965: loss = 3.775468587875366\n",
            "step 2966: loss = 3.7575290203094482\n",
            "step 2967: loss = 3.767442226409912\n",
            "step 2968: loss = 3.8349831104278564\n",
            "step 2969: loss = 3.6653008460998535\n",
            "step 2970: loss = 4.0328803062438965\n",
            "step 2971: loss = 3.932929277420044\n",
            "step 2972: loss = 3.826239824295044\n",
            "step 2973: loss = 3.860064744949341\n",
            "step 2974: loss = 3.585650682449341\n",
            "step 2975: loss = 3.834109306335449\n",
            "step 2976: loss = 4.335156440734863\n",
            "step 2977: loss = 3.677947998046875\n",
            "step 2978: loss = 3.649648427963257\n",
            "step 2979: loss = 3.7681684494018555\n",
            "step 2980: loss = 3.772334575653076\n",
            "step 2981: loss = 3.7048373222351074\n",
            "step 2982: loss = 4.0915913581848145\n",
            "step 2983: loss = 3.990849494934082\n",
            "step 2984: loss = 3.8841404914855957\n",
            "step 2985: loss = 3.806839942932129\n",
            "step 2986: loss = 3.9357662200927734\n",
            "step 2987: loss = 3.931593179702759\n",
            "step 2988: loss = 3.874293804168701\n",
            "step 2989: loss = 3.6349868774414062\n",
            "step 2990: loss = 3.96597957611084\n",
            "step 2991: loss = 3.550990581512451\n",
            "step 2992: loss = 3.7397565841674805\n",
            "step 2993: loss = 4.108633041381836\n",
            "step 2994: loss = 3.6831510066986084\n",
            "step 2995: loss = 3.905457019805908\n",
            "step 2996: loss = 3.94199800491333\n",
            "step 2997: loss = 3.5769448280334473\n",
            "step 2998: loss = 3.976680278778076\n",
            "step 2999: loss = 3.638355016708374\n",
            "step 3000: loss = 4.034585952758789\n",
            "step 3001: loss = 3.4228127002716064\n",
            "step 3002: loss = 3.8726439476013184\n",
            "step 3003: loss = 4.111921310424805\n",
            "step 3004: loss = 3.9899003505706787\n",
            "step 3005: loss = 3.9922266006469727\n",
            "step 3006: loss = 3.6296911239624023\n",
            "step 3007: loss = 4.036715507507324\n",
            "step 3008: loss = 3.74593448638916\n",
            "step 3009: loss = 3.9316940307617188\n",
            "step 3010: loss = 3.884955406188965\n",
            "step 3011: loss = 3.7700395584106445\n",
            "step 3012: loss = 3.671808958053589\n",
            "step 3013: loss = 3.8798208236694336\n",
            "step 3014: loss = 3.538752794265747\n",
            "step 3015: loss = 3.768442153930664\n",
            "step 3016: loss = 3.7063021659851074\n",
            "step 3017: loss = 3.8610482215881348\n",
            "step 3018: loss = 3.9180378913879395\n",
            "step 3019: loss = 3.9684953689575195\n",
            "step 3020: loss = 3.8079192638397217\n",
            "step 3021: loss = 4.046140193939209\n",
            "step 3022: loss = 4.017280578613281\n",
            "step 3023: loss = 4.034599304199219\n",
            "step 3024: loss = 3.5456817150115967\n",
            "step 3025: loss = 3.792235851287842\n",
            "step 3026: loss = 4.334523677825928\n",
            "step 3027: loss = 3.915461540222168\n",
            "step 3028: loss = 3.992872953414917\n",
            "step 3029: loss = 4.211071491241455\n",
            "step 3030: loss = 3.500058650970459\n",
            "step 3031: loss = 3.6785600185394287\n",
            "step 3032: loss = 4.061661720275879\n",
            "step 3033: loss = 4.185736179351807\n",
            "step 3034: loss = 3.7193539142608643\n",
            "step 3035: loss = 4.140261650085449\n",
            "step 3036: loss = 3.899440050125122\n",
            "step 3037: loss = 3.811093807220459\n",
            "step 3038: loss = 3.487100124359131\n",
            "step 3039: loss = 3.986201763153076\n",
            "step 3040: loss = 3.9393956661224365\n",
            "step 3041: loss = 4.052399635314941\n",
            "step 3042: loss = 3.7284984588623047\n",
            "step 3043: loss = 3.790649890899658\n",
            "step 3044: loss = 3.548768997192383\n",
            "step 3045: loss = 3.715644121170044\n",
            "step 3046: loss = 3.6268279552459717\n",
            "step 3047: loss = 3.8121466636657715\n",
            "step 3048: loss = 3.679948091506958\n",
            "step 3049: loss = 4.014726161956787\n",
            "step 3050: loss = 3.8603954315185547\n",
            "step 3051: loss = 3.7035634517669678\n",
            "step 3052: loss = 4.130235195159912\n",
            "step 3053: loss = 4.203610897064209\n",
            "step 3054: loss = 3.8787143230438232\n",
            "step 3055: loss = 3.8159940242767334\n",
            "step 3056: loss = 3.7919962406158447\n",
            "step 3057: loss = 3.884032726287842\n",
            "step 3058: loss = 3.8810532093048096\n",
            "step 3059: loss = 3.6510422229766846\n",
            "step 3060: loss = 3.952033042907715\n",
            "step 3061: loss = 4.166710376739502\n",
            "step 3062: loss = 4.067359447479248\n",
            "step 3063: loss = 3.904933214187622\n",
            "step 3064: loss = 3.9542627334594727\n",
            "step 3065: loss = 3.800168752670288\n",
            "step 3066: loss = 3.8772225379943848\n",
            "step 3067: loss = 3.607372760772705\n",
            "step 3068: loss = 3.9620378017425537\n",
            "step 3069: loss = 3.8074395656585693\n",
            "step 3070: loss = 4.0062432289123535\n",
            "step 3071: loss = 3.8074560165405273\n",
            "step 3072: loss = 3.599950075149536\n",
            "step 3073: loss = 3.7815709114074707\n",
            "step 3074: loss = 3.905695676803589\n",
            "step 3075: loss = 3.559051036834717\n",
            "step 3076: loss = 4.0802178382873535\n",
            "step 3077: loss = 4.170695781707764\n",
            "step 3078: loss = 3.972926139831543\n",
            "step 3079: loss = 3.9223430156707764\n",
            "step 3080: loss = 3.5489819049835205\n",
            "step 3081: loss = 3.6812405586242676\n",
            "step 3082: loss = 3.893376350402832\n",
            "step 3083: loss = 3.606210947036743\n",
            "step 3084: loss = 3.9335319995880127\n",
            "step 3085: loss = 3.648117780685425\n",
            "step 3086: loss = 3.742948293685913\n",
            "step 3087: loss = 3.8001537322998047\n",
            "step 3088: loss = 4.14837121963501\n",
            "step 3089: loss = 3.911992073059082\n",
            "step 3090: loss = 3.7741942405700684\n",
            "step 3091: loss = 4.018957614898682\n",
            "step 3092: loss = 3.9049620628356934\n",
            "step 3093: loss = 3.9586451053619385\n",
            "step 3094: loss = 3.775740623474121\n",
            "step 3095: loss = 4.248282432556152\n",
            "step 3096: loss = 3.965709686279297\n",
            "step 3097: loss = 4.002471923828125\n",
            "step 3098: loss = 3.677973985671997\n",
            "step 3099: loss = 3.5650699138641357\n",
            "step 3100: loss = 3.9451770782470703\n",
            "step 3101: loss = 4.04652738571167\n",
            "step 3102: loss = 3.998683214187622\n",
            "step 3103: loss = 3.92948579788208\n",
            "step 3104: loss = 3.899574041366577\n",
            "step 3105: loss = 3.7581849098205566\n",
            "step 3106: loss = 3.958989381790161\n",
            "step 3107: loss = 3.6215133666992188\n",
            "step 3108: loss = 3.592395782470703\n",
            "step 3109: loss = 4.056020736694336\n",
            "step 3110: loss = 3.803647994995117\n",
            "step 3111: loss = 3.6082117557525635\n",
            "step 3112: loss = 4.085158348083496\n",
            "step 3113: loss = 4.016542434692383\n",
            "step 3114: loss = 3.531205654144287\n",
            "step 3115: loss = 4.0189971923828125\n",
            "step 3116: loss = 3.754511594772339\n",
            "step 3117: loss = 4.153879642486572\n",
            "step 3118: loss = 4.028185844421387\n",
            "step 3119: loss = 4.050800323486328\n",
            "step 3120: loss = 4.010221004486084\n",
            "step 3121: loss = 3.9947996139526367\n",
            "step 3122: loss = 3.734727144241333\n",
            "step 3123: loss = 3.944030523300171\n",
            "step 3124: loss = 3.7476632595062256\n",
            "Finish epoch 2\n",
            "New model saved, minimum loss: 4.092869749295116 \n",
            "\n",
            "step 3125: loss = 3.4062299728393555\n",
            "step 3126: loss = 3.4327385425567627\n",
            "step 3127: loss = 3.5211596488952637\n",
            "step 3128: loss = 3.5410640239715576\n",
            "step 3129: loss = 3.6063361167907715\n",
            "step 3130: loss = 3.3258066177368164\n",
            "step 3131: loss = 3.5279924869537354\n",
            "step 3132: loss = 3.468480110168457\n",
            "step 3133: loss = 3.3536782264709473\n",
            "step 3134: loss = 3.236935615539551\n",
            "step 3135: loss = 3.5212502479553223\n",
            "step 3136: loss = 3.496985912322998\n",
            "step 3137: loss = 3.4031736850738525\n",
            "step 3138: loss = 3.326282501220703\n",
            "step 3139: loss = 3.525247812271118\n",
            "step 3140: loss = 3.4454121589660645\n",
            "step 3141: loss = 3.2146568298339844\n",
            "step 3142: loss = 3.195661783218384\n",
            "step 3143: loss = 3.3284695148468018\n",
            "step 3144: loss = 3.5081119537353516\n",
            "step 3145: loss = 3.520493507385254\n",
            "step 3146: loss = 3.132774591445923\n",
            "step 3147: loss = 3.172851085662842\n",
            "step 3148: loss = 3.349416732788086\n",
            "step 3149: loss = 3.5490167140960693\n",
            "step 3150: loss = 3.579802989959717\n",
            "step 3151: loss = 3.420498847961426\n",
            "step 3152: loss = 3.4199106693267822\n",
            "step 3153: loss = 3.731722593307495\n",
            "step 3154: loss = 2.8674728870391846\n",
            "step 3155: loss = 3.4472763538360596\n",
            "step 3156: loss = 3.1250410079956055\n",
            "step 3157: loss = 3.5631141662597656\n",
            "step 3158: loss = 3.2107279300689697\n",
            "step 3159: loss = 3.2463998794555664\n",
            "step 3160: loss = 3.661561965942383\n",
            "step 3161: loss = 3.3166322708129883\n",
            "step 3162: loss = 3.3027522563934326\n",
            "step 3163: loss = 3.5483410358428955\n",
            "step 3164: loss = 3.4995415210723877\n",
            "step 3165: loss = 3.20231556892395\n",
            "step 3166: loss = 3.2188968658447266\n",
            "step 3167: loss = 3.5000815391540527\n",
            "step 3168: loss = 3.407377004623413\n",
            "step 3169: loss = 3.6844418048858643\n",
            "step 3170: loss = 3.4902262687683105\n",
            "step 3171: loss = 3.2604258060455322\n",
            "step 3172: loss = 3.591491460800171\n",
            "step 3173: loss = 3.3174102306365967\n",
            "step 3174: loss = 3.3461999893188477\n",
            "step 3175: loss = 3.573518991470337\n",
            "step 3176: loss = 3.2227728366851807\n",
            "step 3177: loss = 3.4105985164642334\n",
            "step 3178: loss = 3.2143077850341797\n",
            "step 3179: loss = 3.5391435623168945\n",
            "step 3180: loss = 3.5338363647460938\n",
            "step 3181: loss = 3.388125419616699\n",
            "step 3182: loss = 3.5412189960479736\n",
            "step 3183: loss = 3.2750518321990967\n",
            "step 3184: loss = 3.6905720233917236\n",
            "step 3185: loss = 3.5539541244506836\n",
            "step 3186: loss = 3.5687050819396973\n",
            "step 3187: loss = 3.2713863849639893\n",
            "step 3188: loss = 3.25071382522583\n",
            "step 3189: loss = 3.4496958255767822\n",
            "step 3190: loss = 3.781153917312622\n",
            "step 3191: loss = 3.2595314979553223\n",
            "step 3192: loss = 3.6784021854400635\n",
            "step 3193: loss = 3.347348928451538\n",
            "step 3194: loss = 3.3662874698638916\n",
            "step 3195: loss = 3.6969432830810547\n",
            "step 3196: loss = 3.3933217525482178\n",
            "step 3197: loss = 3.5214455127716064\n",
            "step 3198: loss = 3.440094232559204\n",
            "step 3199: loss = 3.255676507949829\n",
            "step 3200: loss = 3.3328397274017334\n",
            "step 3201: loss = 3.3233070373535156\n",
            "step 3202: loss = 3.389842987060547\n",
            "step 3203: loss = 3.3352482318878174\n",
            "step 3204: loss = 3.461533784866333\n",
            "step 3205: loss = 3.7032957077026367\n",
            "step 3206: loss = 3.3237602710723877\n",
            "step 3207: loss = 3.207212209701538\n",
            "step 3208: loss = 3.3063929080963135\n",
            "step 3209: loss = 3.3958590030670166\n",
            "step 3210: loss = 3.582618474960327\n",
            "step 3211: loss = 3.2751405239105225\n",
            "step 3212: loss = 3.641507863998413\n",
            "step 3213: loss = 3.4899487495422363\n",
            "step 3214: loss = 3.488813877105713\n",
            "step 3215: loss = 3.4981510639190674\n",
            "step 3216: loss = 3.464294672012329\n",
            "step 3217: loss = 3.555466413497925\n",
            "step 3218: loss = 3.234807014465332\n",
            "step 3219: loss = 3.454174757003784\n",
            "step 3220: loss = 3.4698774814605713\n",
            "step 3221: loss = 3.266580820083618\n",
            "step 3222: loss = 3.262484312057495\n",
            "step 3223: loss = 3.4333832263946533\n",
            "step 3224: loss = 3.5178279876708984\n",
            "step 3225: loss = 3.5745699405670166\n",
            "step 3226: loss = 3.3974220752716064\n",
            "step 3227: loss = 3.248648166656494\n",
            "step 3228: loss = 3.495124340057373\n",
            "step 3229: loss = 3.2262203693389893\n",
            "step 3230: loss = 3.496607542037964\n",
            "step 3231: loss = 3.2306346893310547\n",
            "step 3232: loss = 3.4636332988739014\n",
            "step 3233: loss = 3.492318868637085\n",
            "step 3234: loss = 3.0929558277130127\n",
            "step 3235: loss = 3.5284690856933594\n",
            "step 3236: loss = 3.5673842430114746\n",
            "step 3237: loss = 3.3242759704589844\n",
            "step 3238: loss = 3.433387041091919\n",
            "step 3239: loss = 3.194655179977417\n",
            "step 3240: loss = 3.250051259994507\n",
            "step 3241: loss = 3.357818841934204\n",
            "step 3242: loss = 3.56996750831604\n",
            "step 3243: loss = 3.2948312759399414\n",
            "step 3244: loss = 3.446099281311035\n",
            "step 3245: loss = 3.6514012813568115\n",
            "step 3246: loss = 3.1645781993865967\n",
            "step 3247: loss = 3.24601674079895\n",
            "step 3248: loss = 3.1848111152648926\n",
            "step 3249: loss = 3.549755334854126\n",
            "step 3250: loss = 3.2832422256469727\n",
            "step 3251: loss = 3.39664363861084\n",
            "step 3252: loss = 3.3692069053649902\n",
            "step 3253: loss = 3.1928727626800537\n",
            "step 3254: loss = 3.3921823501586914\n",
            "step 3255: loss = 3.624013900756836\n",
            "step 3256: loss = 3.3500771522521973\n",
            "step 3257: loss = 3.4083075523376465\n",
            "step 3258: loss = 3.242506504058838\n",
            "step 3259: loss = 3.240130662918091\n",
            "step 3260: loss = 3.6522068977355957\n",
            "step 3261: loss = 3.64475154876709\n",
            "step 3262: loss = 3.4083914756774902\n",
            "step 3263: loss = 3.4540436267852783\n",
            "step 3264: loss = 3.5277187824249268\n",
            "step 3265: loss = 3.5622498989105225\n",
            "step 3266: loss = 3.8635661602020264\n",
            "step 3267: loss = 3.227476119995117\n",
            "step 3268: loss = 3.4384913444519043\n",
            "step 3269: loss = 3.136014938354492\n",
            "step 3270: loss = 3.6592307090759277\n",
            "step 3271: loss = 3.209939479827881\n",
            "step 3272: loss = 3.483421802520752\n",
            "step 3273: loss = 3.582198143005371\n",
            "step 3274: loss = 3.4532129764556885\n",
            "step 3275: loss = 3.3610329627990723\n",
            "step 3276: loss = 3.5250422954559326\n",
            "step 3277: loss = 3.3675427436828613\n",
            "step 3278: loss = 3.504265546798706\n",
            "step 3279: loss = 3.4360735416412354\n",
            "step 3280: loss = 3.5235869884490967\n",
            "step 3281: loss = 3.2477970123291016\n",
            "step 3282: loss = 3.218862295150757\n",
            "step 3283: loss = 3.552706718444824\n",
            "step 3284: loss = 3.2102904319763184\n",
            "step 3285: loss = 3.6351547241210938\n",
            "step 3286: loss = 3.3072032928466797\n",
            "step 3287: loss = 3.642953634262085\n",
            "step 3288: loss = 3.378476142883301\n",
            "step 3289: loss = 3.373020887374878\n",
            "step 3290: loss = 3.008607864379883\n",
            "step 3291: loss = 3.003098487854004\n",
            "step 3292: loss = 3.6558585166931152\n",
            "step 3293: loss = 3.419342041015625\n",
            "step 3294: loss = 3.48460054397583\n",
            "step 3295: loss = 3.395263195037842\n",
            "step 3296: loss = 3.1915206909179688\n",
            "step 3297: loss = 3.3137571811676025\n",
            "step 3298: loss = 3.564127206802368\n",
            "step 3299: loss = 3.356999158859253\n",
            "step 3300: loss = 3.5957562923431396\n",
            "step 3301: loss = 3.1805567741394043\n",
            "step 3302: loss = 3.312532424926758\n",
            "step 3303: loss = 3.4193472862243652\n",
            "step 3304: loss = 3.47369384765625\n",
            "step 3305: loss = 3.60317063331604\n",
            "step 3306: loss = 3.162112236022949\n",
            "step 3307: loss = 3.298513650894165\n",
            "step 3308: loss = 3.5687122344970703\n",
            "step 3309: loss = 3.6748850345611572\n",
            "step 3310: loss = 3.3833560943603516\n",
            "step 3311: loss = 3.457219362258911\n",
            "step 3312: loss = 3.5150668621063232\n",
            "step 3313: loss = 3.650714874267578\n",
            "step 3314: loss = 3.090527296066284\n",
            "step 3315: loss = 3.385756492614746\n",
            "step 3316: loss = 3.4426755905151367\n",
            "step 3317: loss = 3.311504602432251\n",
            "step 3318: loss = 3.6647589206695557\n",
            "step 3319: loss = 3.3202593326568604\n",
            "step 3320: loss = 3.6222035884857178\n",
            "step 3321: loss = 3.543623447418213\n",
            "step 3322: loss = 3.485567808151245\n",
            "step 3323: loss = 3.733919620513916\n",
            "step 3324: loss = 3.3021602630615234\n",
            "step 3325: loss = 3.235379457473755\n",
            "step 3326: loss = 3.462897539138794\n",
            "step 3327: loss = 3.275252103805542\n",
            "step 3328: loss = 3.5457592010498047\n",
            "step 3329: loss = 3.6654207706451416\n",
            "step 3330: loss = 3.33111834526062\n",
            "step 3331: loss = 3.3991754055023193\n",
            "step 3332: loss = 3.7939183712005615\n",
            "step 3333: loss = 3.4653546810150146\n",
            "step 3334: loss = 3.2744998931884766\n",
            "step 3335: loss = 3.25276780128479\n",
            "step 3336: loss = 3.2897121906280518\n",
            "step 3337: loss = 3.3077139854431152\n",
            "step 3338: loss = 3.3752384185791016\n",
            "step 3339: loss = 3.4931154251098633\n",
            "step 3340: loss = 3.5327353477478027\n",
            "step 3341: loss = 3.341430187225342\n",
            "step 3342: loss = 3.47038197517395\n",
            "step 3343: loss = 3.698939085006714\n",
            "step 3344: loss = 3.3305840492248535\n",
            "step 3345: loss = 3.4419147968292236\n",
            "step 3346: loss = 3.3648600578308105\n",
            "step 3347: loss = 3.344118118286133\n",
            "step 3348: loss = 3.376906633377075\n",
            "step 3349: loss = 3.428873300552368\n",
            "step 3350: loss = 3.536778688430786\n",
            "step 3351: loss = 3.393343210220337\n",
            "step 3352: loss = 3.736987829208374\n",
            "step 3353: loss = 3.4027671813964844\n",
            "step 3354: loss = 3.2370266914367676\n",
            "step 3355: loss = 3.426107168197632\n",
            "step 3356: loss = 3.4709630012512207\n",
            "step 3357: loss = 3.215251922607422\n",
            "step 3358: loss = 3.408324956893921\n",
            "step 3359: loss = 3.2015631198883057\n",
            "step 3360: loss = 3.592697858810425\n",
            "step 3361: loss = 3.8963377475738525\n",
            "step 3362: loss = 3.429655075073242\n",
            "step 3363: loss = 3.4351298809051514\n",
            "step 3364: loss = 3.4777724742889404\n",
            "step 3365: loss = 3.1542036533355713\n",
            "step 3366: loss = 3.2211503982543945\n",
            "step 3367: loss = 3.5448319911956787\n",
            "step 3368: loss = 3.5575947761535645\n",
            "step 3369: loss = 3.486469030380249\n",
            "step 3370: loss = 3.526303291320801\n",
            "step 3371: loss = 3.6379599571228027\n",
            "step 3372: loss = 3.6163623332977295\n",
            "step 3373: loss = 3.548846960067749\n",
            "step 3374: loss = 3.5842432975769043\n",
            "step 3375: loss = 3.312274694442749\n",
            "step 3376: loss = 3.334512948989868\n",
            "step 3377: loss = 3.1615962982177734\n",
            "step 3378: loss = 3.7260708808898926\n",
            "step 3379: loss = 3.4667139053344727\n",
            "step 3380: loss = 3.393634796142578\n",
            "step 3381: loss = 3.456402063369751\n",
            "step 3382: loss = 3.4578475952148438\n",
            "step 3383: loss = 3.4998536109924316\n",
            "step 3384: loss = 3.4176077842712402\n",
            "step 3385: loss = 3.6939609050750732\n",
            "step 3386: loss = 3.4922163486480713\n",
            "step 3387: loss = 3.4714455604553223\n",
            "step 3388: loss = 3.477966070175171\n",
            "step 3389: loss = 3.586808443069458\n",
            "step 3390: loss = 3.3462324142456055\n",
            "step 3391: loss = 3.1511342525482178\n",
            "step 3392: loss = 3.525588274002075\n",
            "step 3393: loss = 3.5066797733306885\n",
            "step 3394: loss = 3.4768495559692383\n",
            "step 3395: loss = 3.6301467418670654\n",
            "step 3396: loss = 3.6645731925964355\n",
            "step 3397: loss = 3.7507522106170654\n",
            "step 3398: loss = 3.8281893730163574\n",
            "step 3399: loss = 3.3989903926849365\n",
            "step 3400: loss = 3.533195734024048\n",
            "step 3401: loss = 3.3574345111846924\n",
            "step 3402: loss = 3.645501136779785\n",
            "step 3403: loss = 3.1265625953674316\n",
            "step 3404: loss = 3.2205352783203125\n",
            "step 3405: loss = 3.4226720333099365\n",
            "step 3406: loss = 3.7082793712615967\n",
            "step 3407: loss = 3.2761435508728027\n",
            "step 3408: loss = 3.5167367458343506\n",
            "step 3409: loss = 3.508249044418335\n",
            "step 3410: loss = 3.7275891304016113\n",
            "step 3411: loss = 3.7083821296691895\n",
            "step 3412: loss = 3.3294527530670166\n",
            "step 3413: loss = 3.3167800903320312\n",
            "step 3414: loss = 3.4240193367004395\n",
            "step 3415: loss = 3.2461295127868652\n",
            "step 3416: loss = 3.466871500015259\n",
            "step 3417: loss = 3.5332107543945312\n",
            "step 3418: loss = 3.466679334640503\n",
            "step 3419: loss = 3.217359781265259\n",
            "step 3420: loss = 3.3866209983825684\n",
            "step 3421: loss = 3.5664734840393066\n",
            "step 3422: loss = 2.951458215713501\n",
            "step 3423: loss = 3.2888402938842773\n",
            "step 3424: loss = 3.5542447566986084\n",
            "step 3425: loss = 3.5806140899658203\n",
            "step 3426: loss = 3.5521318912506104\n",
            "step 3427: loss = 3.383875846862793\n",
            "step 3428: loss = 3.3807997703552246\n",
            "step 3429: loss = 3.254945755004883\n",
            "step 3430: loss = 3.1091971397399902\n",
            "step 3431: loss = 3.568533182144165\n",
            "step 3432: loss = 3.1051483154296875\n",
            "step 3433: loss = 3.085991859436035\n",
            "step 3434: loss = 3.514631748199463\n",
            "step 3435: loss = 3.2881782054901123\n",
            "step 3436: loss = 3.689824104309082\n",
            "step 3437: loss = 3.635244846343994\n",
            "step 3438: loss = 3.4977846145629883\n",
            "step 3439: loss = 3.480107545852661\n",
            "step 3440: loss = 3.6823606491088867\n",
            "step 3441: loss = 3.359337329864502\n",
            "step 3442: loss = 3.509669303894043\n",
            "step 3443: loss = 3.6626169681549072\n",
            "step 3444: loss = 3.295184373855591\n",
            "step 3445: loss = 3.3151538372039795\n",
            "step 3446: loss = 3.2368404865264893\n",
            "step 3447: loss = 3.4305763244628906\n",
            "step 3448: loss = 3.7744507789611816\n",
            "step 3449: loss = 3.41064715385437\n",
            "step 3450: loss = 3.393406629562378\n",
            "step 3451: loss = 3.277184247970581\n",
            "step 3452: loss = 3.4441349506378174\n",
            "step 3453: loss = 3.440016984939575\n",
            "step 3454: loss = 3.7173521518707275\n",
            "step 3455: loss = 3.486236095428467\n",
            "step 3456: loss = 3.3238115310668945\n",
            "step 3457: loss = 3.427001953125\n",
            "step 3458: loss = 3.4860098361968994\n",
            "step 3459: loss = 3.492521286010742\n",
            "step 3460: loss = 3.6774744987487793\n",
            "step 3461: loss = 3.7044270038604736\n",
            "step 3462: loss = 3.531191349029541\n",
            "step 3463: loss = 3.4782555103302\n",
            "step 3464: loss = 3.758207082748413\n",
            "step 3465: loss = 3.4321675300598145\n",
            "step 3466: loss = 3.6134872436523438\n",
            "step 3467: loss = 3.506220817565918\n",
            "step 3468: loss = 3.438616991043091\n",
            "step 3469: loss = 3.5644359588623047\n",
            "step 3470: loss = 3.4802908897399902\n",
            "step 3471: loss = 3.192953109741211\n",
            "step 3472: loss = 3.4048264026641846\n",
            "step 3473: loss = 3.0875346660614014\n",
            "step 3474: loss = 3.436789035797119\n",
            "step 3475: loss = 3.356797933578491\n",
            "step 3476: loss = 3.342566728591919\n",
            "step 3477: loss = 3.564749240875244\n",
            "step 3478: loss = 3.20312237739563\n",
            "step 3479: loss = 3.2730565071105957\n",
            "step 3480: loss = 3.2747600078582764\n",
            "step 3481: loss = 3.593078136444092\n",
            "step 3482: loss = 3.1647229194641113\n",
            "step 3483: loss = 3.2265784740448\n",
            "step 3484: loss = 3.081969976425171\n",
            "step 3485: loss = 3.381852865219116\n",
            "step 3486: loss = 3.340580701828003\n",
            "step 3487: loss = 3.3697659969329834\n",
            "step 3488: loss = 3.639976978302002\n",
            "step 3489: loss = 3.6202588081359863\n",
            "step 3490: loss = 3.096891403198242\n",
            "step 3491: loss = 3.5236217975616455\n",
            "step 3492: loss = 3.5423121452331543\n",
            "step 3493: loss = 3.157484292984009\n",
            "step 3494: loss = 3.3691673278808594\n",
            "step 3495: loss = 3.301142930984497\n",
            "step 3496: loss = 3.752223253250122\n",
            "step 3497: loss = 3.471142530441284\n",
            "step 3498: loss = 3.2433571815490723\n",
            "step 3499: loss = 3.183842897415161\n",
            "step 3500: loss = 3.2186458110809326\n",
            "step 3501: loss = 3.548295736312866\n",
            "step 3502: loss = 3.6888391971588135\n",
            "step 3503: loss = 3.476548671722412\n",
            "step 3504: loss = 3.1337130069732666\n",
            "step 3505: loss = 3.3969199657440186\n",
            "step 3506: loss = 3.6834089756011963\n",
            "step 3507: loss = 3.5023038387298584\n",
            "step 3508: loss = 3.4953014850616455\n",
            "step 3509: loss = 3.7034683227539062\n",
            "step 3510: loss = 3.0655922889709473\n",
            "step 3511: loss = 3.314396381378174\n",
            "step 3512: loss = 3.5471537113189697\n",
            "step 3513: loss = 3.454418897628784\n",
            "step 3514: loss = 3.4329183101654053\n",
            "step 3515: loss = 3.2293033599853516\n",
            "step 3516: loss = 3.0896568298339844\n",
            "step 3517: loss = 3.46785044670105\n",
            "step 3518: loss = 3.173776865005493\n",
            "step 3519: loss = 3.593813896179199\n",
            "step 3520: loss = 3.321990489959717\n",
            "step 3521: loss = 3.6585052013397217\n",
            "step 3522: loss = 3.3881964683532715\n",
            "step 3523: loss = 3.4855854511260986\n",
            "step 3524: loss = 3.585860252380371\n",
            "step 3525: loss = 3.4309041500091553\n",
            "step 3526: loss = 3.5766818523406982\n",
            "step 3527: loss = 3.340543746948242\n",
            "step 3528: loss = 3.469698667526245\n",
            "step 3529: loss = 3.208031177520752\n",
            "step 3530: loss = 3.3867454528808594\n",
            "step 3531: loss = 3.358152151107788\n",
            "step 3532: loss = 3.426358938217163\n",
            "step 3533: loss = 3.6351633071899414\n",
            "step 3534: loss = 3.3407506942749023\n",
            "step 3535: loss = 3.407973289489746\n",
            "step 3536: loss = 3.4543070793151855\n",
            "step 3537: loss = 3.6035099029541016\n",
            "step 3538: loss = 3.432286500930786\n",
            "step 3539: loss = 3.3889269828796387\n",
            "step 3540: loss = 3.751352548599243\n",
            "step 3541: loss = 3.33150577545166\n",
            "step 3542: loss = 3.1358819007873535\n",
            "step 3543: loss = 3.2746224403381348\n",
            "step 3544: loss = 3.7289035320281982\n",
            "step 3545: loss = 3.3998990058898926\n",
            "step 3546: loss = 3.342751979827881\n",
            "step 3547: loss = 3.2461042404174805\n",
            "step 3548: loss = 3.4542391300201416\n",
            "step 3549: loss = 3.524930953979492\n",
            "step 3550: loss = 3.3002729415893555\n",
            "step 3551: loss = 3.5081331729888916\n",
            "step 3552: loss = 3.760091781616211\n",
            "step 3553: loss = 3.2685182094573975\n",
            "step 3554: loss = 3.2646491527557373\n",
            "step 3555: loss = 3.4447553157806396\n",
            "step 3556: loss = 3.306429386138916\n",
            "step 3557: loss = 3.2101962566375732\n",
            "step 3558: loss = 3.167567253112793\n",
            "step 3559: loss = 3.5561962127685547\n",
            "step 3560: loss = 3.285538673400879\n",
            "step 3561: loss = 3.0116796493530273\n",
            "step 3562: loss = 3.333071231842041\n",
            "step 3563: loss = 3.4774649143218994\n",
            "step 3564: loss = 3.491286039352417\n",
            "step 3565: loss = 3.436403751373291\n",
            "step 3566: loss = 3.639369249343872\n",
            "step 3567: loss = 3.3303446769714355\n",
            "step 3568: loss = 3.288810968399048\n",
            "step 3569: loss = 3.4216291904449463\n",
            "step 3570: loss = 3.6396324634552\n",
            "step 3571: loss = 3.621140718460083\n",
            "step 3572: loss = 3.2487740516662598\n",
            "step 3573: loss = 3.4973409175872803\n",
            "step 3574: loss = 3.443727731704712\n",
            "step 3575: loss = 3.493633270263672\n",
            "step 3576: loss = 3.141425848007202\n",
            "step 3577: loss = 3.3322770595550537\n",
            "step 3578: loss = 3.360023021697998\n",
            "step 3579: loss = 3.3352136611938477\n",
            "step 3580: loss = 3.402116298675537\n",
            "step 3581: loss = 3.4938604831695557\n",
            "step 3582: loss = 3.194648265838623\n",
            "step 3583: loss = 3.582298517227173\n",
            "step 3584: loss = 3.5563173294067383\n",
            "step 3585: loss = 3.629460096359253\n",
            "step 3586: loss = 3.359431028366089\n",
            "step 3587: loss = 3.351869821548462\n",
            "step 3588: loss = 3.5714755058288574\n",
            "step 3589: loss = 3.437838315963745\n",
            "step 3590: loss = 3.382154941558838\n",
            "step 3591: loss = 3.2047107219696045\n",
            "step 3592: loss = 3.3218507766723633\n",
            "step 3593: loss = 3.34950852394104\n",
            "step 3594: loss = 3.4066624641418457\n",
            "step 3595: loss = 3.1997780799865723\n",
            "step 3596: loss = 3.060957193374634\n",
            "step 3597: loss = 3.4066860675811768\n",
            "step 3598: loss = 3.4074552059173584\n",
            "step 3599: loss = 3.2101967334747314\n",
            "step 3600: loss = 3.426927328109741\n",
            "step 3601: loss = 3.227423667907715\n",
            "step 3602: loss = 3.451805591583252\n",
            "step 3603: loss = 3.0322225093841553\n",
            "step 3604: loss = 3.1971993446350098\n",
            "step 3605: loss = 3.4751195907592773\n",
            "step 3606: loss = 3.3697261810302734\n",
            "step 3607: loss = 3.75184965133667\n",
            "step 3608: loss = 3.8247435092926025\n",
            "step 3609: loss = 3.4995405673980713\n",
            "step 3610: loss = 3.4459269046783447\n",
            "step 3611: loss = 3.3321754932403564\n",
            "step 3612: loss = 3.5070323944091797\n",
            "step 3613: loss = 3.37245774269104\n",
            "step 3614: loss = 3.46242356300354\n",
            "step 3615: loss = 3.4204442501068115\n",
            "step 3616: loss = 3.487867593765259\n",
            "step 3617: loss = 3.4098055362701416\n",
            "step 3618: loss = 3.2979602813720703\n",
            "step 3619: loss = 3.4228601455688477\n",
            "step 3620: loss = 3.5662319660186768\n",
            "step 3621: loss = 3.634697437286377\n",
            "step 3622: loss = 3.4987423419952393\n",
            "step 3623: loss = 3.6600968837738037\n",
            "step 3624: loss = 3.460973024368286\n",
            "step 3625: loss = 3.3754851818084717\n",
            "step 3626: loss = 3.4965994358062744\n",
            "step 3627: loss = 3.340654134750366\n",
            "step 3628: loss = 3.357006072998047\n",
            "step 3629: loss = 3.373223066329956\n",
            "step 3630: loss = 3.609574794769287\n",
            "step 3631: loss = 3.4012866020202637\n",
            "step 3632: loss = 3.2845444679260254\n",
            "step 3633: loss = 3.382897138595581\n",
            "step 3634: loss = 3.5294382572174072\n",
            "step 3635: loss = 3.525014877319336\n",
            "step 3636: loss = 3.259504556655884\n",
            "step 3637: loss = 3.380856513977051\n",
            "step 3638: loss = 3.1248843669891357\n",
            "step 3639: loss = 3.4090793132781982\n",
            "step 3640: loss = 3.2850303649902344\n",
            "step 3641: loss = 3.3313913345336914\n",
            "step 3642: loss = 3.5327048301696777\n",
            "step 3643: loss = 3.542607069015503\n",
            "step 3644: loss = 3.4415881633758545\n",
            "step 3645: loss = 3.355344533920288\n",
            "step 3646: loss = 3.500919818878174\n",
            "step 3647: loss = 3.277096748352051\n",
            "step 3648: loss = 3.4325146675109863\n",
            "step 3649: loss = 3.683130979537964\n",
            "step 3650: loss = 3.5974230766296387\n",
            "step 3651: loss = 3.563904047012329\n",
            "step 3652: loss = 3.4589874744415283\n",
            "step 3653: loss = 3.2457070350646973\n",
            "step 3654: loss = 3.6412241458892822\n",
            "step 3655: loss = 3.254749059677124\n",
            "step 3656: loss = 3.5963642597198486\n",
            "step 3657: loss = 3.2572762966156006\n",
            "step 3658: loss = 3.4241442680358887\n",
            "step 3659: loss = 3.5986156463623047\n",
            "step 3660: loss = 3.329193592071533\n",
            "step 3661: loss = 3.808058261871338\n",
            "step 3662: loss = 3.2725045680999756\n",
            "step 3663: loss = 3.473585605621338\n",
            "step 3664: loss = 3.378109931945801\n",
            "step 3665: loss = 3.5701582431793213\n",
            "step 3666: loss = 3.322601079940796\n",
            "step 3667: loss = 3.316897392272949\n",
            "step 3668: loss = 3.4049417972564697\n",
            "step 3669: loss = 3.454000473022461\n",
            "step 3670: loss = 3.295847177505493\n",
            "step 3671: loss = 3.5088765621185303\n",
            "step 3672: loss = 3.269923686981201\n",
            "step 3673: loss = 3.7284884452819824\n",
            "step 3674: loss = 3.5998787879943848\n",
            "step 3675: loss = 3.746309995651245\n",
            "step 3676: loss = 3.156344413757324\n",
            "step 3677: loss = 3.1448967456817627\n",
            "step 3678: loss = 3.0916099548339844\n",
            "step 3679: loss = 3.526712417602539\n",
            "step 3680: loss = 3.4090003967285156\n",
            "step 3681: loss = 3.3480656147003174\n",
            "step 3682: loss = 3.6388182640075684\n",
            "step 3683: loss = 3.4568028450012207\n",
            "step 3684: loss = 3.286665916442871\n",
            "step 3685: loss = 3.378108263015747\n",
            "step 3686: loss = 3.720571756362915\n",
            "step 3687: loss = 3.1209888458251953\n",
            "step 3688: loss = 3.2629618644714355\n",
            "step 3689: loss = 3.627894163131714\n",
            "step 3690: loss = 3.6454050540924072\n",
            "step 3691: loss = 3.0933077335357666\n",
            "step 3692: loss = 3.4140572547912598\n",
            "step 3693: loss = 3.385313034057617\n",
            "step 3694: loss = 3.352299690246582\n",
            "step 3695: loss = 3.3085777759552\n",
            "step 3696: loss = 3.4947493076324463\n",
            "step 3697: loss = 3.2757225036621094\n",
            "step 3698: loss = 3.630913734436035\n",
            "step 3699: loss = 3.2963297367095947\n",
            "step 3700: loss = 3.3724255561828613\n",
            "step 3701: loss = 3.6288650035858154\n",
            "step 3702: loss = 3.583012104034424\n",
            "step 3703: loss = 3.4328978061676025\n",
            "step 3704: loss = 3.189948320388794\n",
            "step 3705: loss = 3.095885753631592\n",
            "step 3706: loss = 3.3429274559020996\n",
            "step 3707: loss = 3.6043009757995605\n",
            "step 3708: loss = 3.156493663787842\n",
            "step 3709: loss = 3.3967268466949463\n",
            "step 3710: loss = 3.389940023422241\n",
            "step 3711: loss = 3.0863037109375\n",
            "step 3712: loss = 3.3863141536712646\n",
            "step 3713: loss = 3.4990837574005127\n",
            "step 3714: loss = 3.6157066822052\n",
            "step 3715: loss = 3.699171781539917\n",
            "step 3716: loss = 3.29164457321167\n",
            "step 3717: loss = 3.3007218837738037\n",
            "step 3718: loss = 3.5599679946899414\n",
            "step 3719: loss = 3.456489086151123\n",
            "step 3720: loss = 3.2835769653320312\n",
            "step 3721: loss = 3.6125330924987793\n",
            "step 3722: loss = 3.358819007873535\n",
            "step 3723: loss = 3.247853994369507\n",
            "step 3724: loss = 3.455585241317749\n",
            "step 3725: loss = 3.548976421356201\n",
            "step 3726: loss = 3.292240619659424\n",
            "step 3727: loss = 3.3521950244903564\n",
            "step 3728: loss = 3.4702556133270264\n",
            "step 3729: loss = 3.49463152885437\n",
            "step 3730: loss = 3.5302796363830566\n",
            "step 3731: loss = 3.5706937313079834\n",
            "step 3732: loss = 3.2379462718963623\n",
            "step 3733: loss = 3.2341480255126953\n",
            "step 3734: loss = 3.5414695739746094\n",
            "step 3735: loss = 3.4808287620544434\n",
            "step 3736: loss = 3.9363672733306885\n",
            "step 3737: loss = 3.6117477416992188\n",
            "step 3738: loss = 3.6099295616149902\n",
            "step 3739: loss = 3.560340166091919\n",
            "step 3740: loss = 3.461740016937256\n",
            "step 3741: loss = 3.45638370513916\n",
            "step 3742: loss = 3.102017879486084\n",
            "step 3743: loss = 3.267634391784668\n",
            "step 3744: loss = 3.6776421070098877\n",
            "step 3745: loss = 3.459334135055542\n",
            "step 3746: loss = 3.4513468742370605\n",
            "step 3747: loss = 3.409945249557495\n",
            "step 3748: loss = 3.4838924407958984\n",
            "step 3749: loss = 2.893855571746826\n",
            "step 3750: loss = 3.355349540710449\n",
            "step 3751: loss = 3.6582818031311035\n",
            "step 3752: loss = 3.3180806636810303\n",
            "step 3753: loss = 3.272498846054077\n",
            "step 3754: loss = 3.3231232166290283\n",
            "step 3755: loss = 3.5051681995391846\n",
            "step 3756: loss = 3.1644389629364014\n",
            "step 3757: loss = 3.3003268241882324\n",
            "step 3758: loss = 3.1425843238830566\n",
            "step 3759: loss = 3.4605510234832764\n",
            "step 3760: loss = 3.2790911197662354\n",
            "step 3761: loss = 3.800246477127075\n",
            "step 3762: loss = 3.5967459678649902\n",
            "step 3763: loss = 3.42524790763855\n",
            "step 3764: loss = 3.083209991455078\n",
            "step 3765: loss = 3.504870653152466\n",
            "step 3766: loss = 3.6129863262176514\n",
            "step 3767: loss = 3.578052043914795\n",
            "step 3768: loss = 3.4046432971954346\n",
            "step 3769: loss = 3.289959192276001\n",
            "step 3770: loss = 3.2753467559814453\n",
            "step 3771: loss = 3.517834186553955\n",
            "step 3772: loss = 3.2952969074249268\n",
            "step 3773: loss = 3.3330023288726807\n",
            "step 3774: loss = 3.6862258911132812\n",
            "step 3775: loss = 3.271074056625366\n",
            "step 3776: loss = 3.590397357940674\n",
            "step 3777: loss = 3.196810483932495\n",
            "step 3778: loss = 3.4411325454711914\n",
            "step 3779: loss = 3.5098533630371094\n",
            "step 3780: loss = 3.746910333633423\n",
            "step 3781: loss = 3.6344168186187744\n",
            "step 3782: loss = 3.1988041400909424\n",
            "step 3783: loss = 3.396331548690796\n",
            "step 3784: loss = 3.041879653930664\n",
            "step 3785: loss = 3.432084560394287\n",
            "step 3786: loss = 3.619184732437134\n",
            "step 3787: loss = 3.737226963043213\n",
            "step 3788: loss = 3.4833855628967285\n",
            "step 3789: loss = 3.7771124839782715\n",
            "step 3790: loss = 3.3830513954162598\n",
            "step 3791: loss = 3.322636127471924\n",
            "step 3792: loss = 3.4799585342407227\n",
            "step 3793: loss = 3.134316921234131\n",
            "step 3794: loss = 3.217688798904419\n",
            "step 3795: loss = 3.400001049041748\n",
            "step 3796: loss = 3.1653623580932617\n",
            "step 3797: loss = 3.3395488262176514\n",
            "step 3798: loss = 3.2599847316741943\n",
            "step 3799: loss = 3.2422688007354736\n",
            "step 3800: loss = 3.0786733627319336\n",
            "step 3801: loss = 3.328842878341675\n",
            "step 3802: loss = 3.4665071964263916\n",
            "step 3803: loss = 3.2367782592773438\n",
            "step 3804: loss = 3.4993159770965576\n",
            "step 3805: loss = 3.166530132293701\n",
            "step 3806: loss = 3.3206779956817627\n",
            "step 3807: loss = 3.5603854656219482\n",
            "step 3808: loss = 2.9604923725128174\n",
            "step 3809: loss = 3.4049460887908936\n",
            "step 3810: loss = 3.292250633239746\n",
            "step 3811: loss = 3.333721399307251\n",
            "step 3812: loss = 3.9320788383483887\n",
            "step 3813: loss = 3.3759193420410156\n",
            "step 3814: loss = 3.242851734161377\n",
            "step 3815: loss = 3.5870778560638428\n",
            "step 3816: loss = 3.272902250289917\n",
            "step 3817: loss = 3.1742026805877686\n",
            "step 3818: loss = 3.536888599395752\n",
            "step 3819: loss = 3.447385311126709\n",
            "step 3820: loss = 3.4969642162323\n",
            "step 3821: loss = 3.6555962562561035\n",
            "step 3822: loss = 3.351008415222168\n",
            "step 3823: loss = 3.300415277481079\n",
            "step 3824: loss = 3.6114509105682373\n",
            "step 3825: loss = 2.9135799407958984\n",
            "step 3826: loss = 3.33846378326416\n",
            "step 3827: loss = 3.1922686100006104\n",
            "step 3828: loss = 3.4795925617218018\n",
            "step 3829: loss = 3.372821092605591\n",
            "step 3830: loss = 3.602346420288086\n",
            "step 3831: loss = 3.456315517425537\n",
            "step 3832: loss = 3.584599018096924\n",
            "step 3833: loss = 3.5578653812408447\n",
            "step 3834: loss = 3.1790974140167236\n",
            "step 3835: loss = 3.3566458225250244\n",
            "step 3836: loss = 3.70957088470459\n",
            "step 3837: loss = 3.194387674331665\n",
            "step 3838: loss = 3.255563735961914\n",
            "step 3839: loss = 3.3267390727996826\n",
            "step 3840: loss = 3.3253815174102783\n",
            "step 3841: loss = 3.551234006881714\n",
            "step 3842: loss = 3.8394484519958496\n",
            "step 3843: loss = 3.543602466583252\n",
            "step 3844: loss = 3.1356613636016846\n",
            "step 3845: loss = 3.404067039489746\n",
            "step 3846: loss = 3.6319315433502197\n",
            "step 3847: loss = 3.019397735595703\n",
            "step 3848: loss = 3.3461453914642334\n",
            "step 3849: loss = 3.651611328125\n",
            "step 3850: loss = 3.481971025466919\n",
            "step 3851: loss = 3.408254861831665\n",
            "step 3852: loss = 3.1580541133880615\n",
            "step 3853: loss = 3.411086082458496\n",
            "step 3854: loss = 3.032594680786133\n",
            "step 3855: loss = 3.3738319873809814\n",
            "step 3856: loss = 3.2930147647857666\n",
            "step 3857: loss = 3.4557509422302246\n",
            "step 3858: loss = 3.792595863342285\n",
            "step 3859: loss = 3.1937143802642822\n",
            "step 3860: loss = 3.1912107467651367\n",
            "step 3861: loss = 3.6077158451080322\n",
            "step 3862: loss = 3.262498378753662\n",
            "step 3863: loss = 3.3030412197113037\n",
            "step 3864: loss = 3.396578073501587\n",
            "step 3865: loss = 3.239619731903076\n",
            "step 3866: loss = 2.8851876258850098\n",
            "step 3867: loss = 3.5255513191223145\n",
            "step 3868: loss = 3.4619545936584473\n",
            "step 3869: loss = 3.3853204250335693\n",
            "step 3870: loss = 3.5017318725585938\n",
            "step 3871: loss = 3.2183918952941895\n",
            "step 3872: loss = 3.2701354026794434\n",
            "step 3873: loss = 3.6099085807800293\n",
            "step 3874: loss = 3.3717033863067627\n",
            "step 3875: loss = 3.274567127227783\n",
            "step 3876: loss = 3.2354342937469482\n",
            "step 3877: loss = 3.3453967571258545\n",
            "step 3878: loss = 3.4590342044830322\n",
            "step 3879: loss = 3.676121473312378\n",
            "step 3880: loss = 3.383028030395508\n",
            "step 3881: loss = 3.410048723220825\n",
            "step 3882: loss = 3.5480024814605713\n",
            "step 3883: loss = 3.630154609680176\n",
            "step 3884: loss = 3.2195193767547607\n",
            "step 3885: loss = 3.3202686309814453\n",
            "step 3886: loss = 3.2603049278259277\n",
            "step 3887: loss = 3.358157157897949\n",
            "step 3888: loss = 3.5260159969329834\n",
            "step 3889: loss = 3.224766254425049\n",
            "step 3890: loss = 3.537135124206543\n",
            "step 3891: loss = 3.3952393531799316\n",
            "step 3892: loss = 3.661187171936035\n",
            "step 3893: loss = 3.368011236190796\n",
            "step 3894: loss = 3.5327863693237305\n",
            "step 3895: loss = 3.5043132305145264\n",
            "step 3896: loss = 3.0915307998657227\n",
            "step 3897: loss = 3.4393789768218994\n",
            "step 3898: loss = 3.560692071914673\n",
            "step 3899: loss = 3.3625741004943848\n",
            "step 3900: loss = 3.4091556072235107\n",
            "step 3901: loss = 3.8773036003112793\n",
            "step 3902: loss = 3.6338486671447754\n",
            "step 3903: loss = 3.5476932525634766\n",
            "step 3904: loss = 3.1646289825439453\n",
            "step 3905: loss = 3.265838146209717\n",
            "step 3906: loss = 3.383016586303711\n",
            "step 3907: loss = 3.1243858337402344\n",
            "step 3908: loss = 3.5457215309143066\n",
            "step 3909: loss = 3.5964155197143555\n",
            "step 3910: loss = 3.2522027492523193\n",
            "step 3911: loss = 3.212658166885376\n",
            "step 3912: loss = 3.443568468093872\n",
            "step 3913: loss = 3.490450620651245\n",
            "step 3914: loss = 3.3707363605499268\n",
            "step 3915: loss = 3.2570295333862305\n",
            "step 3916: loss = 3.2923316955566406\n",
            "step 3917: loss = 3.7286453247070312\n",
            "step 3918: loss = 3.5004477500915527\n",
            "step 3919: loss = 3.327160596847534\n",
            "step 3920: loss = 3.4362525939941406\n",
            "step 3921: loss = 3.3454031944274902\n",
            "step 3922: loss = 3.372523784637451\n",
            "step 3923: loss = 3.2970001697540283\n",
            "step 3924: loss = 3.529578447341919\n",
            "step 3925: loss = 3.601459503173828\n",
            "step 3926: loss = 3.232192039489746\n",
            "step 3927: loss = 3.250270366668701\n",
            "step 3928: loss = 3.295236587524414\n",
            "step 3929: loss = 3.3321402072906494\n",
            "step 3930: loss = 3.4801411628723145\n",
            "step 3931: loss = 3.1640188694000244\n",
            "step 3932: loss = 3.30269718170166\n",
            "step 3933: loss = 3.4426541328430176\n",
            "step 3934: loss = 3.1885764598846436\n",
            "step 3935: loss = 3.2080044746398926\n",
            "step 3936: loss = 3.5323903560638428\n",
            "step 3937: loss = 3.4411866664886475\n",
            "step 3938: loss = 3.602607488632202\n",
            "step 3939: loss = 3.502078056335449\n",
            "step 3940: loss = 3.3061680793762207\n",
            "step 3941: loss = 3.3440375328063965\n",
            "step 3942: loss = 3.406893730163574\n",
            "step 3943: loss = 3.225050210952759\n",
            "step 3944: loss = 3.434372901916504\n",
            "step 3945: loss = 3.7882425785064697\n",
            "step 3946: loss = 3.4684176445007324\n",
            "step 3947: loss = 3.490460157394409\n",
            "step 3948: loss = 3.605583429336548\n",
            "step 3949: loss = 3.710402727127075\n",
            "step 3950: loss = 3.036130905151367\n",
            "step 3951: loss = 3.3536369800567627\n",
            "step 3952: loss = 3.081974983215332\n",
            "step 3953: loss = 3.5280532836914062\n",
            "step 3954: loss = 3.1796960830688477\n",
            "step 3955: loss = 3.440046548843384\n",
            "step 3956: loss = 3.377930164337158\n",
            "step 3957: loss = 2.795961618423462\n",
            "step 3958: loss = 3.441629409790039\n",
            "step 3959: loss = 3.2781612873077393\n",
            "step 3960: loss = 3.5683515071868896\n",
            "step 3961: loss = 3.401860475540161\n",
            "step 3962: loss = 3.3013975620269775\n",
            "step 3963: loss = 3.466552257537842\n",
            "step 3964: loss = 3.377842426300049\n",
            "step 3965: loss = 3.467738151550293\n",
            "step 3966: loss = 3.386773109436035\n",
            "step 3967: loss = 2.9872894287109375\n",
            "step 3968: loss = 3.305511474609375\n",
            "step 3969: loss = 3.238755941390991\n",
            "step 3970: loss = 3.5551106929779053\n",
            "step 3971: loss = 3.43526291847229\n",
            "step 3972: loss = 3.732448101043701\n",
            "step 3973: loss = 3.4131500720977783\n",
            "step 3974: loss = 3.432061195373535\n",
            "step 3975: loss = 3.3796679973602295\n",
            "step 3976: loss = 3.5415570735931396\n",
            "step 3977: loss = 3.1585474014282227\n",
            "step 3978: loss = 3.4207215309143066\n",
            "step 3979: loss = 3.643998861312866\n",
            "step 3980: loss = 3.613940715789795\n",
            "step 3981: loss = 3.239636182785034\n",
            "step 3982: loss = 3.1535704135894775\n",
            "step 3983: loss = 3.114003896713257\n",
            "step 3984: loss = 3.1854209899902344\n",
            "step 3985: loss = 3.465068817138672\n",
            "step 3986: loss = 3.200162172317505\n",
            "step 3987: loss = 3.7502601146698\n",
            "step 3988: loss = 3.2572338581085205\n",
            "step 3989: loss = 3.3550808429718018\n",
            "step 3990: loss = 3.3840479850769043\n",
            "step 3991: loss = 3.2545840740203857\n",
            "step 3992: loss = 3.427067518234253\n",
            "step 3993: loss = 3.391479730606079\n",
            "step 3994: loss = 3.376802444458008\n",
            "step 3995: loss = 3.566206216812134\n",
            "step 3996: loss = 3.2385339736938477\n",
            "step 3997: loss = 3.2705435752868652\n",
            "step 3998: loss = 3.664008140563965\n",
            "step 3999: loss = 3.949920892715454\n",
            "step 4000: loss = 3.323561668395996\n",
            "step 4001: loss = 3.2156713008880615\n",
            "step 4002: loss = 3.282438039779663\n",
            "step 4003: loss = 3.1439056396484375\n",
            "step 4004: loss = 3.2535767555236816\n",
            "step 4005: loss = 3.0247349739074707\n",
            "step 4006: loss = 3.436161518096924\n",
            "step 4007: loss = 3.3936119079589844\n",
            "step 4008: loss = 3.2566001415252686\n",
            "step 4009: loss = 3.276104688644409\n",
            "step 4010: loss = 3.4906346797943115\n",
            "step 4011: loss = 3.7308926582336426\n",
            "step 4012: loss = 2.823023796081543\n",
            "step 4013: loss = 3.4439003467559814\n",
            "step 4014: loss = 3.4579436779022217\n",
            "step 4015: loss = 3.4306302070617676\n",
            "step 4016: loss = 3.789083480834961\n",
            "step 4017: loss = 3.305149555206299\n",
            "step 4018: loss = 2.9479587078094482\n",
            "step 4019: loss = 3.4278666973114014\n",
            "step 4020: loss = 3.3381803035736084\n",
            "step 4021: loss = 3.1186776161193848\n",
            "step 4022: loss = 3.310300827026367\n",
            "step 4023: loss = 3.431062936782837\n",
            "step 4024: loss = 3.518841028213501\n",
            "step 4025: loss = 3.2016592025756836\n",
            "step 4026: loss = 3.092724084854126\n",
            "step 4027: loss = 3.4193990230560303\n",
            "step 4028: loss = 3.3410000801086426\n",
            "step 4029: loss = 3.472459554672241\n",
            "step 4030: loss = 3.6699891090393066\n",
            "step 4031: loss = 3.549211263656616\n",
            "step 4032: loss = 3.7252447605133057\n",
            "step 4033: loss = 3.57987117767334\n",
            "step 4034: loss = 3.3998093605041504\n",
            "step 4035: loss = 3.2315924167633057\n",
            "step 4036: loss = 3.20174503326416\n",
            "step 4037: loss = 3.77386474609375\n",
            "step 4038: loss = 3.3426132202148438\n",
            "step 4039: loss = 3.097522735595703\n",
            "step 4040: loss = 3.3691718578338623\n",
            "step 4041: loss = 3.545224905014038\n",
            "step 4042: loss = 3.2687439918518066\n",
            "step 4043: loss = 3.1742095947265625\n",
            "step 4044: loss = 3.093977212905884\n",
            "step 4045: loss = 3.5635342597961426\n",
            "step 4046: loss = 3.084779739379883\n",
            "step 4047: loss = 3.3742406368255615\n",
            "step 4048: loss = 3.7664802074432373\n",
            "step 4049: loss = 3.448350667953491\n",
            "step 4050: loss = 3.355225086212158\n",
            "step 4051: loss = 3.189776659011841\n",
            "step 4052: loss = 3.389740467071533\n",
            "step 4053: loss = 3.327338695526123\n",
            "step 4054: loss = 3.078031063079834\n",
            "step 4055: loss = 3.5188982486724854\n",
            "step 4056: loss = 3.517097234725952\n",
            "step 4057: loss = 3.328369379043579\n",
            "step 4058: loss = 3.529711961746216\n",
            "step 4059: loss = 3.3865103721618652\n",
            "step 4060: loss = 2.9546568393707275\n",
            "step 4061: loss = 3.188927412033081\n",
            "step 4062: loss = 3.3777835369110107\n",
            "step 4063: loss = 3.3368678092956543\n",
            "step 4064: loss = 3.529012680053711\n",
            "step 4065: loss = 3.635460138320923\n",
            "step 4066: loss = 3.3943533897399902\n",
            "step 4067: loss = 3.463693380355835\n",
            "step 4068: loss = 3.3081133365631104\n",
            "step 4069: loss = 3.60357403755188\n",
            "step 4070: loss = 3.521211624145508\n",
            "step 4071: loss = 3.6202616691589355\n",
            "step 4072: loss = 3.2858901023864746\n",
            "step 4073: loss = 3.2117552757263184\n",
            "step 4074: loss = 3.4420104026794434\n",
            "step 4075: loss = 3.5193371772766113\n",
            "step 4076: loss = 3.600517988204956\n",
            "step 4077: loss = 3.458031177520752\n",
            "step 4078: loss = 3.5160818099975586\n",
            "step 4079: loss = 3.3322994709014893\n",
            "step 4080: loss = 3.488668918609619\n",
            "step 4081: loss = 3.572303533554077\n",
            "step 4082: loss = 3.8873238563537598\n",
            "step 4083: loss = 3.560612678527832\n",
            "step 4084: loss = 3.58007550239563\n",
            "step 4085: loss = 3.3028788566589355\n",
            "step 4086: loss = 3.1880059242248535\n",
            "step 4087: loss = 3.302217483520508\n",
            "step 4088: loss = 3.5614802837371826\n",
            "step 4089: loss = 3.406313896179199\n",
            "step 4090: loss = 3.2557406425476074\n",
            "step 4091: loss = 3.5919010639190674\n",
            "step 4092: loss = 3.2009782791137695\n",
            "step 4093: loss = 3.129692316055298\n",
            "step 4094: loss = 3.671374559402466\n",
            "step 4095: loss = 3.3639066219329834\n",
            "step 4096: loss = 3.7007696628570557\n",
            "step 4097: loss = 3.2529287338256836\n",
            "step 4098: loss = 3.708211898803711\n",
            "step 4099: loss = 3.6533749103546143\n",
            "step 4100: loss = 3.473020076751709\n",
            "step 4101: loss = 3.4691033363342285\n",
            "step 4102: loss = 3.047553539276123\n",
            "step 4103: loss = 3.538268566131592\n",
            "step 4104: loss = 3.4026176929473877\n",
            "step 4105: loss = 3.276540756225586\n",
            "step 4106: loss = 3.267974376678467\n",
            "step 4107: loss = 3.4572176933288574\n",
            "step 4108: loss = 3.199340581893921\n",
            "step 4109: loss = 3.338214874267578\n",
            "step 4110: loss = 3.4800095558166504\n",
            "step 4111: loss = 3.5005483627319336\n",
            "step 4112: loss = 3.2722671031951904\n",
            "step 4113: loss = 3.6557862758636475\n",
            "step 4114: loss = 3.415937662124634\n",
            "step 4115: loss = 3.1504335403442383\n",
            "step 4116: loss = 3.501055955886841\n",
            "step 4117: loss = 3.3593027591705322\n",
            "step 4118: loss = 3.4558448791503906\n",
            "step 4119: loss = 3.287534236907959\n",
            "step 4120: loss = 3.169833183288574\n",
            "step 4121: loss = 3.415242910385132\n",
            "step 4122: loss = 3.436347484588623\n",
            "step 4123: loss = 3.4872937202453613\n",
            "step 4124: loss = 3.548704147338867\n",
            "step 4125: loss = 3.2006819248199463\n",
            "step 4126: loss = 3.3408586978912354\n",
            "step 4127: loss = 3.3351151943206787\n",
            "step 4128: loss = 3.311387538909912\n",
            "step 4129: loss = 3.423074960708618\n",
            "step 4130: loss = 3.448596715927124\n",
            "step 4131: loss = 3.5127689838409424\n",
            "step 4132: loss = 3.7048499584198\n",
            "step 4133: loss = 3.3937182426452637\n",
            "step 4134: loss = 3.547028064727783\n",
            "step 4135: loss = 3.1942625045776367\n",
            "step 4136: loss = 3.4617998600006104\n",
            "step 4137: loss = 3.4460268020629883\n",
            "step 4138: loss = 3.393005609512329\n",
            "step 4139: loss = 3.4903810024261475\n",
            "step 4140: loss = 3.410824775695801\n",
            "step 4141: loss = 3.875298023223877\n",
            "step 4142: loss = 3.316862106323242\n",
            "step 4143: loss = 3.0248265266418457\n",
            "step 4144: loss = 3.291538953781128\n",
            "step 4145: loss = 3.1229968070983887\n",
            "step 4146: loss = 3.523344039916992\n",
            "step 4147: loss = 3.5391533374786377\n",
            "step 4148: loss = 3.382671594619751\n",
            "step 4149: loss = 3.536623239517212\n",
            "step 4150: loss = 3.2123796939849854\n",
            "step 4151: loss = 3.482266426086426\n",
            "step 4152: loss = 3.3829474449157715\n",
            "step 4153: loss = 2.9925947189331055\n",
            "step 4154: loss = 3.2865118980407715\n",
            "step 4155: loss = 3.2453315258026123\n",
            "step 4156: loss = 3.363692283630371\n",
            "step 4157: loss = 3.201890230178833\n",
            "step 4158: loss = 3.4772000312805176\n",
            "step 4159: loss = 3.195841073989868\n",
            "step 4160: loss = 3.171233654022217\n",
            "step 4161: loss = 3.6291840076446533\n",
            "step 4162: loss = 3.3918769359588623\n",
            "step 4163: loss = 3.2933638095855713\n",
            "step 4164: loss = 3.497424602508545\n",
            "step 4165: loss = 3.2545011043548584\n",
            "step 4166: loss = 3.4673736095428467\n",
            "step 4167: loss = 3.522202253341675\n",
            "step 4168: loss = 3.201277256011963\n",
            "step 4169: loss = 3.270695209503174\n",
            "step 4170: loss = 3.581613779067993\n",
            "step 4171: loss = 3.3489389419555664\n",
            "step 4172: loss = 3.830531120300293\n",
            "step 4173: loss = 3.526526927947998\n",
            "step 4174: loss = 3.3669769763946533\n",
            "step 4175: loss = 3.4730899333953857\n",
            "step 4176: loss = 3.4619333744049072\n",
            "step 4177: loss = 3.283642530441284\n",
            "step 4178: loss = 3.4573910236358643\n",
            "step 4179: loss = 3.736953020095825\n",
            "step 4180: loss = 3.3380343914031982\n",
            "step 4181: loss = 3.3876450061798096\n",
            "step 4182: loss = 3.3387181758880615\n",
            "step 4183: loss = 3.4913570880889893\n",
            "step 4184: loss = 3.5375497341156006\n",
            "step 4185: loss = 3.2236618995666504\n",
            "step 4186: loss = 3.330980062484741\n",
            "step 4187: loss = 3.6571531295776367\n",
            "step 4188: loss = 3.6017282009124756\n",
            "step 4189: loss = 3.1562914848327637\n",
            "step 4190: loss = 3.2142386436462402\n",
            "step 4191: loss = 3.3615736961364746\n",
            "step 4192: loss = 3.4520187377929688\n",
            "step 4193: loss = 3.38527774810791\n",
            "step 4194: loss = 3.451080322265625\n",
            "step 4195: loss = 3.3728976249694824\n",
            "step 4196: loss = 3.3007071018218994\n",
            "step 4197: loss = 3.465430974960327\n",
            "step 4198: loss = 3.6019885540008545\n",
            "step 4199: loss = 3.2081568241119385\n",
            "step 4200: loss = 3.284874439239502\n",
            "step 4201: loss = 3.5087130069732666\n",
            "step 4202: loss = 3.485550880432129\n",
            "step 4203: loss = 3.600233316421509\n",
            "step 4204: loss = 3.3065919876098633\n",
            "step 4205: loss = 3.674443483352661\n",
            "step 4206: loss = 3.0681405067443848\n",
            "step 4207: loss = 3.4431838989257812\n",
            "step 4208: loss = 3.4114840030670166\n",
            "step 4209: loss = 3.5664148330688477\n",
            "step 4210: loss = 3.1733663082122803\n",
            "step 4211: loss = 3.359959602355957\n",
            "step 4212: loss = 3.2338366508483887\n",
            "step 4213: loss = 3.3053476810455322\n",
            "step 4214: loss = 3.392979383468628\n",
            "step 4215: loss = 3.4420993328094482\n",
            "step 4216: loss = 3.5493459701538086\n",
            "step 4217: loss = 3.6349098682403564\n",
            "step 4218: loss = 3.462153911590576\n",
            "step 4219: loss = 3.1825482845306396\n",
            "step 4220: loss = 3.7758500576019287\n",
            "step 4221: loss = 3.7416625022888184\n",
            "step 4222: loss = 3.3239331245422363\n",
            "step 4223: loss = 3.3758304119110107\n",
            "step 4224: loss = 3.4978444576263428\n",
            "step 4225: loss = 3.2866666316986084\n",
            "step 4226: loss = 3.179865598678589\n",
            "step 4227: loss = 3.039824962615967\n",
            "step 4228: loss = 3.212682008743286\n",
            "step 4229: loss = 3.313790798187256\n",
            "step 4230: loss = 3.085057020187378\n",
            "step 4231: loss = 3.3329296112060547\n",
            "step 4232: loss = 3.236440658569336\n",
            "step 4233: loss = 3.4697656631469727\n",
            "step 4234: loss = 3.544142246246338\n",
            "step 4235: loss = 3.487362861633301\n",
            "step 4236: loss = 3.3612616062164307\n",
            "step 4237: loss = 3.543879270553589\n",
            "step 4238: loss = 3.4914629459381104\n",
            "step 4239: loss = 3.4306142330169678\n",
            "step 4240: loss = 3.6007909774780273\n",
            "step 4241: loss = 3.214613199234009\n",
            "step 4242: loss = 3.4725120067596436\n",
            "step 4243: loss = 3.1662254333496094\n",
            "step 4244: loss = 3.0628702640533447\n",
            "step 4245: loss = 3.0462193489074707\n",
            "step 4246: loss = 3.443408250808716\n",
            "step 4247: loss = 3.6669042110443115\n",
            "step 4248: loss = 3.4797136783599854\n",
            "step 4249: loss = 3.4179024696350098\n",
            "step 4250: loss = 3.153317928314209\n",
            "step 4251: loss = 3.598853349685669\n",
            "step 4252: loss = 3.3548998832702637\n",
            "step 4253: loss = 3.2314093112945557\n",
            "step 4254: loss = 3.3831374645233154\n",
            "step 4255: loss = 3.3152096271514893\n",
            "step 4256: loss = 3.706092596054077\n",
            "step 4257: loss = 3.4093642234802246\n",
            "step 4258: loss = 3.0950469970703125\n",
            "step 4259: loss = 3.3674769401550293\n",
            "step 4260: loss = 3.4043211936950684\n",
            "step 4261: loss = 3.2458627223968506\n",
            "step 4262: loss = 3.250201463699341\n",
            "step 4263: loss = 3.2675247192382812\n",
            "step 4264: loss = 3.2324554920196533\n",
            "step 4265: loss = 3.131760835647583\n",
            "step 4266: loss = 3.2575795650482178\n",
            "step 4267: loss = 3.440155029296875\n",
            "step 4268: loss = 3.433741807937622\n",
            "step 4269: loss = 3.6346075534820557\n",
            "step 4270: loss = 3.28353214263916\n",
            "step 4271: loss = 3.3503966331481934\n",
            "step 4272: loss = 3.188908576965332\n",
            "step 4273: loss = 3.333059310913086\n",
            "step 4274: loss = 3.288980007171631\n",
            "step 4275: loss = 3.4001433849334717\n",
            "step 4276: loss = 3.261868953704834\n",
            "step 4277: loss = 3.34055233001709\n",
            "step 4278: loss = 3.412130117416382\n",
            "step 4279: loss = 3.1350302696228027\n",
            "step 4280: loss = 3.2765235900878906\n",
            "step 4281: loss = 3.771552324295044\n",
            "step 4282: loss = 3.619337320327759\n",
            "step 4283: loss = 3.273798704147339\n",
            "step 4284: loss = 3.36645770072937\n",
            "step 4285: loss = 3.3086929321289062\n",
            "step 4286: loss = 3.416285276412964\n",
            "step 4287: loss = 3.300269365310669\n",
            "step 4288: loss = 3.463348150253296\n",
            "step 4289: loss = 3.3642380237579346\n",
            "step 4290: loss = 3.4482340812683105\n",
            "step 4291: loss = 3.093780755996704\n",
            "step 4292: loss = 3.4448957443237305\n",
            "step 4293: loss = 2.9530858993530273\n",
            "step 4294: loss = 3.569225788116455\n",
            "step 4295: loss = 3.6313538551330566\n",
            "step 4296: loss = 3.0801117420196533\n",
            "step 4297: loss = 3.1265881061553955\n",
            "step 4298: loss = 3.284257411956787\n",
            "step 4299: loss = 3.5083556175231934\n",
            "step 4300: loss = 3.2813947200775146\n",
            "step 4301: loss = 3.2526464462280273\n",
            "step 4302: loss = 3.4870598316192627\n",
            "step 4303: loss = 3.6979260444641113\n",
            "step 4304: loss = 3.350181818008423\n",
            "step 4305: loss = 3.3411052227020264\n",
            "step 4306: loss = 3.7324836254119873\n",
            "step 4307: loss = 3.2712655067443848\n",
            "step 4308: loss = 3.217634916305542\n",
            "step 4309: loss = 3.508786201477051\n",
            "step 4310: loss = 3.3104894161224365\n",
            "step 4311: loss = 3.2475173473358154\n",
            "step 4312: loss = 3.3135552406311035\n",
            "step 4313: loss = 2.936760425567627\n",
            "step 4314: loss = 3.4898898601531982\n",
            "step 4315: loss = 3.8002872467041016\n",
            "step 4316: loss = 3.226705312728882\n",
            "step 4317: loss = 3.520237684249878\n",
            "step 4318: loss = 3.4637675285339355\n",
            "step 4319: loss = 3.110145330429077\n",
            "step 4320: loss = 3.186640501022339\n",
            "step 4321: loss = 3.3916656970977783\n",
            "step 4322: loss = 3.2420029640197754\n",
            "step 4323: loss = 3.5027432441711426\n",
            "step 4324: loss = 3.4010188579559326\n",
            "step 4325: loss = 3.5551676750183105\n",
            "step 4326: loss = 3.353292465209961\n",
            "step 4327: loss = 3.392416477203369\n",
            "step 4328: loss = 3.2181098461151123\n",
            "step 4329: loss = 3.8074917793273926\n",
            "step 4330: loss = 3.320725202560425\n",
            "step 4331: loss = 3.252258062362671\n",
            "step 4332: loss = 3.5242464542388916\n",
            "step 4333: loss = 3.1655306816101074\n",
            "step 4334: loss = 3.645352840423584\n",
            "step 4335: loss = 3.218090772628784\n",
            "step 4336: loss = 3.070437431335449\n",
            "step 4337: loss = 3.100847005844116\n",
            "step 4338: loss = 3.402344226837158\n",
            "step 4339: loss = 3.2885985374450684\n",
            "step 4340: loss = 3.2382731437683105\n",
            "step 4341: loss = 3.0164337158203125\n",
            "step 4342: loss = 3.209331512451172\n",
            "step 4343: loss = 3.376023292541504\n",
            "step 4344: loss = 3.2715415954589844\n",
            "step 4345: loss = 3.3769173622131348\n",
            "step 4346: loss = 3.77323579788208\n",
            "step 4347: loss = 3.4187638759613037\n",
            "step 4348: loss = 3.484428644180298\n",
            "step 4349: loss = 3.659494161605835\n",
            "step 4350: loss = 3.371307849884033\n",
            "step 4351: loss = 3.3982596397399902\n",
            "step 4352: loss = 3.3142106533050537\n",
            "step 4353: loss = 3.291501998901367\n",
            "step 4354: loss = 3.2753610610961914\n",
            "step 4355: loss = 3.3781466484069824\n",
            "step 4356: loss = 3.2800145149230957\n",
            "step 4357: loss = 3.131037473678589\n",
            "step 4358: loss = 3.155876636505127\n",
            "step 4359: loss = 3.3779003620147705\n",
            "step 4360: loss = 3.659928560256958\n",
            "step 4361: loss = 3.3028032779693604\n",
            "step 4362: loss = 3.442340612411499\n",
            "step 4363: loss = 3.617689371109009\n",
            "step 4364: loss = 3.4839043617248535\n",
            "step 4365: loss = 3.3006253242492676\n",
            "step 4366: loss = 3.443499803543091\n",
            "step 4367: loss = 3.4179766178131104\n",
            "step 4368: loss = 3.328464984893799\n",
            "step 4369: loss = 3.5107975006103516\n",
            "step 4370: loss = 3.383762836456299\n",
            "step 4371: loss = 3.606478452682495\n",
            "step 4372: loss = 3.3825783729553223\n",
            "step 4373: loss = 3.175502300262451\n",
            "step 4374: loss = 3.415754556655884\n",
            "step 4375: loss = 3.4610867500305176\n",
            "step 4376: loss = 3.5541460514068604\n",
            "step 4377: loss = 3.5906026363372803\n",
            "step 4378: loss = 3.357252836227417\n",
            "step 4379: loss = 3.3196911811828613\n",
            "step 4380: loss = 3.2185935974121094\n",
            "step 4381: loss = 3.2697606086730957\n",
            "step 4382: loss = 3.3039042949676514\n",
            "step 4383: loss = 3.3701794147491455\n",
            "step 4384: loss = 3.443290948867798\n",
            "step 4385: loss = 3.4532105922698975\n",
            "step 4386: loss = 3.394573926925659\n",
            "step 4387: loss = 3.5318846702575684\n",
            "step 4388: loss = 3.250213861465454\n",
            "step 4389: loss = 3.357560634613037\n",
            "step 4390: loss = 3.367445468902588\n",
            "step 4391: loss = 3.4780588150024414\n",
            "step 4392: loss = 3.660747528076172\n",
            "step 4393: loss = 3.2971057891845703\n",
            "step 4394: loss = 3.3976829051971436\n",
            "step 4395: loss = 3.621061086654663\n",
            "step 4396: loss = 3.1520438194274902\n",
            "step 4397: loss = 3.2652359008789062\n",
            "step 4398: loss = 3.147284746170044\n",
            "step 4399: loss = 3.4748291969299316\n",
            "step 4400: loss = 3.47287654876709\n",
            "step 4401: loss = 3.2266011238098145\n",
            "step 4402: loss = 3.3168418407440186\n",
            "step 4403: loss = 3.39981746673584\n",
            "step 4404: loss = 3.1483397483825684\n",
            "step 4405: loss = 3.53680419921875\n",
            "step 4406: loss = 3.4608066082000732\n",
            "step 4407: loss = 3.3154399394989014\n",
            "step 4408: loss = 3.2160074710845947\n",
            "step 4409: loss = 3.5230302810668945\n",
            "step 4410: loss = 3.4004218578338623\n",
            "step 4411: loss = 3.327321767807007\n",
            "step 4412: loss = 3.4045767784118652\n",
            "step 4413: loss = 3.4967055320739746\n",
            "step 4414: loss = 3.016848087310791\n",
            "step 4415: loss = 3.831559419631958\n",
            "step 4416: loss = 3.302877426147461\n",
            "step 4417: loss = 3.46165132522583\n",
            "step 4418: loss = 3.2714905738830566\n",
            "step 4419: loss = 3.5491108894348145\n",
            "step 4420: loss = 3.7747962474823\n",
            "step 4421: loss = 3.0212411880493164\n",
            "step 4422: loss = 3.339263677597046\n",
            "step 4423: loss = 3.1400821208953857\n",
            "step 4424: loss = 3.4850821495056152\n",
            "step 4425: loss = 3.296450614929199\n",
            "step 4426: loss = 3.393132209777832\n",
            "step 4427: loss = 3.545936346054077\n",
            "step 4428: loss = 3.3495712280273438\n",
            "step 4429: loss = 3.277878522872925\n",
            "step 4430: loss = 3.5064845085144043\n",
            "step 4431: loss = 3.187349796295166\n",
            "step 4432: loss = 3.335228681564331\n",
            "step 4433: loss = 3.562296152114868\n",
            "step 4434: loss = 3.353013753890991\n",
            "step 4435: loss = 3.620462417602539\n",
            "step 4436: loss = 3.2874271869659424\n",
            "step 4437: loss = 3.3491744995117188\n",
            "step 4438: loss = 3.1669280529022217\n",
            "step 4439: loss = 3.3910608291625977\n",
            "step 4440: loss = 3.4708011150360107\n",
            "step 4441: loss = 3.490436553955078\n",
            "step 4442: loss = 3.2410175800323486\n",
            "step 4443: loss = 3.3246707916259766\n",
            "step 4444: loss = 3.639313220977783\n",
            "step 4445: loss = 3.2427120208740234\n",
            "step 4446: loss = 3.008617639541626\n",
            "step 4447: loss = 3.590803384780884\n",
            "step 4448: loss = 3.0823049545288086\n",
            "step 4449: loss = 3.2259750366210938\n",
            "step 4450: loss = 3.3730978965759277\n",
            "step 4451: loss = 2.9874653816223145\n",
            "step 4452: loss = 3.628322124481201\n",
            "step 4453: loss = 3.117546558380127\n",
            "step 4454: loss = 3.5369317531585693\n",
            "step 4455: loss = 3.42611026763916\n",
            "step 4456: loss = 3.382769823074341\n",
            "step 4457: loss = 3.663578510284424\n",
            "step 4458: loss = 3.286412477493286\n",
            "step 4459: loss = 3.727977752685547\n",
            "step 4460: loss = 3.320490837097168\n",
            "step 4461: loss = 3.188458204269409\n",
            "step 4462: loss = 3.235788583755493\n",
            "step 4463: loss = 3.2509407997131348\n",
            "step 4464: loss = 3.1859042644500732\n",
            "step 4465: loss = 3.264580249786377\n",
            "step 4466: loss = 3.398390769958496\n",
            "step 4467: loss = 3.173781633377075\n",
            "step 4468: loss = 3.612299919128418\n",
            "step 4469: loss = 3.585089683532715\n",
            "step 4470: loss = 3.4302639961242676\n",
            "step 4471: loss = 3.1519012451171875\n",
            "step 4472: loss = 3.175842046737671\n",
            "step 4473: loss = 3.0921530723571777\n",
            "step 4474: loss = 3.4397175312042236\n",
            "step 4475: loss = 3.502000093460083\n",
            "step 4476: loss = 3.0760819911956787\n",
            "step 4477: loss = 3.3419814109802246\n",
            "step 4478: loss = 3.2399191856384277\n",
            "step 4479: loss = 3.0876498222351074\n",
            "step 4480: loss = 3.3438169956207275\n",
            "step 4481: loss = 3.618669271469116\n",
            "step 4482: loss = 3.4164559841156006\n",
            "step 4483: loss = 3.4766879081726074\n",
            "step 4484: loss = 3.289224863052368\n",
            "step 4485: loss = 3.5102813243865967\n",
            "step 4486: loss = 3.3574039936065674\n",
            "step 4487: loss = 3.2056660652160645\n",
            "step 4488: loss = 3.043243885040283\n",
            "step 4489: loss = 3.263117551803589\n",
            "step 4490: loss = 3.3290979862213135\n",
            "step 4491: loss = 3.0512285232543945\n",
            "step 4492: loss = 3.608480930328369\n",
            "step 4493: loss = 3.384287118911743\n",
            "step 4494: loss = 3.43131160736084\n",
            "step 4495: loss = 3.088434934616089\n",
            "step 4496: loss = 3.348829984664917\n",
            "step 4497: loss = 3.2674872875213623\n",
            "step 4498: loss = 3.5533905029296875\n",
            "step 4499: loss = 3.526883125305176\n",
            "step 4500: loss = 3.3380987644195557\n",
            "step 4501: loss = 3.141289710998535\n",
            "step 4502: loss = 3.2018604278564453\n",
            "step 4503: loss = 3.3770999908447266\n",
            "step 4504: loss = 3.47208833694458\n",
            "step 4505: loss = 3.39168381690979\n",
            "step 4506: loss = 3.398648500442505\n",
            "step 4507: loss = 3.474339723587036\n",
            "step 4508: loss = 3.4352192878723145\n",
            "step 4509: loss = 3.484710693359375\n",
            "step 4510: loss = 3.1128265857696533\n",
            "step 4511: loss = 3.6495020389556885\n",
            "step 4512: loss = 3.4574272632598877\n",
            "step 4513: loss = 3.5724146366119385\n",
            "step 4514: loss = 3.518460750579834\n",
            "step 4515: loss = 3.4266302585601807\n",
            "step 4516: loss = 3.5047802925109863\n",
            "step 4517: loss = 3.1554670333862305\n",
            "step 4518: loss = 3.175149917602539\n",
            "step 4519: loss = 3.464315176010132\n",
            "step 4520: loss = 3.319118022918701\n",
            "step 4521: loss = 2.9146623611450195\n",
            "step 4522: loss = 3.348252058029175\n",
            "step 4523: loss = 3.271182060241699\n",
            "step 4524: loss = 3.5168046951293945\n",
            "step 4525: loss = 3.0372202396392822\n",
            "step 4526: loss = 3.7056539058685303\n",
            "step 4527: loss = 3.368590831756592\n",
            "step 4528: loss = 3.1784229278564453\n",
            "step 4529: loss = 3.241272449493408\n",
            "step 4530: loss = 3.3228600025177\n",
            "step 4531: loss = 3.203824043273926\n",
            "step 4532: loss = 3.1082136631011963\n",
            "step 4533: loss = 3.207913398742676\n",
            "step 4534: loss = 3.0952422618865967\n",
            "step 4535: loss = 2.9988925457000732\n",
            "step 4536: loss = 3.3217451572418213\n",
            "step 4537: loss = 3.1198315620422363\n",
            "step 4538: loss = 3.2576701641082764\n",
            "step 4539: loss = 3.104299545288086\n",
            "step 4540: loss = 3.326007843017578\n",
            "step 4541: loss = 3.164767265319824\n",
            "step 4542: loss = 3.2745249271392822\n",
            "step 4543: loss = 3.2019948959350586\n",
            "step 4544: loss = 3.4606823921203613\n",
            "step 4545: loss = 3.3371472358703613\n",
            "step 4546: loss = 3.537529230117798\n",
            "step 4547: loss = 3.4462227821350098\n",
            "step 4548: loss = 3.3359217643737793\n",
            "step 4549: loss = 3.345036029815674\n",
            "step 4550: loss = 3.37673020362854\n",
            "step 4551: loss = 3.137998580932617\n",
            "step 4552: loss = 3.317671298980713\n",
            "step 4553: loss = 3.3976595401763916\n",
            "step 4554: loss = 3.354416847229004\n",
            "step 4555: loss = 3.321413993835449\n",
            "step 4556: loss = 3.0668468475341797\n",
            "step 4557: loss = 3.0449697971343994\n",
            "step 4558: loss = 3.2415871620178223\n",
            "step 4559: loss = 3.374037027359009\n",
            "step 4560: loss = 3.249014377593994\n",
            "step 4561: loss = 3.208050012588501\n",
            "step 4562: loss = 3.1997578144073486\n",
            "step 4563: loss = 3.5703864097595215\n",
            "step 4564: loss = 3.1942055225372314\n",
            "step 4565: loss = 3.412031650543213\n",
            "step 4566: loss = 3.3298466205596924\n",
            "step 4567: loss = 3.297045946121216\n",
            "step 4568: loss = 2.9572744369506836\n",
            "step 4569: loss = 3.380873203277588\n",
            "step 4570: loss = 3.489300489425659\n",
            "step 4571: loss = 3.0314059257507324\n",
            "step 4572: loss = 3.23171329498291\n",
            "step 4573: loss = 3.164332628250122\n",
            "step 4574: loss = 3.6468188762664795\n",
            "step 4575: loss = 3.344029188156128\n",
            "step 4576: loss = 3.3943886756896973\n",
            "step 4577: loss = 3.126291036605835\n",
            "step 4578: loss = 3.455576181411743\n",
            "step 4579: loss = 3.4476213455200195\n",
            "step 4580: loss = 3.3754453659057617\n",
            "step 4581: loss = 3.2190463542938232\n",
            "step 4582: loss = 3.532201051712036\n",
            "step 4583: loss = 3.3084394931793213\n",
            "step 4584: loss = 3.131134271621704\n",
            "step 4585: loss = 3.5682497024536133\n",
            "step 4586: loss = 3.4338595867156982\n",
            "step 4587: loss = 3.524789810180664\n",
            "step 4588: loss = 3.269507646560669\n",
            "step 4589: loss = 3.236149787902832\n",
            "step 4590: loss = 3.086425542831421\n",
            "step 4591: loss = 3.721642017364502\n",
            "step 4592: loss = 3.5192627906799316\n",
            "step 4593: loss = 3.290792226791382\n",
            "step 4594: loss = 3.3575942516326904\n",
            "step 4595: loss = 3.358006000518799\n",
            "step 4596: loss = 3.2186052799224854\n",
            "step 4597: loss = 3.3083250522613525\n",
            "step 4598: loss = 3.34648060798645\n",
            "step 4599: loss = 3.1782374382019043\n",
            "step 4600: loss = 3.0850584506988525\n",
            "step 4601: loss = 3.6177709102630615\n",
            "step 4602: loss = 3.5176010131835938\n",
            "step 4603: loss = 3.3877949714660645\n",
            "step 4604: loss = 3.144804000854492\n",
            "step 4605: loss = 3.5078628063201904\n",
            "step 4606: loss = 3.2176599502563477\n",
            "step 4607: loss = 3.4150869846343994\n",
            "step 4608: loss = 3.470086097717285\n",
            "step 4609: loss = 3.2568490505218506\n",
            "step 4610: loss = 3.256295680999756\n",
            "step 4611: loss = 3.122546672821045\n",
            "step 4612: loss = 3.1632256507873535\n",
            "step 4613: loss = 3.180405378341675\n",
            "step 4614: loss = 3.3716697692871094\n",
            "step 4615: loss = 3.310452461242676\n",
            "step 4616: loss = 3.410159111022949\n",
            "step 4617: loss = 2.9313905239105225\n",
            "step 4618: loss = 3.329277276992798\n",
            "step 4619: loss = 3.2859838008880615\n",
            "step 4620: loss = 3.3290576934814453\n",
            "step 4621: loss = 3.2391867637634277\n",
            "step 4622: loss = 3.318620443344116\n",
            "step 4623: loss = 3.0432965755462646\n",
            "step 4624: loss = 3.2760167121887207\n",
            "step 4625: loss = 3.4538092613220215\n",
            "step 4626: loss = 3.349050283432007\n",
            "step 4627: loss = 3.1775527000427246\n",
            "step 4628: loss = 3.338266611099243\n",
            "step 4629: loss = 3.403287410736084\n",
            "step 4630: loss = 3.48205828666687\n",
            "step 4631: loss = 3.362643241882324\n",
            "step 4632: loss = 3.3679261207580566\n",
            "step 4633: loss = 3.2499170303344727\n",
            "step 4634: loss = 3.4453787803649902\n",
            "step 4635: loss = 3.422703504562378\n",
            "step 4636: loss = 3.2072713375091553\n",
            "step 4637: loss = 3.2247626781463623\n",
            "step 4638: loss = 3.479853630065918\n",
            "step 4639: loss = 3.297140598297119\n",
            "step 4640: loss = 3.4781482219696045\n",
            "step 4641: loss = 3.051051616668701\n",
            "step 4642: loss = 3.4223406314849854\n",
            "step 4643: loss = 3.267230272293091\n",
            "step 4644: loss = 3.5356361865997314\n",
            "step 4645: loss = 3.2235357761383057\n",
            "step 4646: loss = 3.2471890449523926\n",
            "step 4647: loss = 3.2841525077819824\n",
            "step 4648: loss = 3.3970487117767334\n",
            "step 4649: loss = 3.458737850189209\n",
            "step 4650: loss = 3.2338356971740723\n",
            "step 4651: loss = 3.1028249263763428\n",
            "step 4652: loss = 3.151782512664795\n",
            "step 4653: loss = 3.511354923248291\n",
            "step 4654: loss = 3.6498632431030273\n",
            "step 4655: loss = 3.084481954574585\n",
            "step 4656: loss = 2.7767157554626465\n",
            "step 4657: loss = 3.2713968753814697\n",
            "step 4658: loss = 3.110607862472534\n",
            "step 4659: loss = 3.0708212852478027\n",
            "step 4660: loss = 3.3942794799804688\n",
            "step 4661: loss = 3.466625213623047\n",
            "step 4662: loss = 3.589210271835327\n",
            "step 4663: loss = 3.930765151977539\n",
            "step 4664: loss = 3.216242790222168\n",
            "step 4665: loss = 3.663226366043091\n",
            "step 4666: loss = 3.4005239009857178\n",
            "step 4667: loss = 3.3539397716522217\n",
            "step 4668: loss = 3.432335376739502\n",
            "step 4669: loss = 3.2178397178649902\n",
            "step 4670: loss = 3.5319957733154297\n",
            "step 4671: loss = 3.070389747619629\n",
            "step 4672: loss = 3.2984371185302734\n",
            "step 4673: loss = 3.6634535789489746\n",
            "step 4674: loss = 3.371114730834961\n",
            "step 4675: loss = 3.194532871246338\n",
            "step 4676: loss = 3.320544958114624\n",
            "step 4677: loss = 3.040982484817505\n",
            "step 4678: loss = 3.481753349304199\n",
            "step 4679: loss = 3.2157678604125977\n",
            "step 4680: loss = 3.489245891571045\n",
            "step 4681: loss = 3.623790740966797\n",
            "step 4682: loss = 3.5320029258728027\n",
            "step 4683: loss = 2.8675503730773926\n",
            "step 4684: loss = 3.473130464553833\n",
            "step 4685: loss = 3.662980079650879\n",
            "step 4686: loss = 3.2144320011138916\n",
            "Finish epoch 3\n",
            "New model saved, minimum loss: 3.3907292232623325 \n",
            "\n",
            "step 4687: loss = 3.063368797302246\n",
            "step 4688: loss = 2.671342134475708\n",
            "step 4689: loss = 2.849069833755493\n",
            "step 4690: loss = 3.1553990840911865\n",
            "step 4691: loss = 3.1289596557617188\n",
            "step 4692: loss = 2.8542087078094482\n",
            "step 4693: loss = 2.988352060317993\n",
            "step 4694: loss = 2.8166866302490234\n",
            "step 4695: loss = 2.898160696029663\n",
            "step 4696: loss = 2.994663715362549\n",
            "step 4697: loss = 2.763500452041626\n",
            "step 4698: loss = 2.9043421745300293\n",
            "step 4699: loss = 2.8988354206085205\n",
            "step 4700: loss = 2.896458387374878\n",
            "step 4701: loss = 2.808666706085205\n",
            "step 4702: loss = 3.0753533840179443\n",
            "step 4703: loss = 2.9910521507263184\n",
            "step 4704: loss = 2.9288909435272217\n",
            "step 4705: loss = 2.837136745452881\n",
            "step 4706: loss = 3.090000629425049\n",
            "step 4707: loss = 2.764350414276123\n",
            "step 4708: loss = 2.7456207275390625\n",
            "step 4709: loss = 2.628586769104004\n",
            "step 4710: loss = 2.9200632572174072\n",
            "step 4711: loss = 2.701922655105591\n",
            "step 4712: loss = 2.848890781402588\n",
            "step 4713: loss = 2.87982439994812\n",
            "step 4714: loss = 2.783700466156006\n",
            "step 4715: loss = 2.8778374195098877\n",
            "step 4716: loss = 2.720407009124756\n",
            "step 4717: loss = 2.920034170150757\n",
            "step 4718: loss = 2.8265607357025146\n",
            "step 4719: loss = 3.149198293685913\n",
            "step 4720: loss = 2.5219321250915527\n",
            "step 4721: loss = 2.9352312088012695\n",
            "step 4722: loss = 2.8104372024536133\n",
            "step 4723: loss = 3.2293787002563477\n",
            "step 4724: loss = 2.9862372875213623\n",
            "step 4725: loss = 2.7584121227264404\n",
            "step 4726: loss = 2.7064361572265625\n",
            "step 4727: loss = 2.5623693466186523\n",
            "step 4728: loss = 2.8486053943634033\n",
            "step 4729: loss = 2.979853868484497\n",
            "step 4730: loss = 3.067117214202881\n",
            "step 4731: loss = 2.9536123275756836\n",
            "step 4732: loss = 2.504363536834717\n",
            "step 4733: loss = 3.0391032695770264\n",
            "step 4734: loss = 2.8260433673858643\n",
            "step 4735: loss = 3.1251277923583984\n",
            "step 4736: loss = 2.8206591606140137\n",
            "step 4737: loss = 2.6514856815338135\n",
            "step 4738: loss = 2.8210766315460205\n",
            "step 4739: loss = 2.729217290878296\n",
            "step 4740: loss = 2.6430246829986572\n",
            "step 4741: loss = 2.854860782623291\n",
            "step 4742: loss = 2.8299977779388428\n",
            "step 4743: loss = 3.0540897846221924\n",
            "step 4744: loss = 2.4541525840759277\n",
            "step 4745: loss = 2.7654576301574707\n",
            "step 4746: loss = 2.780583381652832\n",
            "step 4747: loss = 2.7517740726470947\n",
            "step 4748: loss = 2.8845927715301514\n",
            "step 4749: loss = 2.602932929992676\n",
            "step 4750: loss = 3.063490390777588\n",
            "step 4751: loss = 2.8719322681427\n",
            "step 4752: loss = 2.7820515632629395\n",
            "step 4753: loss = 3.0025243759155273\n",
            "step 4754: loss = 2.675762414932251\n",
            "step 4755: loss = 3.0387189388275146\n",
            "step 4756: loss = 2.6565663814544678\n",
            "step 4757: loss = 2.9275238513946533\n",
            "step 4758: loss = 2.7231359481811523\n",
            "step 4759: loss = 2.784423351287842\n",
            "step 4760: loss = 2.8230245113372803\n",
            "step 4761: loss = 2.932673215866089\n",
            "step 4762: loss = 2.640852928161621\n",
            "step 4763: loss = 2.9253177642822266\n",
            "step 4764: loss = 2.9113192558288574\n",
            "step 4765: loss = 2.930999994277954\n",
            "step 4766: loss = 3.1564886569976807\n",
            "step 4767: loss = 2.698922872543335\n",
            "step 4768: loss = 2.875981330871582\n",
            "step 4769: loss = 2.9679696559906006\n",
            "step 4770: loss = 2.623239755630493\n",
            "step 4771: loss = 2.7753682136535645\n",
            "step 4772: loss = 2.821631908416748\n",
            "step 4773: loss = 2.753742218017578\n",
            "step 4774: loss = 2.7639405727386475\n",
            "step 4775: loss = 2.7929632663726807\n",
            "step 4776: loss = 2.7976937294006348\n",
            "step 4777: loss = 2.842167854309082\n",
            "step 4778: loss = 2.828338861465454\n",
            "step 4779: loss = 2.9479827880859375\n",
            "step 4780: loss = 2.7320451736450195\n",
            "step 4781: loss = 2.981433391571045\n",
            "step 4782: loss = 3.0938720703125\n",
            "step 4783: loss = 3.022864580154419\n",
            "step 4784: loss = 3.101433515548706\n",
            "step 4785: loss = 3.0901358127593994\n",
            "step 4786: loss = 2.8375244140625\n",
            "step 4787: loss = 3.2120330333709717\n",
            "step 4788: loss = 2.6121017932891846\n",
            "step 4789: loss = 2.7767553329467773\n",
            "step 4790: loss = 2.7314748764038086\n",
            "step 4791: loss = 2.8991637229919434\n",
            "step 4792: loss = 3.0006542205810547\n",
            "step 4793: loss = 2.750851631164551\n",
            "step 4794: loss = 2.9176132678985596\n",
            "step 4795: loss = 2.717545986175537\n",
            "step 4796: loss = 2.7226979732513428\n",
            "step 4797: loss = 2.8346242904663086\n",
            "step 4798: loss = 2.6670339107513428\n",
            "step 4799: loss = 2.757490396499634\n",
            "step 4800: loss = 2.9716930389404297\n",
            "step 4801: loss = 3.1140031814575195\n",
            "step 4802: loss = 2.8824031352996826\n",
            "step 4803: loss = 2.5489859580993652\n",
            "step 4804: loss = 2.755841016769409\n",
            "step 4805: loss = 2.67923903465271\n",
            "step 4806: loss = 3.033823013305664\n",
            "step 4807: loss = 2.8968420028686523\n",
            "step 4808: loss = 2.9846744537353516\n",
            "step 4809: loss = 2.8300907611846924\n",
            "step 4810: loss = 2.978492259979248\n",
            "step 4811: loss = 2.68002986907959\n",
            "step 4812: loss = 2.696962356567383\n",
            "step 4813: loss = 2.8098292350769043\n",
            "step 4814: loss = 3.072718381881714\n",
            "step 4815: loss = 2.9728407859802246\n",
            "step 4816: loss = 2.711491823196411\n",
            "step 4817: loss = 2.8187880516052246\n",
            "step 4818: loss = 2.891845941543579\n",
            "step 4819: loss = 2.5954320430755615\n",
            "step 4820: loss = 2.986844778060913\n",
            "step 4821: loss = 2.711305618286133\n",
            "step 4822: loss = 2.9960808753967285\n",
            "step 4823: loss = 2.899796485900879\n",
            "step 4824: loss = 2.76267671585083\n",
            "step 4825: loss = 2.733149528503418\n",
            "step 4826: loss = 2.9595518112182617\n",
            "step 4827: loss = 3.1403467655181885\n",
            "step 4828: loss = 2.856147050857544\n",
            "step 4829: loss = 2.76625919342041\n",
            "step 4830: loss = 2.6706371307373047\n",
            "step 4831: loss = 2.8998396396636963\n",
            "step 4832: loss = 2.8501462936401367\n",
            "step 4833: loss = 2.8933956623077393\n",
            "step 4834: loss = 2.784837245941162\n",
            "step 4835: loss = 2.729959726333618\n",
            "step 4836: loss = 2.794360637664795\n",
            "step 4837: loss = 2.7734856605529785\n",
            "step 4838: loss = 2.9321606159210205\n",
            "step 4839: loss = 3.0174903869628906\n",
            "step 4840: loss = 2.659015417098999\n",
            "step 4841: loss = 2.805755376815796\n",
            "step 4842: loss = 2.6996572017669678\n",
            "step 4843: loss = 2.727947473526001\n",
            "step 4844: loss = 3.002725601196289\n",
            "step 4845: loss = 2.8905560970306396\n",
            "step 4846: loss = 2.8128316402435303\n",
            "step 4847: loss = 2.9816205501556396\n",
            "step 4848: loss = 3.0055911540985107\n",
            "step 4849: loss = 2.982555866241455\n",
            "step 4850: loss = 2.848788261413574\n",
            "step 4851: loss = 2.8621201515197754\n",
            "step 4852: loss = 2.975003957748413\n",
            "step 4853: loss = 2.9774272441864014\n",
            "step 4854: loss = 2.708442449569702\n",
            "step 4855: loss = 2.8328349590301514\n",
            "step 4856: loss = 2.8366878032684326\n",
            "step 4857: loss = 2.8340508937835693\n",
            "step 4858: loss = 3.0211708545684814\n",
            "step 4859: loss = 2.9960827827453613\n",
            "step 4860: loss = 2.7745821475982666\n",
            "step 4861: loss = 2.88568377494812\n",
            "step 4862: loss = 3.0333662033081055\n",
            "step 4863: loss = 2.784109354019165\n",
            "step 4864: loss = 2.5697948932647705\n",
            "step 4865: loss = 3.0141549110412598\n",
            "step 4866: loss = 3.059255838394165\n",
            "step 4867: loss = 3.1662425994873047\n",
            "step 4868: loss = 2.9240126609802246\n",
            "step 4869: loss = 2.4040720462799072\n",
            "step 4870: loss = 2.862886428833008\n",
            "step 4871: loss = 2.98834228515625\n",
            "step 4872: loss = 2.9560134410858154\n",
            "step 4873: loss = 2.6279752254486084\n",
            "step 4874: loss = 3.1349117755889893\n",
            "step 4875: loss = 2.8578717708587646\n",
            "step 4876: loss = 2.8894498348236084\n",
            "step 4877: loss = 2.9113550186157227\n",
            "step 4878: loss = 3.1058478355407715\n",
            "step 4879: loss = 2.939101457595825\n",
            "step 4880: loss = 2.667342185974121\n",
            "step 4881: loss = 3.0125479698181152\n",
            "step 4882: loss = 2.9270172119140625\n",
            "step 4883: loss = 2.770073413848877\n",
            "step 4884: loss = 2.7038862705230713\n",
            "step 4885: loss = 2.7571768760681152\n",
            "step 4886: loss = 3.0554659366607666\n",
            "step 4887: loss = 2.5540201663970947\n",
            "step 4888: loss = 3.2063004970550537\n",
            "step 4889: loss = 2.7831010818481445\n",
            "step 4890: loss = 2.680706739425659\n",
            "step 4891: loss = 2.4730770587921143\n",
            "step 4892: loss = 2.982149124145508\n",
            "step 4893: loss = 2.7012686729431152\n",
            "step 4894: loss = 3.0780880451202393\n",
            "step 4895: loss = 2.8919999599456787\n",
            "step 4896: loss = 2.712472677230835\n",
            "step 4897: loss = 2.791534185409546\n",
            "step 4898: loss = 2.703507900238037\n",
            "step 4899: loss = 3.101844072341919\n",
            "step 4900: loss = 3.176427125930786\n",
            "step 4901: loss = 2.8245444297790527\n",
            "step 4902: loss = 3.2052435874938965\n",
            "step 4903: loss = 2.7313759326934814\n",
            "step 4904: loss = 2.8448495864868164\n",
            "step 4905: loss = 3.0082550048828125\n",
            "step 4906: loss = 2.872648000717163\n",
            "step 4907: loss = 2.9331159591674805\n",
            "step 4908: loss = 2.717097520828247\n",
            "step 4909: loss = 2.738286018371582\n",
            "step 4910: loss = 2.772545337677002\n",
            "step 4911: loss = 2.6589009761810303\n",
            "step 4912: loss = 2.897461414337158\n",
            "step 4913: loss = 3.001614570617676\n",
            "step 4914: loss = 2.782087564468384\n",
            "step 4915: loss = 2.8141729831695557\n",
            "step 4916: loss = 2.8015267848968506\n",
            "step 4917: loss = 2.864751100540161\n",
            "step 4918: loss = 3.0685324668884277\n",
            "step 4919: loss = 2.7225563526153564\n",
            "step 4920: loss = 2.8411660194396973\n",
            "step 4921: loss = 2.509725332260132\n",
            "step 4922: loss = 2.717357635498047\n",
            "step 4923: loss = 2.926182985305786\n",
            "step 4924: loss = 2.811671733856201\n",
            "step 4925: loss = 2.733057975769043\n",
            "step 4926: loss = 2.995723247528076\n",
            "step 4927: loss = 2.6680736541748047\n",
            "step 4928: loss = 2.916454315185547\n",
            "step 4929: loss = 2.9529850482940674\n",
            "step 4930: loss = 2.9789328575134277\n",
            "step 4931: loss = 2.739701986312866\n",
            "step 4932: loss = 2.775919198989868\n",
            "step 4933: loss = 2.8576862812042236\n",
            "step 4934: loss = 2.916149616241455\n",
            "step 4935: loss = 2.72151255607605\n",
            "step 4936: loss = 3.084746837615967\n",
            "step 4937: loss = 2.971792459487915\n",
            "step 4938: loss = 2.6658854484558105\n",
            "step 4939: loss = 2.963563919067383\n",
            "step 4940: loss = 2.9785373210906982\n",
            "step 4941: loss = 2.5822551250457764\n",
            "step 4942: loss = 3.2291648387908936\n",
            "step 4943: loss = 2.654571294784546\n",
            "step 4944: loss = 2.7090299129486084\n",
            "step 4945: loss = 3.1830458641052246\n",
            "step 4946: loss = 3.111396312713623\n",
            "step 4947: loss = 2.776467800140381\n",
            "step 4948: loss = 2.9648921489715576\n",
            "step 4949: loss = 2.9087610244750977\n",
            "step 4950: loss = 3.1655280590057373\n",
            "step 4951: loss = 2.9911534786224365\n",
            "step 4952: loss = 2.9278485774993896\n",
            "step 4953: loss = 3.02378511428833\n",
            "step 4954: loss = 2.927049160003662\n",
            "step 4955: loss = 2.8401296138763428\n",
            "step 4956: loss = 3.0789811611175537\n",
            "step 4957: loss = 2.8437387943267822\n",
            "step 4958: loss = 2.7109405994415283\n",
            "step 4959: loss = 2.7135884761810303\n",
            "step 4960: loss = 2.58913516998291\n",
            "step 4961: loss = 3.106170654296875\n",
            "step 4962: loss = 3.0913572311401367\n",
            "step 4963: loss = 2.8673694133758545\n",
            "step 4964: loss = 3.089712381362915\n",
            "step 4965: loss = 3.0979182720184326\n",
            "step 4966: loss = 2.9680018424987793\n",
            "step 4967: loss = 2.5584630966186523\n",
            "step 4968: loss = 2.8488028049468994\n",
            "step 4969: loss = 2.569985866546631\n",
            "step 4970: loss = 2.7331550121307373\n",
            "step 4971: loss = 2.978087902069092\n",
            "step 4972: loss = 2.7326388359069824\n",
            "step 4973: loss = 2.8611817359924316\n",
            "step 4974: loss = 2.8012256622314453\n",
            "step 4975: loss = 2.9044134616851807\n",
            "step 4976: loss = 2.876734495162964\n",
            "step 4977: loss = 2.9457828998565674\n",
            "step 4978: loss = 2.8656558990478516\n",
            "step 4979: loss = 2.960977792739868\n",
            "step 4980: loss = 2.650932788848877\n",
            "step 4981: loss = 2.753927230834961\n",
            "step 4982: loss = 3.24843692779541\n",
            "step 4983: loss = 3.1075193881988525\n",
            "step 4984: loss = 2.610330820083618\n",
            "step 4985: loss = 3.0510313510894775\n",
            "step 4986: loss = 2.741016387939453\n",
            "step 4987: loss = 2.8828656673431396\n",
            "step 4988: loss = 3.0669167041778564\n",
            "step 4989: loss = 2.7849178314208984\n",
            "step 4990: loss = 2.515998601913452\n",
            "step 4991: loss = 2.6719250679016113\n",
            "step 4992: loss = 2.646552562713623\n",
            "step 4993: loss = 3.1515491008758545\n",
            "step 4994: loss = 2.884204864501953\n",
            "step 4995: loss = 2.788957118988037\n",
            "step 4996: loss = 2.8916003704071045\n",
            "step 4997: loss = 2.7337543964385986\n",
            "step 4998: loss = 2.978313684463501\n",
            "step 4999: loss = 2.963527202606201\n",
            "step 5000: loss = 2.8872969150543213\n",
            "step 5001: loss = 2.8946425914764404\n",
            "step 5002: loss = 2.8098304271698\n",
            "step 5003: loss = 2.898057460784912\n",
            "step 5004: loss = 2.9808249473571777\n",
            "step 5005: loss = 2.851534843444824\n",
            "step 5006: loss = 2.89286208152771\n",
            "step 5007: loss = 2.9747061729431152\n",
            "step 5008: loss = 2.9901416301727295\n",
            "step 5009: loss = 3.070185661315918\n",
            "step 5010: loss = 2.749319076538086\n",
            "step 5011: loss = 2.9349477291107178\n",
            "step 5012: loss = 2.935307502746582\n",
            "step 5013: loss = 2.825491428375244\n",
            "step 5014: loss = 2.804403305053711\n",
            "step 5015: loss = 3.0414655208587646\n",
            "step 5016: loss = 3.2106354236602783\n",
            "step 5017: loss = 2.823516368865967\n",
            "step 5018: loss = 2.9979584217071533\n",
            "step 5019: loss = 3.066821813583374\n",
            "step 5020: loss = 2.922961711883545\n",
            "step 5021: loss = 3.176974058151245\n",
            "step 5022: loss = 2.811846971511841\n",
            "step 5023: loss = 2.654244899749756\n",
            "step 5024: loss = 2.9939167499542236\n",
            "step 5025: loss = 2.939892292022705\n",
            "step 5026: loss = 2.873281955718994\n",
            "step 5027: loss = 2.968247413635254\n",
            "step 5028: loss = 2.4822754859924316\n",
            "step 5029: loss = 2.7406764030456543\n",
            "step 5030: loss = 2.674517869949341\n",
            "step 5031: loss = 2.661237955093384\n",
            "step 5032: loss = 2.9538254737854004\n",
            "step 5033: loss = 2.9387550354003906\n",
            "step 5034: loss = 3.1502153873443604\n",
            "step 5035: loss = 2.9418752193450928\n",
            "step 5036: loss = 2.8898885250091553\n",
            "step 5037: loss = 3.0019047260284424\n",
            "step 5038: loss = 3.2142345905303955\n",
            "step 5039: loss = 2.9139420986175537\n",
            "step 5040: loss = 2.911994457244873\n",
            "step 5041: loss = 2.9522361755371094\n",
            "step 5042: loss = 3.0459535121917725\n",
            "step 5043: loss = 3.0228052139282227\n",
            "step 5044: loss = 2.955909252166748\n",
            "step 5045: loss = 2.955453872680664\n",
            "step 5046: loss = 2.695674180984497\n",
            "step 5047: loss = 2.933236837387085\n",
            "step 5048: loss = 2.8399739265441895\n",
            "step 5049: loss = 3.0745630264282227\n",
            "step 5050: loss = 2.8561859130859375\n",
            "step 5051: loss = 2.798207998275757\n",
            "step 5052: loss = 2.811445474624634\n",
            "step 5053: loss = 2.8568642139434814\n",
            "step 5054: loss = 2.6213841438293457\n",
            "step 5055: loss = 3.1045279502868652\n",
            "step 5056: loss = 3.0197415351867676\n",
            "step 5057: loss = 2.797860622406006\n",
            "step 5058: loss = 2.7678933143615723\n",
            "step 5059: loss = 2.7649085521698\n",
            "step 5060: loss = 2.7190146446228027\n",
            "step 5061: loss = 2.8608548641204834\n",
            "step 5062: loss = 2.719578981399536\n",
            "step 5063: loss = 2.6609950065612793\n",
            "step 5064: loss = 3.1033718585968018\n",
            "step 5065: loss = 3.089876174926758\n",
            "step 5066: loss = 3.0094218254089355\n",
            "step 5067: loss = 2.847430944442749\n",
            "step 5068: loss = 2.9177932739257812\n",
            "step 5069: loss = 3.2801895141601562\n",
            "step 5070: loss = 3.121002197265625\n",
            "step 5071: loss = 3.1909711360931396\n",
            "step 5072: loss = 2.46855092048645\n",
            "step 5073: loss = 3.1061224937438965\n",
            "step 5074: loss = 2.809314489364624\n",
            "step 5075: loss = 2.6903891563415527\n",
            "step 5076: loss = 2.846123695373535\n",
            "step 5077: loss = 2.7719101905822754\n",
            "step 5078: loss = 2.8623430728912354\n",
            "step 5079: loss = 2.8374104499816895\n",
            "step 5080: loss = 2.8770644664764404\n",
            "step 5081: loss = 2.958467483520508\n",
            "step 5082: loss = 2.8994016647338867\n",
            "step 5083: loss = 2.8313069343566895\n",
            "step 5084: loss = 2.8042948246002197\n",
            "step 5085: loss = 3.1646931171417236\n",
            "step 5086: loss = 2.9556312561035156\n",
            "step 5087: loss = 2.815300941467285\n",
            "step 5088: loss = 3.0340416431427\n",
            "step 5089: loss = 2.7231693267822266\n",
            "step 5090: loss = 3.003610372543335\n",
            "step 5091: loss = 3.102656841278076\n",
            "step 5092: loss = 2.8621795177459717\n",
            "step 5093: loss = 2.814667224884033\n",
            "step 5094: loss = 2.9399733543395996\n",
            "step 5095: loss = 2.972748041152954\n",
            "step 5096: loss = 2.858851671218872\n",
            "step 5097: loss = 2.7863478660583496\n",
            "step 5098: loss = 2.3993966579437256\n",
            "step 5099: loss = 2.872016191482544\n",
            "step 5100: loss = 2.8211231231689453\n",
            "step 5101: loss = 3.1789350509643555\n",
            "step 5102: loss = 3.1777184009552\n",
            "step 5103: loss = 2.89678692817688\n",
            "step 5104: loss = 3.070161819458008\n",
            "step 5105: loss = 2.848176956176758\n",
            "step 5106: loss = 3.118175745010376\n",
            "step 5107: loss = 2.981131076812744\n",
            "step 5108: loss = 2.774773120880127\n",
            "step 5109: loss = 2.927457809448242\n",
            "step 5110: loss = 3.0451107025146484\n",
            "step 5111: loss = 2.994386911392212\n",
            "step 5112: loss = 3.0217509269714355\n",
            "step 5113: loss = 3.1800594329833984\n",
            "step 5114: loss = 2.9898555278778076\n",
            "step 5115: loss = 2.9275810718536377\n",
            "step 5116: loss = 2.841839551925659\n",
            "step 5117: loss = 2.9815258979797363\n",
            "step 5118: loss = 2.924099922180176\n",
            "step 5119: loss = 3.166257381439209\n",
            "step 5120: loss = 3.121610164642334\n",
            "step 5121: loss = 2.9171438217163086\n",
            "step 5122: loss = 3.0996930599212646\n",
            "step 5123: loss = 2.7233376502990723\n",
            "step 5124: loss = 2.6756374835968018\n",
            "step 5125: loss = 2.703860282897949\n",
            "step 5126: loss = 3.1847567558288574\n",
            "step 5127: loss = 3.0928046703338623\n",
            "step 5128: loss = 2.647552490234375\n",
            "step 5129: loss = 3.080852746963501\n",
            "step 5130: loss = 3.0793673992156982\n",
            "step 5131: loss = 2.974433183670044\n",
            "step 5132: loss = 2.944638967514038\n",
            "step 5133: loss = 2.994795083999634\n",
            "step 5134: loss = 2.8042314052581787\n",
            "step 5135: loss = 2.861537218093872\n",
            "step 5136: loss = 3.044792890548706\n",
            "step 5137: loss = 3.2079460620880127\n",
            "step 5138: loss = 2.8305857181549072\n",
            "step 5139: loss = 2.8923861980438232\n",
            "step 5140: loss = 2.859102249145508\n",
            "step 5141: loss = 3.0109951496124268\n",
            "step 5142: loss = 3.1417922973632812\n",
            "step 5143: loss = 2.9462902545928955\n",
            "step 5144: loss = 2.7513461112976074\n",
            "step 5145: loss = 2.9838154315948486\n",
            "step 5146: loss = 3.4149529933929443\n",
            "step 5147: loss = 3.1650359630584717\n",
            "step 5148: loss = 2.7805309295654297\n",
            "step 5149: loss = 2.8900108337402344\n",
            "step 5150: loss = 2.710404872894287\n",
            "step 5151: loss = 2.7924280166625977\n",
            "step 5152: loss = 2.854485511779785\n",
            "step 5153: loss = 2.6466665267944336\n",
            "step 5154: loss = 2.8670358657836914\n",
            "step 5155: loss = 3.0014915466308594\n",
            "step 5156: loss = 3.1026504039764404\n",
            "step 5157: loss = 3.011439085006714\n",
            "step 5158: loss = 2.8772220611572266\n",
            "step 5159: loss = 3.112133264541626\n",
            "step 5160: loss = 2.747610569000244\n",
            "step 5161: loss = 3.2644290924072266\n",
            "step 5162: loss = 2.8903355598449707\n",
            "step 5163: loss = 3.1051578521728516\n",
            "step 5164: loss = 2.6729917526245117\n",
            "step 5165: loss = 3.0713131427764893\n",
            "step 5166: loss = 2.8481013774871826\n",
            "step 5167: loss = 2.9282140731811523\n",
            "step 5168: loss = 2.870347261428833\n",
            "step 5169: loss = 3.1692957878112793\n",
            "step 5170: loss = 2.696094274520874\n",
            "step 5171: loss = 2.762714147567749\n",
            "step 5172: loss = 2.957932710647583\n",
            "step 5173: loss = 2.890148878097534\n",
            "step 5174: loss = 2.6510074138641357\n",
            "step 5175: loss = 3.3585197925567627\n",
            "step 5176: loss = 3.1539969444274902\n",
            "step 5177: loss = 3.0442960262298584\n",
            "step 5178: loss = 3.030021905899048\n",
            "step 5179: loss = 3.0253827571868896\n",
            "step 5180: loss = 3.026293992996216\n",
            "step 5181: loss = 2.663444757461548\n",
            "step 5182: loss = 2.922847270965576\n",
            "step 5183: loss = 2.8050339221954346\n",
            "step 5184: loss = 2.942211151123047\n",
            "step 5185: loss = 3.014756679534912\n",
            "step 5186: loss = 2.9428043365478516\n",
            "step 5187: loss = 3.0860509872436523\n",
            "step 5188: loss = 3.048638105392456\n",
            "step 5189: loss = 3.032073497772217\n",
            "step 5190: loss = 3.042414903640747\n",
            "step 5191: loss = 3.086590051651001\n",
            "step 5192: loss = 2.983370065689087\n",
            "step 5193: loss = 3.1356544494628906\n",
            "step 5194: loss = 2.9491660594940186\n",
            "step 5195: loss = 2.7102742195129395\n",
            "step 5196: loss = 3.083730697631836\n",
            "step 5197: loss = 3.1309845447540283\n",
            "step 5198: loss = 3.0767440795898438\n",
            "step 5199: loss = 2.683506727218628\n",
            "step 5200: loss = 3.4708495140075684\n",
            "step 5201: loss = 3.1746065616607666\n",
            "step 5202: loss = 3.073638916015625\n",
            "step 5203: loss = 2.9276816844940186\n",
            "step 5204: loss = 2.983198404312134\n",
            "step 5205: loss = 2.9879682064056396\n",
            "step 5206: loss = 2.7505712509155273\n",
            "step 5207: loss = 2.9534008502960205\n",
            "step 5208: loss = 2.9310576915740967\n",
            "step 5209: loss = 3.0171539783477783\n",
            "step 5210: loss = 2.9767744541168213\n",
            "step 5211: loss = 2.7979164123535156\n",
            "step 5212: loss = 3.0888941287994385\n",
            "step 5213: loss = 3.0640881061553955\n",
            "step 5214: loss = 2.885629653930664\n",
            "step 5215: loss = 2.8440186977386475\n",
            "step 5216: loss = 3.1625733375549316\n",
            "step 5217: loss = 3.0391335487365723\n",
            "step 5218: loss = 3.05041241645813\n",
            "step 5219: loss = 2.92980694770813\n",
            "step 5220: loss = 2.893035411834717\n",
            "step 5221: loss = 2.9573471546173096\n",
            "step 5222: loss = 2.942422389984131\n",
            "step 5223: loss = 3.037343978881836\n",
            "step 5224: loss = 2.736839771270752\n",
            "step 5225: loss = 3.073953866958618\n",
            "step 5226: loss = 2.8568027019500732\n",
            "step 5227: loss = 3.0033671855926514\n",
            "step 5228: loss = 2.8200531005859375\n",
            "step 5229: loss = 3.0214319229125977\n",
            "step 5230: loss = 3.0599801540374756\n",
            "step 5231: loss = 2.921008825302124\n",
            "step 5232: loss = 2.543670177459717\n",
            "step 5233: loss = 3.033318042755127\n",
            "step 5234: loss = 2.6146962642669678\n",
            "step 5235: loss = 3.0579323768615723\n",
            "step 5236: loss = 2.8762965202331543\n",
            "step 5237: loss = 3.0174779891967773\n",
            "step 5238: loss = 2.852862596511841\n",
            "step 5239: loss = 2.9762113094329834\n",
            "step 5240: loss = 3.0431222915649414\n",
            "step 5241: loss = 2.784749746322632\n",
            "step 5242: loss = 2.532773733139038\n",
            "step 5243: loss = 3.1841440200805664\n",
            "step 5244: loss = 2.865929365158081\n",
            "step 5245: loss = 3.0223324298858643\n",
            "step 5246: loss = 2.901772975921631\n",
            "step 5247: loss = 2.893061876296997\n",
            "step 5248: loss = 2.767056941986084\n",
            "step 5249: loss = 2.954918146133423\n",
            "step 5250: loss = 2.786773443222046\n",
            "step 5251: loss = 3.0718913078308105\n",
            "step 5252: loss = 3.0282797813415527\n",
            "step 5253: loss = 3.371274709701538\n",
            "step 5254: loss = 3.2408437728881836\n",
            "step 5255: loss = 2.807378053665161\n",
            "step 5256: loss = 3.01141619682312\n",
            "step 5257: loss = 2.9097464084625244\n",
            "step 5258: loss = 3.0476229190826416\n",
            "step 5259: loss = 2.7844908237457275\n",
            "step 5260: loss = 3.1420137882232666\n",
            "step 5261: loss = 2.828612804412842\n",
            "step 5262: loss = 3.125680446624756\n",
            "step 5263: loss = 2.6233668327331543\n",
            "step 5264: loss = 2.6977994441986084\n",
            "step 5265: loss = 2.8553626537323\n",
            "step 5266: loss = 3.037388801574707\n",
            "step 5267: loss = 2.9004769325256348\n",
            "step 5268: loss = 3.118203639984131\n",
            "step 5269: loss = 2.8700151443481445\n",
            "step 5270: loss = 3.1035661697387695\n",
            "step 5271: loss = 2.861619472503662\n",
            "step 5272: loss = 2.827761173248291\n",
            "step 5273: loss = 2.854708671569824\n",
            "step 5274: loss = 2.96488618850708\n",
            "step 5275: loss = 2.770822048187256\n",
            "step 5276: loss = 3.1834630966186523\n",
            "step 5277: loss = 2.9165549278259277\n",
            "step 5278: loss = 2.9378373622894287\n",
            "step 5279: loss = 3.220719575881958\n",
            "step 5280: loss = 3.036497116088867\n",
            "step 5281: loss = 3.082721710205078\n",
            "step 5282: loss = 2.9424328804016113\n",
            "step 5283: loss = 3.0780489444732666\n",
            "step 5284: loss = 2.739518642425537\n",
            "step 5285: loss = 2.851757049560547\n",
            "step 5286: loss = 3.1563632488250732\n",
            "step 5287: loss = 2.7376925945281982\n",
            "step 5288: loss = 2.885678291320801\n",
            "step 5289: loss = 3.1252498626708984\n",
            "step 5290: loss = 2.6891236305236816\n",
            "step 5291: loss = 2.9017181396484375\n",
            "step 5292: loss = 2.8905160427093506\n",
            "step 5293: loss = 2.8043529987335205\n",
            "step 5294: loss = 3.3650152683258057\n",
            "step 5295: loss = 3.1540305614471436\n",
            "step 5296: loss = 3.1604933738708496\n",
            "step 5297: loss = 2.80815052986145\n",
            "step 5298: loss = 3.130967378616333\n",
            "step 5299: loss = 3.0901358127593994\n",
            "step 5300: loss = 2.8623387813568115\n",
            "step 5301: loss = 2.8753368854522705\n",
            "step 5302: loss = 2.8359031677246094\n",
            "step 5303: loss = 3.020594835281372\n",
            "step 5304: loss = 3.055757761001587\n",
            "step 5305: loss = 3.1032397747039795\n",
            "step 5306: loss = 3.0177760124206543\n",
            "step 5307: loss = 2.947364568710327\n",
            "step 5308: loss = 2.787877082824707\n",
            "step 5309: loss = 3.3008761405944824\n",
            "step 5310: loss = 2.7868642807006836\n",
            "step 5311: loss = 2.8590166568756104\n",
            "step 5312: loss = 2.7133700847625732\n",
            "step 5313: loss = 3.2698423862457275\n",
            "step 5314: loss = 3.329026699066162\n",
            "step 5315: loss = 2.925440788269043\n",
            "step 5316: loss = 2.6911935806274414\n",
            "step 5317: loss = 2.812323570251465\n",
            "step 5318: loss = 3.1040265560150146\n",
            "step 5319: loss = 2.955414295196533\n",
            "step 5320: loss = 2.736241579055786\n",
            "step 5321: loss = 3.094703197479248\n",
            "step 5322: loss = 3.201418399810791\n",
            "step 5323: loss = 2.9942688941955566\n",
            "step 5324: loss = 3.023960828781128\n",
            "step 5325: loss = 2.933908462524414\n",
            "step 5326: loss = 3.098417282104492\n",
            "step 5327: loss = 2.976001739501953\n",
            "step 5328: loss = 3.1747982501983643\n",
            "step 5329: loss = 2.8608014583587646\n",
            "step 5330: loss = 3.2265431880950928\n",
            "step 5331: loss = 3.010668992996216\n",
            "step 5332: loss = 3.0223283767700195\n",
            "step 5333: loss = 2.782667875289917\n",
            "step 5334: loss = 2.8442542552948\n",
            "step 5335: loss = 2.8379013538360596\n",
            "step 5336: loss = 2.8274879455566406\n",
            "step 5337: loss = 3.0945353507995605\n",
            "step 5338: loss = 2.7165937423706055\n",
            "step 5339: loss = 2.883312225341797\n",
            "step 5340: loss = 3.244786262512207\n",
            "step 5341: loss = 3.0501697063446045\n",
            "step 5342: loss = 2.8851187229156494\n",
            "step 5343: loss = 2.7950732707977295\n",
            "step 5344: loss = 3.2042834758758545\n",
            "step 5345: loss = 3.1591029167175293\n",
            "step 5346: loss = 2.8905715942382812\n",
            "step 5347: loss = 2.820237159729004\n",
            "step 5348: loss = 2.9743592739105225\n",
            "step 5349: loss = 2.895068407058716\n",
            "step 5350: loss = 2.908928871154785\n",
            "step 5351: loss = 2.9336533546447754\n",
            "step 5352: loss = 3.0143039226531982\n",
            "step 5353: loss = 3.21112322807312\n",
            "step 5354: loss = 3.1812779903411865\n",
            "step 5355: loss = 3.0840728282928467\n",
            "step 5356: loss = 2.8322136402130127\n",
            "step 5357: loss = 2.791154384613037\n",
            "step 5358: loss = 3.1037962436676025\n",
            "step 5359: loss = 2.6109933853149414\n",
            "step 5360: loss = 2.7054312229156494\n",
            "step 5361: loss = 2.9093363285064697\n",
            "step 5362: loss = 2.7903077602386475\n",
            "step 5363: loss = 2.828956127166748\n",
            "step 5364: loss = 3.148120880126953\n",
            "step 5365: loss = 2.8275296688079834\n",
            "step 5366: loss = 2.9804069995880127\n",
            "step 5367: loss = 2.7899703979492188\n",
            "step 5368: loss = 2.761155843734741\n",
            "step 5369: loss = 2.969702959060669\n",
            "step 5370: loss = 2.724673271179199\n",
            "step 5371: loss = 2.769752025604248\n",
            "step 5372: loss = 2.9774837493896484\n",
            "step 5373: loss = 2.9979982376098633\n",
            "step 5374: loss = 2.626037120819092\n",
            "step 5375: loss = 2.961709976196289\n",
            "step 5376: loss = 2.969146966934204\n",
            "step 5377: loss = 2.7488350868225098\n",
            "step 5378: loss = 3.094560146331787\n",
            "step 5379: loss = 2.9706287384033203\n",
            "step 5380: loss = 3.106616735458374\n",
            "step 5381: loss = 3.171609401702881\n",
            "step 5382: loss = 2.981282949447632\n",
            "step 5383: loss = 3.4155423641204834\n",
            "step 5384: loss = 2.8518941402435303\n",
            "step 5385: loss = 2.905428886413574\n",
            "step 5386: loss = 3.02517032623291\n",
            "step 5387: loss = 2.9117791652679443\n",
            "step 5388: loss = 2.7756893634796143\n",
            "step 5389: loss = 3.1343636512756348\n",
            "step 5390: loss = 3.128448724746704\n",
            "step 5391: loss = 2.9075050354003906\n",
            "step 5392: loss = 3.0798401832580566\n",
            "step 5393: loss = 2.840968608856201\n",
            "step 5394: loss = 2.9432525634765625\n",
            "step 5395: loss = 2.673671245574951\n",
            "step 5396: loss = 3.153141736984253\n",
            "step 5397: loss = 2.870279312133789\n",
            "step 5398: loss = 2.734875440597534\n",
            "step 5399: loss = 3.0006117820739746\n",
            "step 5400: loss = 2.9589688777923584\n",
            "step 5401: loss = 3.4404518604278564\n",
            "step 5402: loss = 3.0266318321228027\n",
            "step 5403: loss = 2.9818291664123535\n",
            "step 5404: loss = 3.0153250694274902\n",
            "step 5405: loss = 2.8651816844940186\n",
            "step 5406: loss = 3.0239882469177246\n",
            "step 5407: loss = 2.945558786392212\n",
            "step 5408: loss = 2.923062324523926\n",
            "step 5409: loss = 3.0776538848876953\n",
            "step 5410: loss = 2.5175833702087402\n",
            "step 5411: loss = 2.9977521896362305\n",
            "step 5412: loss = 2.8673465251922607\n",
            "step 5413: loss = 3.107360363006592\n",
            "step 5414: loss = 2.9537782669067383\n",
            "step 5415: loss = 2.9736711978912354\n",
            "step 5416: loss = 2.95709490776062\n",
            "step 5417: loss = 2.708836555480957\n",
            "step 5418: loss = 2.9032318592071533\n",
            "step 5419: loss = 3.0492289066314697\n",
            "step 5420: loss = 2.841754913330078\n",
            "step 5421: loss = 2.742433786392212\n",
            "step 5422: loss = 2.740511894226074\n",
            "step 5423: loss = 2.9458274841308594\n",
            "step 5424: loss = 2.887277603149414\n",
            "step 5425: loss = 2.7466626167297363\n",
            "step 5426: loss = 3.0889689922332764\n",
            "step 5427: loss = 2.9671757221221924\n",
            "step 5428: loss = 2.996835470199585\n",
            "step 5429: loss = 2.9890427589416504\n",
            "step 5430: loss = 2.8945369720458984\n",
            "step 5431: loss = 3.30607533454895\n",
            "step 5432: loss = 3.0176119804382324\n",
            "step 5433: loss = 2.968294620513916\n",
            "step 5434: loss = 2.9843761920928955\n",
            "step 5435: loss = 2.945516586303711\n",
            "step 5436: loss = 2.9323856830596924\n",
            "step 5437: loss = 3.2333521842956543\n",
            "step 5438: loss = 2.9097633361816406\n",
            "step 5439: loss = 2.8190157413482666\n",
            "step 5440: loss = 2.5369749069213867\n",
            "step 5441: loss = 3.048248529434204\n",
            "step 5442: loss = 3.176224946975708\n",
            "step 5443: loss = 2.9154598712921143\n",
            "step 5444: loss = 3.1659741401672363\n",
            "step 5445: loss = 2.9919373989105225\n",
            "step 5446: loss = 2.8047096729278564\n",
            "step 5447: loss = 2.9693825244903564\n",
            "step 5448: loss = 2.806509017944336\n",
            "step 5449: loss = 3.065748453140259\n",
            "step 5450: loss = 2.819988489151001\n",
            "step 5451: loss = 2.860548734664917\n",
            "step 5452: loss = 2.9773740768432617\n",
            "step 5453: loss = 3.116927146911621\n",
            "step 5454: loss = 2.619473457336426\n",
            "step 5455: loss = 2.8877596855163574\n",
            "step 5456: loss = 2.850593328475952\n",
            "step 5457: loss = 3.1543526649475098\n",
            "step 5458: loss = 2.8141446113586426\n",
            "step 5459: loss = 2.918513774871826\n",
            "step 5460: loss = 3.2843379974365234\n",
            "step 5461: loss = 3.167030096054077\n",
            "step 5462: loss = 2.969430446624756\n",
            "step 5463: loss = 2.740917921066284\n",
            "step 5464: loss = 2.9722342491149902\n",
            "step 5465: loss = 2.9045252799987793\n",
            "step 5466: loss = 2.8308308124542236\n",
            "step 5467: loss = 3.0469603538513184\n",
            "step 5468: loss = 3.021623134613037\n",
            "step 5469: loss = 2.7552638053894043\n",
            "step 5470: loss = 3.025907516479492\n",
            "step 5471: loss = 3.081465005874634\n",
            "step 5472: loss = 2.97943115234375\n",
            "step 5473: loss = 3.005049228668213\n",
            "step 5474: loss = 2.8401124477386475\n",
            "step 5475: loss = 2.933547258377075\n",
            "step 5476: loss = 3.0650527477264404\n",
            "step 5477: loss = 2.734272003173828\n",
            "step 5478: loss = 2.920067071914673\n",
            "step 5479: loss = 2.9087257385253906\n",
            "step 5480: loss = 2.825576066970825\n",
            "step 5481: loss = 3.0404891967773438\n",
            "step 5482: loss = 2.981337070465088\n",
            "step 5483: loss = 3.0599706172943115\n",
            "step 5484: loss = 2.5967023372650146\n",
            "step 5485: loss = 3.004923105239868\n",
            "step 5486: loss = 3.052597999572754\n",
            "step 5487: loss = 3.0853183269500732\n",
            "step 5488: loss = 2.8344218730926514\n",
            "step 5489: loss = 2.8691234588623047\n",
            "step 5490: loss = 2.936046600341797\n",
            "step 5491: loss = 3.038250207901001\n",
            "step 5492: loss = 2.952951669692993\n",
            "step 5493: loss = 3.077995777130127\n",
            "step 5494: loss = 3.2003085613250732\n",
            "step 5495: loss = 3.012695789337158\n",
            "step 5496: loss = 2.7241249084472656\n",
            "step 5497: loss = 3.0127944946289062\n",
            "step 5498: loss = 3.041318416595459\n",
            "step 5499: loss = 2.7911453247070312\n",
            "step 5500: loss = 3.0120999813079834\n",
            "step 5501: loss = 3.2797791957855225\n",
            "step 5502: loss = 2.841775417327881\n",
            "step 5503: loss = 2.7295167446136475\n",
            "step 5504: loss = 2.744140386581421\n",
            "step 5505: loss = 2.953193426132202\n",
            "step 5506: loss = 2.8319337368011475\n",
            "step 5507: loss = 3.0795650482177734\n",
            "step 5508: loss = 3.2138166427612305\n",
            "step 5509: loss = 2.981010675430298\n",
            "step 5510: loss = 2.9098470211029053\n",
            "step 5511: loss = 3.030001401901245\n",
            "step 5512: loss = 2.9192240238189697\n",
            "step 5513: loss = 2.880401849746704\n",
            "step 5514: loss = 3.0627715587615967\n",
            "step 5515: loss = 2.8254239559173584\n",
            "step 5516: loss = 3.156724452972412\n",
            "step 5517: loss = 2.9824652671813965\n",
            "step 5518: loss = 3.0007095336914062\n",
            "step 5519: loss = 2.979588270187378\n",
            "step 5520: loss = 2.87613844871521\n",
            "step 5521: loss = 2.832468032836914\n",
            "step 5522: loss = 2.8456451892852783\n",
            "step 5523: loss = 3.114100217819214\n",
            "step 5524: loss = 3.1310818195343018\n",
            "step 5525: loss = 2.800159215927124\n",
            "step 5526: loss = 3.0235495567321777\n",
            "step 5527: loss = 2.943270206451416\n",
            "step 5528: loss = 2.7283692359924316\n",
            "step 5529: loss = 2.6725587844848633\n",
            "step 5530: loss = 3.149184226989746\n",
            "step 5531: loss = 3.118459701538086\n",
            "step 5532: loss = 3.257939338684082\n",
            "step 5533: loss = 2.9238643646240234\n",
            "step 5534: loss = 2.6942760944366455\n",
            "step 5535: loss = 3.1134707927703857\n",
            "step 5536: loss = 2.9014177322387695\n",
            "step 5537: loss = 2.876718521118164\n",
            "step 5538: loss = 3.1735682487487793\n",
            "step 5539: loss = 2.956371307373047\n",
            "step 5540: loss = 2.7330875396728516\n",
            "step 5541: loss = 3.284087896347046\n",
            "step 5542: loss = 3.1003475189208984\n",
            "step 5543: loss = 2.729781150817871\n",
            "step 5544: loss = 3.0209898948669434\n",
            "step 5545: loss = 2.8984127044677734\n",
            "step 5546: loss = 2.9023303985595703\n",
            "step 5547: loss = 2.7446086406707764\n",
            "step 5548: loss = 2.9421744346618652\n",
            "step 5549: loss = 2.9290974140167236\n",
            "step 5550: loss = 3.2896735668182373\n",
            "step 5551: loss = 2.8338801860809326\n",
            "step 5552: loss = 3.0577611923217773\n",
            "step 5553: loss = 2.7245070934295654\n",
            "step 5554: loss = 3.299565076828003\n",
            "step 5555: loss = 2.80957293510437\n",
            "step 5556: loss = 2.6168460845947266\n",
            "step 5557: loss = 2.831467628479004\n",
            "step 5558: loss = 3.1153697967529297\n",
            "step 5559: loss = 2.796010971069336\n",
            "step 5560: loss = 2.9106178283691406\n",
            "step 5561: loss = 3.1356372833251953\n",
            "step 5562: loss = 2.9260895252227783\n",
            "step 5563: loss = 3.038280963897705\n",
            "step 5564: loss = 2.764214515686035\n",
            "step 5565: loss = 3.072416067123413\n",
            "step 5566: loss = 3.093031406402588\n",
            "step 5567: loss = 3.1113548278808594\n",
            "step 5568: loss = 3.0718674659729004\n",
            "step 5569: loss = 3.0419905185699463\n",
            "step 5570: loss = 3.0804226398468018\n",
            "step 5571: loss = 3.156665325164795\n",
            "step 5572: loss = 3.2011163234710693\n",
            "step 5573: loss = 2.8839128017425537\n",
            "step 5574: loss = 3.162506103515625\n",
            "step 5575: loss = 2.6894636154174805\n",
            "step 5576: loss = 3.115753412246704\n",
            "step 5577: loss = 2.997684955596924\n",
            "step 5578: loss = 3.017354726791382\n",
            "step 5579: loss = 2.9961893558502197\n",
            "step 5580: loss = 3.142022132873535\n",
            "step 5581: loss = 2.9407007694244385\n",
            "step 5582: loss = 3.0611026287078857\n",
            "step 5583: loss = 2.999331474304199\n",
            "step 5584: loss = 3.294954776763916\n",
            "step 5585: loss = 2.792501211166382\n",
            "step 5586: loss = 3.0411107540130615\n",
            "step 5587: loss = 2.8638288974761963\n",
            "step 5588: loss = 3.1358232498168945\n",
            "step 5589: loss = 3.146345853805542\n",
            "step 5590: loss = 2.9065423011779785\n",
            "step 5591: loss = 2.916543483734131\n",
            "step 5592: loss = 3.0540966987609863\n",
            "step 5593: loss = 2.7753098011016846\n",
            "step 5594: loss = 2.94185733795166\n",
            "step 5595: loss = 2.9480972290039062\n",
            "step 5596: loss = 3.1194286346435547\n",
            "step 5597: loss = 2.766444683074951\n",
            "step 5598: loss = 3.1854889392852783\n",
            "step 5599: loss = 3.230703592300415\n",
            "step 5600: loss = 2.8917341232299805\n",
            "step 5601: loss = 2.9236984252929688\n",
            "step 5602: loss = 3.05570125579834\n",
            "step 5603: loss = 2.873596668243408\n",
            "step 5604: loss = 3.031944513320923\n",
            "step 5605: loss = 3.0304043292999268\n",
            "step 5606: loss = 2.990997076034546\n",
            "step 5607: loss = 3.2556028366088867\n",
            "step 5608: loss = 2.782010793685913\n",
            "step 5609: loss = 3.077993392944336\n",
            "step 5610: loss = 3.136021614074707\n",
            "step 5611: loss = 3.1159064769744873\n",
            "step 5612: loss = 2.8089969158172607\n",
            "step 5613: loss = 3.049206256866455\n",
            "step 5614: loss = 2.708310842514038\n",
            "step 5615: loss = 3.065866231918335\n",
            "step 5616: loss = 2.938494920730591\n",
            "step 5617: loss = 2.840941905975342\n",
            "step 5618: loss = 3.217055082321167\n",
            "step 5619: loss = 2.9588873386383057\n",
            "step 5620: loss = 2.87725830078125\n",
            "step 5621: loss = 2.920808792114258\n",
            "step 5622: loss = 3.1624350547790527\n",
            "step 5623: loss = 2.8294196128845215\n",
            "step 5624: loss = 3.21917986869812\n",
            "step 5625: loss = 2.9783482551574707\n",
            "step 5626: loss = 2.793738603591919\n",
            "step 5627: loss = 3.027254104614258\n",
            "step 5628: loss = 2.8629019260406494\n",
            "step 5629: loss = 3.1427385807037354\n",
            "step 5630: loss = 2.9322926998138428\n",
            "step 5631: loss = 2.8730578422546387\n",
            "step 5632: loss = 2.970017194747925\n",
            "step 5633: loss = 3.1365392208099365\n",
            "step 5634: loss = 2.7953662872314453\n",
            "step 5635: loss = 3.2166781425476074\n",
            "step 5636: loss = 2.998755693435669\n",
            "step 5637: loss = 2.985304594039917\n",
            "step 5638: loss = 3.0827531814575195\n",
            "step 5639: loss = 3.5408365726470947\n",
            "step 5640: loss = 2.978621006011963\n",
            "step 5641: loss = 2.9896247386932373\n",
            "step 5642: loss = 3.022653341293335\n",
            "step 5643: loss = 3.166900873184204\n",
            "step 5644: loss = 3.050597667694092\n",
            "step 5645: loss = 2.898932456970215\n",
            "step 5646: loss = 3.0251519680023193\n",
            "step 5647: loss = 3.1743040084838867\n",
            "step 5648: loss = 2.9617092609405518\n",
            "step 5649: loss = 3.189077615737915\n",
            "step 5650: loss = 3.1482369899749756\n",
            "step 5651: loss = 3.0148422718048096\n",
            "step 5652: loss = 2.9542760848999023\n",
            "step 5653: loss = 2.8319735527038574\n",
            "step 5654: loss = 3.0855495929718018\n",
            "step 5655: loss = 2.992149591445923\n",
            "step 5656: loss = 2.9492316246032715\n",
            "step 5657: loss = 2.7991998195648193\n",
            "step 5658: loss = 3.177696466445923\n",
            "step 5659: loss = 2.9601247310638428\n",
            "step 5660: loss = 2.996161699295044\n",
            "step 5661: loss = 2.7824981212615967\n",
            "step 5662: loss = 2.8356146812438965\n",
            "step 5663: loss = 2.9373891353607178\n",
            "step 5664: loss = 3.171501398086548\n",
            "step 5665: loss = 2.6766245365142822\n",
            "step 5666: loss = 2.728888750076294\n",
            "step 5667: loss = 2.9625236988067627\n",
            "step 5668: loss = 2.769134521484375\n",
            "step 5669: loss = 2.8250832557678223\n",
            "step 5670: loss = 3.1526145935058594\n",
            "step 5671: loss = 2.9994940757751465\n",
            "step 5672: loss = 2.7688238620758057\n",
            "step 5673: loss = 2.9835050106048584\n",
            "step 5674: loss = 3.2111318111419678\n",
            "step 5675: loss = 3.059967041015625\n",
            "step 5676: loss = 3.052950382232666\n",
            "step 5677: loss = 3.1959848403930664\n",
            "step 5678: loss = 2.7550292015075684\n",
            "step 5679: loss = 3.2206263542175293\n",
            "step 5680: loss = 2.8453006744384766\n",
            "step 5681: loss = 2.8868916034698486\n",
            "step 5682: loss = 2.9986109733581543\n",
            "step 5683: loss = 2.917337417602539\n",
            "step 5684: loss = 2.6836998462677\n",
            "step 5685: loss = 2.914766788482666\n",
            "step 5686: loss = 2.727189064025879\n",
            "step 5687: loss = 3.131969451904297\n",
            "step 5688: loss = 2.936429738998413\n",
            "step 5689: loss = 2.7924633026123047\n",
            "step 5690: loss = 2.717937469482422\n",
            "step 5691: loss = 2.935687303543091\n",
            "step 5692: loss = 3.0766093730926514\n",
            "step 5693: loss = 3.041675567626953\n",
            "step 5694: loss = 2.9242560863494873\n",
            "step 5695: loss = 3.043645143508911\n",
            "step 5696: loss = 3.0700979232788086\n",
            "step 5697: loss = 2.706498384475708\n",
            "step 5698: loss = 2.829606294631958\n",
            "step 5699: loss = 2.6250791549682617\n",
            "step 5700: loss = 2.983675479888916\n",
            "step 5701: loss = 2.836594820022583\n",
            "step 5702: loss = 2.992797374725342\n",
            "step 5703: loss = 2.936779499053955\n",
            "step 5704: loss = 3.085463285446167\n",
            "step 5705: loss = 2.9025392532348633\n",
            "step 5706: loss = 3.0861997604370117\n",
            "step 5707: loss = 2.973240375518799\n",
            "step 5708: loss = 2.941555976867676\n",
            "step 5709: loss = 2.9816958904266357\n",
            "step 5710: loss = 2.9618091583251953\n",
            "step 5711: loss = 2.692035675048828\n",
            "step 5712: loss = 3.3128485679626465\n",
            "step 5713: loss = 2.8885836601257324\n",
            "step 5714: loss = 2.899040460586548\n",
            "step 5715: loss = 2.911447286605835\n",
            "step 5716: loss = 3.103362798690796\n",
            "step 5717: loss = 3.086473226547241\n",
            "step 5718: loss = 2.8515608310699463\n",
            "step 5719: loss = 3.018109083175659\n",
            "step 5720: loss = 2.9584336280822754\n",
            "step 5721: loss = 2.841646671295166\n",
            "step 5722: loss = 2.993298292160034\n",
            "step 5723: loss = 3.184154987335205\n",
            "step 5724: loss = 3.2425925731658936\n",
            "step 5725: loss = 2.5504307746887207\n",
            "step 5726: loss = 3.1895012855529785\n",
            "step 5727: loss = 2.7420432567596436\n",
            "step 5728: loss = 3.0772273540496826\n",
            "step 5729: loss = 2.7201499938964844\n",
            "step 5730: loss = 2.8085148334503174\n",
            "step 5731: loss = 3.2526895999908447\n",
            "step 5732: loss = 3.0880308151245117\n",
            "step 5733: loss = 3.039188861846924\n",
            "step 5734: loss = 3.0637106895446777\n",
            "step 5735: loss = 3.187307596206665\n",
            "step 5736: loss = 2.6531519889831543\n",
            "step 5737: loss = 2.714756727218628\n",
            "step 5738: loss = 3.039198160171509\n",
            "step 5739: loss = 2.851799249649048\n",
            "step 5740: loss = 2.967559576034546\n",
            "step 5741: loss = 2.851917028427124\n",
            "step 5742: loss = 2.8196961879730225\n",
            "step 5743: loss = 3.0803370475769043\n",
            "step 5744: loss = 2.9469387531280518\n",
            "step 5745: loss = 2.9730064868927\n",
            "step 5746: loss = 2.8490982055664062\n",
            "step 5747: loss = 2.970064640045166\n",
            "step 5748: loss = 2.9457714557647705\n",
            "step 5749: loss = 2.7435266971588135\n",
            "step 5750: loss = 2.8175549507141113\n",
            "step 5751: loss = 3.0090184211730957\n",
            "step 5752: loss = 2.813260555267334\n",
            "step 5753: loss = 3.196859836578369\n",
            "step 5754: loss = 2.9730851650238037\n",
            "step 5755: loss = 3.253018379211426\n",
            "step 5756: loss = 2.860656499862671\n",
            "step 5757: loss = 2.82724928855896\n",
            "step 5758: loss = 2.541505813598633\n",
            "step 5759: loss = 2.9952635765075684\n",
            "step 5760: loss = 3.004619836807251\n",
            "step 5761: loss = 2.818258762359619\n",
            "step 5762: loss = 3.088559865951538\n",
            "step 5763: loss = 2.978929042816162\n",
            "step 5764: loss = 2.9049081802368164\n",
            "step 5765: loss = 2.860205888748169\n",
            "step 5766: loss = 2.9298083782196045\n",
            "step 5767: loss = 2.9858250617980957\n",
            "step 5768: loss = 3.159294605255127\n",
            "step 5769: loss = 2.9025442600250244\n",
            "step 5770: loss = 2.919250726699829\n",
            "step 5771: loss = 2.889770269393921\n",
            "step 5772: loss = 2.8736042976379395\n",
            "step 5773: loss = 2.9859259128570557\n",
            "step 5774: loss = 2.8084473609924316\n",
            "step 5775: loss = 3.1701080799102783\n",
            "step 5776: loss = 2.858621597290039\n",
            "step 5777: loss = 3.1994380950927734\n",
            "step 5778: loss = 2.730386257171631\n",
            "step 5779: loss = 2.9449381828308105\n",
            "step 5780: loss = 2.9169561862945557\n",
            "step 5781: loss = 3.249537944793701\n",
            "step 5782: loss = 2.8567066192626953\n",
            "step 5783: loss = 3.121213912963867\n",
            "step 5784: loss = 2.801774501800537\n",
            "step 5785: loss = 3.08320689201355\n",
            "step 5786: loss = 3.080824851989746\n",
            "step 5787: loss = 3.2974390983581543\n",
            "step 5788: loss = 3.2342135906219482\n",
            "step 5789: loss = 2.788409471511841\n",
            "step 5790: loss = 3.1255075931549072\n",
            "step 5791: loss = 2.7932231426239014\n",
            "step 5792: loss = 3.0157721042633057\n",
            "step 5793: loss = 3.096221446990967\n",
            "step 5794: loss = 3.0685436725616455\n",
            "step 5795: loss = 2.9866979122161865\n",
            "step 5796: loss = 2.9330198764801025\n",
            "step 5797: loss = 3.3568601608276367\n",
            "step 5798: loss = 2.8972105979919434\n",
            "step 5799: loss = 3.3219144344329834\n",
            "step 5800: loss = 3.01672625541687\n",
            "step 5801: loss = 2.9549100399017334\n",
            "step 5802: loss = 2.8314902782440186\n",
            "step 5803: loss = 2.8472416400909424\n",
            "step 5804: loss = 2.794762372970581\n",
            "step 5805: loss = 2.907360315322876\n",
            "step 5806: loss = 2.5086750984191895\n",
            "step 5807: loss = 3.0357508659362793\n",
            "step 5808: loss = 3.1788110733032227\n",
            "step 5809: loss = 2.5979342460632324\n",
            "step 5810: loss = 3.2667787075042725\n",
            "step 5811: loss = 2.8788416385650635\n",
            "step 5812: loss = 2.715581178665161\n",
            "step 5813: loss = 3.2961506843566895\n",
            "step 5814: loss = 3.083118200302124\n",
            "step 5815: loss = 3.0605998039245605\n",
            "step 5816: loss = 2.9602229595184326\n",
            "step 5817: loss = 2.8735077381134033\n",
            "step 5818: loss = 2.7763710021972656\n",
            "step 5819: loss = 2.8451194763183594\n",
            "step 5820: loss = 2.8507955074310303\n",
            "step 5821: loss = 2.9404664039611816\n",
            "step 5822: loss = 3.168905019760132\n",
            "step 5823: loss = 2.815706491470337\n",
            "step 5824: loss = 3.0909392833709717\n",
            "step 5825: loss = 2.7057714462280273\n",
            "step 5826: loss = 2.8533289432525635\n",
            "step 5827: loss = 2.9668262004852295\n",
            "step 5828: loss = 3.241137742996216\n",
            "step 5829: loss = 2.8322622776031494\n",
            "step 5830: loss = 3.1701087951660156\n",
            "step 5831: loss = 2.8976924419403076\n",
            "step 5832: loss = 2.755070924758911\n",
            "step 5833: loss = 2.9237401485443115\n",
            "step 5834: loss = 2.809119701385498\n",
            "step 5835: loss = 2.8936963081359863\n",
            "step 5836: loss = 2.548417806625366\n",
            "step 5837: loss = 2.8936452865600586\n",
            "step 5838: loss = 2.869539260864258\n",
            "step 5839: loss = 3.0654454231262207\n",
            "step 5840: loss = 2.8851141929626465\n",
            "step 5841: loss = 2.9270927906036377\n",
            "step 5842: loss = 3.214351177215576\n",
            "step 5843: loss = 2.984724521636963\n",
            "step 5844: loss = 3.0327248573303223\n",
            "step 5845: loss = 2.9717633724212646\n",
            "step 5846: loss = 3.2027475833892822\n",
            "step 5847: loss = 2.7705371379852295\n",
            "step 5848: loss = 2.9071574211120605\n",
            "step 5849: loss = 2.667379140853882\n",
            "step 5850: loss = 3.1952953338623047\n",
            "step 5851: loss = 3.1158218383789062\n",
            "step 5852: loss = 2.953861713409424\n",
            "step 5853: loss = 2.835444927215576\n",
            "step 5854: loss = 2.9312894344329834\n",
            "step 5855: loss = 3.2780158519744873\n",
            "step 5856: loss = 3.0180201530456543\n",
            "step 5857: loss = 3.1093060970306396\n",
            "step 5858: loss = 3.001237392425537\n",
            "step 5859: loss = 2.911968469619751\n",
            "step 5860: loss = 3.1196937561035156\n",
            "step 5861: loss = 2.9349751472473145\n",
            "step 5862: loss = 2.907909393310547\n",
            "step 5863: loss = 3.052497386932373\n",
            "step 5864: loss = 3.2222301959991455\n",
            "step 5865: loss = 2.954477310180664\n",
            "step 5866: loss = 2.876711130142212\n",
            "step 5867: loss = 3.1789355278015137\n",
            "step 5868: loss = 2.9898006916046143\n",
            "step 5869: loss = 2.997082233428955\n",
            "step 5870: loss = 3.018216848373413\n",
            "step 5871: loss = 2.834622383117676\n",
            "step 5872: loss = 2.977008104324341\n",
            "step 5873: loss = 2.971458673477173\n",
            "step 5874: loss = 2.8118913173675537\n",
            "step 5875: loss = 3.0755183696746826\n",
            "step 5876: loss = 3.334167718887329\n",
            "step 5877: loss = 2.955561876296997\n",
            "step 5878: loss = 3.150520086288452\n",
            "step 5879: loss = 3.125340461730957\n",
            "step 5880: loss = 2.9489352703094482\n",
            "step 5881: loss = 3.0905303955078125\n",
            "step 5882: loss = 2.6792898178100586\n",
            "step 5883: loss = 3.037114143371582\n",
            "step 5884: loss = 2.8221986293792725\n",
            "step 5885: loss = 2.9569740295410156\n",
            "step 5886: loss = 2.977686643600464\n",
            "step 5887: loss = 2.711106538772583\n",
            "step 5888: loss = 2.9006712436676025\n",
            "step 5889: loss = 3.152066230773926\n",
            "step 5890: loss = 3.0220839977264404\n",
            "step 5891: loss = 3.1786890029907227\n",
            "step 5892: loss = 3.266810417175293\n",
            "step 5893: loss = 3.165571689605713\n",
            "step 5894: loss = 2.9575893878936768\n",
            "step 5895: loss = 2.622642755508423\n",
            "step 5896: loss = 3.045257568359375\n",
            "step 5897: loss = 2.9282803535461426\n",
            "step 5898: loss = 3.0221927165985107\n",
            "step 5899: loss = 2.896369457244873\n",
            "step 5900: loss = 2.884753942489624\n",
            "step 5901: loss = 3.0067965984344482\n",
            "step 5902: loss = 2.9173407554626465\n",
            "step 5903: loss = 2.942371129989624\n",
            "step 5904: loss = 3.094026565551758\n",
            "step 5905: loss = 3.080929756164551\n",
            "step 5906: loss = 2.8645918369293213\n",
            "step 5907: loss = 3.1005771160125732\n",
            "step 5908: loss = 3.0731494426727295\n",
            "step 5909: loss = 2.8209612369537354\n",
            "step 5910: loss = 3.0060667991638184\n",
            "step 5911: loss = 3.163235664367676\n",
            "step 5912: loss = 2.8507354259490967\n",
            "step 5913: loss = 2.914921283721924\n",
            "step 5914: loss = 3.017510175704956\n",
            "step 5915: loss = 2.9315381050109863\n",
            "step 5916: loss = 2.8716063499450684\n",
            "step 5917: loss = 2.9293951988220215\n",
            "step 5918: loss = 3.0506699085235596\n",
            "step 5919: loss = 2.9595959186553955\n",
            "step 5920: loss = 3.261655807495117\n",
            "step 5921: loss = 2.953038215637207\n",
            "step 5922: loss = 2.874622106552124\n",
            "step 5923: loss = 2.85137939453125\n",
            "step 5924: loss = 2.9240355491638184\n",
            "step 5925: loss = 2.9100840091705322\n",
            "step 5926: loss = 3.0136702060699463\n",
            "step 5927: loss = 3.2300658226013184\n",
            "step 5928: loss = 2.743706464767456\n",
            "step 5929: loss = 2.711308479309082\n",
            "step 5930: loss = 3.195662021636963\n",
            "step 5931: loss = 2.8913443088531494\n",
            "step 5932: loss = 2.9093120098114014\n",
            "step 5933: loss = 3.274958610534668\n",
            "step 5934: loss = 2.743920087814331\n",
            "step 5935: loss = 3.211643695831299\n",
            "step 5936: loss = 3.348357677459717\n",
            "step 5937: loss = 2.671767234802246\n",
            "step 5938: loss = 3.201655864715576\n",
            "step 5939: loss = 2.9298653602600098\n",
            "step 5940: loss = 2.5748841762542725\n",
            "step 5941: loss = 2.7323663234710693\n",
            "step 5942: loss = 3.050158739089966\n",
            "step 5943: loss = 3.2083184719085693\n",
            "step 5944: loss = 2.6348795890808105\n",
            "step 5945: loss = 3.0290884971618652\n",
            "step 5946: loss = 2.9102063179016113\n",
            "step 5947: loss = 3.081725835800171\n",
            "step 5948: loss = 3.0863749980926514\n",
            "step 5949: loss = 3.102705478668213\n",
            "step 5950: loss = 2.657979726791382\n",
            "step 5951: loss = 2.786935567855835\n",
            "step 5952: loss = 2.892350196838379\n",
            "step 5953: loss = 2.8854289054870605\n",
            "step 5954: loss = 3.3209595680236816\n",
            "step 5955: loss = 2.8671340942382812\n",
            "step 5956: loss = 2.8972365856170654\n",
            "step 5957: loss = 2.9210939407348633\n",
            "step 5958: loss = 2.9793753623962402\n",
            "step 5959: loss = 3.0186004638671875\n",
            "step 5960: loss = 3.0318400859832764\n",
            "step 5961: loss = 2.8718643188476562\n",
            "step 5962: loss = 2.9726407527923584\n",
            "step 5963: loss = 2.839092969894409\n",
            "step 5964: loss = 3.035304546356201\n",
            "step 5965: loss = 3.2498509883880615\n",
            "step 5966: loss = 3.0562779903411865\n",
            "step 5967: loss = 2.765166997909546\n",
            "step 5968: loss = 3.018373966217041\n",
            "step 5969: loss = 2.712006092071533\n",
            "step 5970: loss = 2.8008179664611816\n",
            "step 5971: loss = 2.8561244010925293\n",
            "step 5972: loss = 3.0554935932159424\n",
            "step 5973: loss = 2.912729263305664\n",
            "step 5974: loss = 2.919741153717041\n",
            "step 5975: loss = 2.8439159393310547\n",
            "step 5976: loss = 3.091317892074585\n",
            "step 5977: loss = 3.0102992057800293\n",
            "step 5978: loss = 2.7914559841156006\n",
            "step 5979: loss = 3.009617805480957\n",
            "step 5980: loss = 3.1018645763397217\n",
            "step 5981: loss = 3.290653944015503\n",
            "step 5982: loss = 3.1378674507141113\n",
            "step 5983: loss = 3.075712203979492\n",
            "step 5984: loss = 3.038580894470215\n",
            "step 5985: loss = 2.953253984451294\n",
            "step 5986: loss = 3.1757898330688477\n",
            "step 5987: loss = 2.8025383949279785\n",
            "step 5988: loss = 3.2238059043884277\n",
            "step 5989: loss = 2.6661064624786377\n",
            "step 5990: loss = 3.202486753463745\n",
            "step 5991: loss = 2.943974256515503\n",
            "step 5992: loss = 3.0577566623687744\n",
            "step 5993: loss = 3.0117220878601074\n",
            "step 5994: loss = 3.3890504837036133\n",
            "step 5995: loss = 2.925173759460449\n",
            "step 5996: loss = 2.9939730167388916\n",
            "step 5997: loss = 3.017294406890869\n",
            "step 5998: loss = 2.6739141941070557\n",
            "step 5999: loss = 2.801679849624634\n",
            "step 6000: loss = 2.907719850540161\n",
            "step 6001: loss = 2.9485442638397217\n",
            "step 6002: loss = 2.8948283195495605\n",
            "step 6003: loss = 2.869097948074341\n",
            "step 6004: loss = 2.9557254314422607\n",
            "step 6005: loss = 2.861934185028076\n",
            "step 6006: loss = 2.842730760574341\n",
            "step 6007: loss = 2.955751419067383\n",
            "step 6008: loss = 2.9802629947662354\n",
            "step 6009: loss = 3.0935943126678467\n",
            "step 6010: loss = 2.949195146560669\n",
            "step 6011: loss = 3.2600162029266357\n",
            "step 6012: loss = 2.6525886058807373\n",
            "step 6013: loss = 3.160365581512451\n",
            "step 6014: loss = 2.7604150772094727\n",
            "step 6015: loss = 2.8071932792663574\n",
            "step 6016: loss = 3.0052480697631836\n",
            "step 6017: loss = 3.0291991233825684\n",
            "step 6018: loss = 2.6144752502441406\n",
            "step 6019: loss = 3.137604236602783\n",
            "step 6020: loss = 2.769249439239502\n",
            "step 6021: loss = 3.1197855472564697\n",
            "step 6022: loss = 2.7965927124023438\n",
            "step 6023: loss = 2.8621368408203125\n",
            "step 6024: loss = 3.0609099864959717\n",
            "step 6025: loss = 3.0777595043182373\n",
            "step 6026: loss = 2.7985246181488037\n",
            "step 6027: loss = 2.8013224601745605\n",
            "step 6028: loss = 2.7903800010681152\n",
            "step 6029: loss = 3.141413688659668\n",
            "step 6030: loss = 3.189770460128784\n",
            "step 6031: loss = 2.9505863189697266\n",
            "step 6032: loss = 3.0955865383148193\n",
            "step 6033: loss = 2.7978532314300537\n",
            "step 6034: loss = 3.427522897720337\n",
            "step 6035: loss = 3.062215566635132\n",
            "step 6036: loss = 3.280076265335083\n",
            "step 6037: loss = 2.8604695796966553\n",
            "step 6038: loss = 2.933201313018799\n",
            "step 6039: loss = 2.949944019317627\n",
            "step 6040: loss = 2.5864598751068115\n",
            "step 6041: loss = 3.455465078353882\n",
            "step 6042: loss = 2.8075168132781982\n",
            "step 6043: loss = 3.276113748550415\n",
            "step 6044: loss = 3.0170738697052\n",
            "step 6045: loss = 2.9472200870513916\n",
            "step 6046: loss = 2.967099905014038\n",
            "step 6047: loss = 2.732038974761963\n",
            "step 6048: loss = 3.034101724624634\n",
            "step 6049: loss = 2.666856050491333\n",
            "step 6050: loss = 3.0044033527374268\n",
            "step 6051: loss = 2.876333475112915\n",
            "step 6052: loss = 2.5237183570861816\n",
            "step 6053: loss = 2.9984426498413086\n",
            "step 6054: loss = 3.222825527191162\n",
            "step 6055: loss = 2.883307456970215\n",
            "step 6056: loss = 3.008010149002075\n",
            "step 6057: loss = 2.6925277709960938\n",
            "step 6058: loss = 2.9724957942962646\n",
            "step 6059: loss = 2.9087297916412354\n",
            "step 6060: loss = 2.7152464389801025\n",
            "step 6061: loss = 2.743628978729248\n",
            "step 6062: loss = 2.803678512573242\n",
            "step 6063: loss = 3.058227777481079\n",
            "step 6064: loss = 2.61075758934021\n",
            "step 6065: loss = 2.9385061264038086\n",
            "step 6066: loss = 2.937891721725464\n",
            "step 6067: loss = 3.0771725177764893\n",
            "step 6068: loss = 3.0980730056762695\n",
            "step 6069: loss = 2.6285321712493896\n",
            "step 6070: loss = 2.9906227588653564\n",
            "step 6071: loss = 3.215151786804199\n",
            "step 6072: loss = 3.064709186553955\n",
            "step 6073: loss = 2.944737434387207\n",
            "step 6074: loss = 3.070113182067871\n",
            "step 6075: loss = 2.9504892826080322\n",
            "step 6076: loss = 2.758307695388794\n",
            "step 6077: loss = 2.837217330932617\n",
            "step 6078: loss = 3.411996364593506\n",
            "step 6079: loss = 3.0443365573883057\n",
            "step 6080: loss = 3.181910753250122\n",
            "step 6081: loss = 2.9245834350585938\n",
            "step 6082: loss = 3.1578257083892822\n",
            "step 6083: loss = 2.7568163871765137\n",
            "step 6084: loss = 3.246173620223999\n",
            "step 6085: loss = 2.7335691452026367\n",
            "step 6086: loss = 2.72937273979187\n",
            "step 6087: loss = 2.921792984008789\n",
            "step 6088: loss = 2.8622734546661377\n",
            "step 6089: loss = 2.5208094120025635\n",
            "step 6090: loss = 3.1569533348083496\n",
            "step 6091: loss = 3.107114553451538\n",
            "step 6092: loss = 3.152230739593506\n",
            "step 6093: loss = 3.101327657699585\n",
            "step 6094: loss = 2.9211273193359375\n",
            "step 6095: loss = 2.8852436542510986\n",
            "step 6096: loss = 2.8757810592651367\n",
            "step 6097: loss = 3.076733350753784\n",
            "step 6098: loss = 2.8805408477783203\n",
            "step 6099: loss = 3.115222215652466\n",
            "step 6100: loss = 3.156412124633789\n",
            "step 6101: loss = 2.953453779220581\n",
            "step 6102: loss = 3.14393949508667\n",
            "step 6103: loss = 2.945566415786743\n",
            "step 6104: loss = 2.9803261756896973\n",
            "step 6105: loss = 3.2036209106445312\n",
            "step 6106: loss = 2.899766683578491\n",
            "step 6107: loss = 3.1713926792144775\n",
            "step 6108: loss = 3.0978095531463623\n",
            "step 6109: loss = 3.0129449367523193\n",
            "step 6110: loss = 3.076113700866699\n",
            "step 6111: loss = 3.0292465686798096\n",
            "step 6112: loss = 3.074950695037842\n",
            "step 6113: loss = 2.8801965713500977\n",
            "step 6114: loss = 3.135571002960205\n",
            "step 6115: loss = 3.098585605621338\n",
            "step 6116: loss = 3.322756052017212\n",
            "step 6117: loss = 2.766627550125122\n",
            "step 6118: loss = 2.889573335647583\n",
            "step 6119: loss = 3.153729200363159\n",
            "step 6120: loss = 2.917391538619995\n",
            "step 6121: loss = 3.176154613494873\n",
            "step 6122: loss = 3.2157678604125977\n",
            "step 6123: loss = 3.200510263442993\n",
            "step 6124: loss = 2.768958330154419\n",
            "step 6125: loss = 3.1849453449249268\n",
            "step 6126: loss = 2.972504138946533\n",
            "step 6127: loss = 3.104957103729248\n",
            "step 6128: loss = 3.1358189582824707\n",
            "step 6129: loss = 2.7269446849823\n",
            "step 6130: loss = 3.0315732955932617\n",
            "step 6131: loss = 2.9953606128692627\n",
            "step 6132: loss = 3.04296875\n",
            "step 6133: loss = 3.081615924835205\n",
            "step 6134: loss = 3.1164982318878174\n",
            "step 6135: loss = 3.0331225395202637\n",
            "step 6136: loss = 2.873755693435669\n",
            "step 6137: loss = 3.3918206691741943\n",
            "step 6138: loss = 2.7619898319244385\n",
            "step 6139: loss = 2.868497371673584\n",
            "step 6140: loss = 2.849489450454712\n",
            "step 6141: loss = 3.0972845554351807\n",
            "step 6142: loss = 3.267510414123535\n",
            "step 6143: loss = 2.9260942935943604\n",
            "step 6144: loss = 2.9341084957122803\n",
            "step 6145: loss = 2.9556620121002197\n",
            "step 6146: loss = 3.023724317550659\n",
            "step 6147: loss = 2.8940093517303467\n",
            "step 6148: loss = 2.9465458393096924\n",
            "step 6149: loss = 2.79384446144104\n",
            "step 6150: loss = 2.8331973552703857\n",
            "step 6151: loss = 2.8274741172790527\n",
            "step 6152: loss = 2.856679916381836\n",
            "step 6153: loss = 2.995847463607788\n",
            "step 6154: loss = 3.2514288425445557\n",
            "step 6155: loss = 2.864865303039551\n",
            "step 6156: loss = 2.948899984359741\n",
            "step 6157: loss = 3.163708209991455\n",
            "step 6158: loss = 3.0176589488983154\n",
            "step 6159: loss = 3.0302469730377197\n",
            "step 6160: loss = 2.876276731491089\n",
            "step 6161: loss = 2.8724677562713623\n",
            "step 6162: loss = 3.127061128616333\n",
            "step 6163: loss = 2.911616086959839\n",
            "step 6164: loss = 3.002047300338745\n",
            "step 6165: loss = 2.9172003269195557\n",
            "step 6166: loss = 2.9622154235839844\n",
            "step 6167: loss = 3.0511727333068848\n",
            "step 6168: loss = 2.964987277984619\n",
            "step 6169: loss = 2.8131351470947266\n",
            "step 6170: loss = 3.2799839973449707\n",
            "step 6171: loss = 2.797335386276245\n",
            "step 6172: loss = 2.4616951942443848\n",
            "step 6173: loss = 3.3063151836395264\n",
            "step 6174: loss = 2.8598294258117676\n",
            "step 6175: loss = 3.0742061138153076\n",
            "step 6176: loss = 2.965014696121216\n",
            "step 6177: loss = 3.10850191116333\n",
            "step 6178: loss = 3.1185925006866455\n",
            "step 6179: loss = 2.804718494415283\n",
            "step 6180: loss = 3.1698715686798096\n",
            "step 6181: loss = 3.0862905979156494\n",
            "step 6182: loss = 3.101958990097046\n",
            "step 6183: loss = 2.7982687950134277\n",
            "step 6184: loss = 2.790463447570801\n",
            "step 6185: loss = 2.8806850910186768\n",
            "step 6186: loss = 2.926368474960327\n",
            "step 6187: loss = 2.7234883308410645\n",
            "step 6188: loss = 2.8122916221618652\n",
            "step 6189: loss = 2.9834606647491455\n",
            "step 6190: loss = 2.802635431289673\n",
            "step 6191: loss = 3.010875701904297\n",
            "step 6192: loss = 2.86004900932312\n",
            "step 6193: loss = 2.8351922035217285\n",
            "step 6194: loss = 2.988304853439331\n",
            "step 6195: loss = 3.4608514308929443\n",
            "step 6196: loss = 2.6694092750549316\n",
            "step 6197: loss = 3.169466257095337\n",
            "step 6198: loss = 3.0248796939849854\n",
            "step 6199: loss = 3.2091004848480225\n",
            "step 6200: loss = 2.9865808486938477\n",
            "step 6201: loss = 2.757188081741333\n",
            "step 6202: loss = 3.0696780681610107\n",
            "step 6203: loss = 2.927598237991333\n",
            "step 6204: loss = 3.004310131072998\n",
            "step 6205: loss = 2.9467554092407227\n",
            "step 6206: loss = 3.0048329830169678\n",
            "step 6207: loss = 3.146963357925415\n",
            "step 6208: loss = 2.768847942352295\n",
            "step 6209: loss = 3.070629358291626\n",
            "step 6210: loss = 2.948700428009033\n",
            "step 6211: loss = 3.276577949523926\n",
            "step 6212: loss = 2.84428071975708\n",
            "step 6213: loss = 3.35734486579895\n",
            "step 6214: loss = 2.9806759357452393\n",
            "step 6215: loss = 2.935246706008911\n",
            "step 6216: loss = 2.6969118118286133\n",
            "step 6217: loss = 2.634413242340088\n",
            "step 6218: loss = 2.662235736846924\n",
            "step 6219: loss = 2.7811756134033203\n",
            "step 6220: loss = 2.7516205310821533\n",
            "step 6221: loss = 2.973114490509033\n",
            "step 6222: loss = 2.9245104789733887\n",
            "step 6223: loss = 2.8292109966278076\n",
            "step 6224: loss = 3.2157225608825684\n",
            "step 6225: loss = 3.1429083347320557\n",
            "step 6226: loss = 2.92277455329895\n",
            "step 6227: loss = 2.7285659313201904\n",
            "step 6228: loss = 3.3175477981567383\n",
            "step 6229: loss = 2.8739161491394043\n",
            "step 6230: loss = 3.0346639156341553\n",
            "step 6231: loss = 3.1257221698760986\n",
            "step 6232: loss = 3.0921638011932373\n",
            "step 6233: loss = 3.0792856216430664\n",
            "step 6234: loss = 3.0254127979278564\n",
            "step 6235: loss = 2.965714693069458\n",
            "step 6236: loss = 2.7727904319763184\n",
            "step 6237: loss = 2.9343786239624023\n",
            "step 6238: loss = 2.987309694290161\n",
            "step 6239: loss = 2.8971219062805176\n",
            "step 6240: loss = 2.9072229862213135\n",
            "step 6241: loss = 2.8704023361206055\n",
            "step 6242: loss = 2.866363286972046\n",
            "step 6243: loss = 3.012354850769043\n",
            "step 6244: loss = 3.0475385189056396\n",
            "step 6245: loss = 2.841959238052368\n",
            "step 6246: loss = 3.287363052368164\n",
            "step 6247: loss = 2.9684360027313232\n",
            "step 6248: loss = 3.094259262084961\n",
            "Finish epoch 4\n",
            "New model saved, minimum loss: 2.9414501690834034 \n",
            "\n",
            "step 6249: loss = 2.5178349018096924\n",
            "step 6250: loss = 2.706590175628662\n",
            "step 6251: loss = 2.5108590126037598\n",
            "step 6252: loss = 2.6531808376312256\n",
            "step 6253: loss = 2.3743324279785156\n",
            "step 6254: loss = 2.5991299152374268\n",
            "step 6255: loss = 2.4075381755828857\n",
            "step 6256: loss = 2.47004771232605\n",
            "step 6257: loss = 2.462217330932617\n",
            "step 6258: loss = 2.3105127811431885\n",
            "step 6259: loss = 2.773519277572632\n",
            "step 6260: loss = 2.4986767768859863\n",
            "step 6261: loss = 2.4603686332702637\n",
            "step 6262: loss = 2.4898486137390137\n",
            "step 6263: loss = 2.391000270843506\n",
            "step 6264: loss = 2.474073886871338\n",
            "step 6265: loss = 2.4768004417419434\n",
            "step 6266: loss = 2.447624683380127\n",
            "step 6267: loss = 2.7462706565856934\n",
            "step 6268: loss = 2.583754539489746\n",
            "step 6269: loss = 2.2718589305877686\n",
            "step 6270: loss = 2.619436740875244\n",
            "step 6271: loss = 2.4451797008514404\n",
            "step 6272: loss = 2.490046977996826\n",
            "step 6273: loss = 2.6352286338806152\n",
            "step 6274: loss = 2.4783709049224854\n",
            "step 6275: loss = 2.615125894546509\n",
            "step 6276: loss = 2.5699918270111084\n",
            "step 6277: loss = 2.5825183391571045\n",
            "step 6278: loss = 2.7486202716827393\n",
            "step 6279: loss = 2.5949313640594482\n",
            "step 6280: loss = 2.5071401596069336\n",
            "step 6281: loss = 2.548229932785034\n",
            "step 6282: loss = 2.444817543029785\n",
            "step 6283: loss = 2.398188829421997\n",
            "step 6284: loss = 2.5347495079040527\n",
            "step 6285: loss = 2.321112871170044\n",
            "step 6286: loss = 2.46236252784729\n",
            "step 6287: loss = 2.350600004196167\n",
            "step 6288: loss = 2.4460415840148926\n",
            "step 6289: loss = 2.4382569789886475\n",
            "step 6290: loss = 2.2502434253692627\n",
            "step 6291: loss = 2.729846239089966\n",
            "step 6292: loss = 2.186249256134033\n",
            "step 6293: loss = 2.467655897140503\n",
            "step 6294: loss = 2.464691638946533\n",
            "step 6295: loss = 2.2915849685668945\n",
            "step 6296: loss = 2.671160936355591\n",
            "step 6297: loss = 2.589818239212036\n",
            "step 6298: loss = 2.312912702560425\n",
            "step 6299: loss = 2.552586793899536\n",
            "step 6300: loss = 2.3514084815979004\n",
            "step 6301: loss = 2.621002674102783\n",
            "step 6302: loss = 2.4024033546447754\n",
            "step 6303: loss = 2.289581537246704\n",
            "step 6304: loss = 2.503119468688965\n",
            "step 6305: loss = 2.3001656532287598\n",
            "step 6306: loss = 2.5164260864257812\n",
            "step 6307: loss = 2.4885358810424805\n",
            "step 6308: loss = 2.6453418731689453\n",
            "step 6309: loss = 2.423030376434326\n",
            "step 6310: loss = 2.6139957904815674\n",
            "step 6311: loss = 2.299550771713257\n",
            "step 6312: loss = 2.525202512741089\n",
            "step 6313: loss = 2.7075371742248535\n",
            "step 6314: loss = 2.450917959213257\n",
            "step 6315: loss = 2.562187671661377\n",
            "step 6316: loss = 2.269902467727661\n",
            "step 6317: loss = 2.2391135692596436\n",
            "step 6318: loss = 2.767603635787964\n",
            "step 6319: loss = 2.0077497959136963\n",
            "step 6320: loss = 2.560044288635254\n",
            "step 6321: loss = 2.580796480178833\n",
            "step 6322: loss = 2.694293975830078\n",
            "step 6323: loss = 2.5406136512756348\n",
            "step 6324: loss = 2.589193105697632\n",
            "step 6325: loss = 2.4067330360412598\n",
            "step 6326: loss = 2.532590627670288\n",
            "step 6327: loss = 2.718533992767334\n",
            "step 6328: loss = 2.598688840866089\n",
            "step 6329: loss = 2.725277900695801\n",
            "step 6330: loss = 2.4084768295288086\n",
            "step 6331: loss = 2.5969462394714355\n",
            "step 6332: loss = 2.4543912410736084\n",
            "step 6333: loss = 2.706282138824463\n",
            "step 6334: loss = 2.456331253051758\n",
            "step 6335: loss = 2.1070988178253174\n",
            "step 6336: loss = 2.6292455196380615\n",
            "step 6337: loss = 2.347196102142334\n",
            "step 6338: loss = 2.225536823272705\n",
            "step 6339: loss = 2.304868459701538\n",
            "step 6340: loss = 2.879707098007202\n",
            "step 6341: loss = 2.4779558181762695\n",
            "step 6342: loss = 2.30914306640625\n",
            "step 6343: loss = 2.5710010528564453\n",
            "step 6344: loss = 2.5964131355285645\n",
            "step 6345: loss = 2.303504705429077\n",
            "step 6346: loss = 2.5342626571655273\n",
            "step 6347: loss = 2.4672319889068604\n",
            "step 6348: loss = 2.505937099456787\n",
            "step 6349: loss = 2.350098133087158\n",
            "step 6350: loss = 2.3368911743164062\n",
            "step 6351: loss = 2.4885671138763428\n",
            "step 6352: loss = 2.335806369781494\n",
            "step 6353: loss = 2.290457010269165\n",
            "step 6354: loss = 2.6947824954986572\n",
            "step 6355: loss = 2.4739322662353516\n",
            "step 6356: loss = 2.2709081172943115\n",
            "step 6357: loss = 2.369241952896118\n",
            "step 6358: loss = 2.499063730239868\n",
            "step 6359: loss = 2.56817626953125\n",
            "step 6360: loss = 2.359306812286377\n",
            "step 6361: loss = 2.4648561477661133\n",
            "step 6362: loss = 2.2391974925994873\n",
            "step 6363: loss = 2.459458112716675\n",
            "step 6364: loss = 2.670985460281372\n",
            "step 6365: loss = 2.485288381576538\n",
            "step 6366: loss = 2.725698947906494\n",
            "step 6367: loss = 2.292102575302124\n",
            "step 6368: loss = 2.3788371086120605\n",
            "step 6369: loss = 2.3459229469299316\n",
            "step 6370: loss = 2.455810546875\n",
            "step 6371: loss = 2.578504800796509\n",
            "step 6372: loss = 2.7329814434051514\n",
            "step 6373: loss = 2.286869764328003\n",
            "step 6374: loss = 2.555036783218384\n",
            "step 6375: loss = 2.637890577316284\n",
            "step 6376: loss = 2.5729568004608154\n",
            "step 6377: loss = 2.4652929306030273\n",
            "step 6378: loss = 2.509111166000366\n",
            "step 6379: loss = 2.435671091079712\n",
            "step 6380: loss = 2.6887285709381104\n",
            "step 6381: loss = 2.5224673748016357\n",
            "step 6382: loss = 2.5200612545013428\n",
            "step 6383: loss = 2.8113908767700195\n",
            "step 6384: loss = 2.5127580165863037\n",
            "step 6385: loss = 2.4017391204833984\n",
            "step 6386: loss = 2.3794589042663574\n",
            "step 6387: loss = 2.515376091003418\n",
            "step 6388: loss = 2.5934505462646484\n",
            "step 6389: loss = 2.729585647583008\n",
            "step 6390: loss = 2.262321710586548\n",
            "step 6391: loss = 2.4808995723724365\n",
            "step 6392: loss = 2.7483978271484375\n",
            "step 6393: loss = 2.4666764736175537\n",
            "step 6394: loss = 2.640014171600342\n",
            "step 6395: loss = 2.552219867706299\n",
            "step 6396: loss = 2.4245765209198\n",
            "step 6397: loss = 2.322716236114502\n",
            "step 6398: loss = 2.2111518383026123\n",
            "step 6399: loss = 2.56911039352417\n",
            "step 6400: loss = 2.421886682510376\n",
            "step 6401: loss = 2.470114231109619\n",
            "step 6402: loss = 2.613398790359497\n",
            "step 6403: loss = 2.324507236480713\n",
            "step 6404: loss = 2.545356273651123\n",
            "step 6405: loss = 2.463724136352539\n",
            "step 6406: loss = 2.5499720573425293\n",
            "step 6407: loss = 2.3158962726593018\n",
            "step 6408: loss = 2.6262519359588623\n",
            "step 6409: loss = 2.765475034713745\n",
            "step 6410: loss = 2.450166940689087\n",
            "step 6411: loss = 2.7686233520507812\n",
            "step 6412: loss = 2.5276403427124023\n",
            "step 6413: loss = 2.6885111331939697\n",
            "step 6414: loss = 2.4765758514404297\n",
            "step 6415: loss = 2.7242043018341064\n",
            "step 6416: loss = 2.5894670486450195\n",
            "step 6417: loss = 2.534285306930542\n",
            "step 6418: loss = 2.6144423484802246\n",
            "step 6419: loss = 2.378916025161743\n",
            "step 6420: loss = 2.466539144515991\n",
            "step 6421: loss = 2.4558775424957275\n",
            "step 6422: loss = 2.3680121898651123\n",
            "step 6423: loss = 2.851475715637207\n",
            "step 6424: loss = 2.717984437942505\n",
            "step 6425: loss = 2.3762683868408203\n",
            "step 6426: loss = 2.345338821411133\n",
            "step 6427: loss = 2.786973714828491\n",
            "step 6428: loss = 2.189673900604248\n",
            "step 6429: loss = 2.5932087898254395\n",
            "step 6430: loss = 2.5345826148986816\n",
            "step 6431: loss = 2.6843807697296143\n",
            "step 6432: loss = 2.7803328037261963\n",
            "step 6433: loss = 2.720026969909668\n",
            "step 6434: loss = 2.5610456466674805\n",
            "step 6435: loss = 2.716278076171875\n",
            "step 6436: loss = 2.5499207973480225\n",
            "step 6437: loss = 2.568178415298462\n",
            "step 6438: loss = 2.4265694618225098\n",
            "step 6439: loss = 2.4105305671691895\n",
            "step 6440: loss = 2.459388017654419\n",
            "step 6441: loss = 2.425842523574829\n",
            "step 6442: loss = 2.7653324604034424\n",
            "step 6443: loss = 2.4893720149993896\n",
            "step 6444: loss = 2.489582061767578\n",
            "step 6445: loss = 2.3552300930023193\n",
            "step 6446: loss = 2.5408177375793457\n",
            "step 6447: loss = 2.493715763092041\n",
            "step 6448: loss = 2.451079845428467\n",
            "step 6449: loss = 2.6781065464019775\n",
            "step 6450: loss = 2.4445130825042725\n",
            "step 6451: loss = 2.642817258834839\n",
            "step 6452: loss = 2.368946075439453\n",
            "step 6453: loss = 2.2767179012298584\n",
            "step 6454: loss = 2.504664659500122\n",
            "step 6455: loss = 2.628946542739868\n",
            "step 6456: loss = 2.358931541442871\n",
            "step 6457: loss = 2.5979223251342773\n",
            "step 6458: loss = 2.5298097133636475\n",
            "step 6459: loss = 2.4050261974334717\n",
            "step 6460: loss = 2.512397289276123\n",
            "step 6461: loss = 2.648597002029419\n",
            "step 6462: loss = 2.51006817817688\n",
            "step 6463: loss = 2.6790266036987305\n",
            "step 6464: loss = 2.964175224304199\n",
            "step 6465: loss = 2.7094886302948\n",
            "step 6466: loss = 2.2040586471557617\n",
            "step 6467: loss = 2.7856719493865967\n",
            "step 6468: loss = 2.710831642150879\n",
            "step 6469: loss = 2.5554213523864746\n",
            "step 6470: loss = 2.3661625385284424\n",
            "step 6471: loss = 2.6295688152313232\n",
            "step 6472: loss = 2.532799005508423\n",
            "step 6473: loss = 2.5863571166992188\n",
            "step 6474: loss = 2.6577208042144775\n",
            "step 6475: loss = 2.455920934677124\n",
            "step 6476: loss = 2.637540102005005\n",
            "step 6477: loss = 2.7865068912506104\n",
            "step 6478: loss = 2.659069776535034\n",
            "step 6479: loss = 2.3623509407043457\n",
            "step 6480: loss = 2.4842560291290283\n",
            "step 6481: loss = 2.625805616378784\n",
            "step 6482: loss = 2.6233832836151123\n",
            "step 6483: loss = 2.2222859859466553\n",
            "step 6484: loss = 2.4307656288146973\n",
            "step 6485: loss = 2.4444124698638916\n",
            "step 6486: loss = 2.7680516242980957\n",
            "step 6487: loss = 2.3101041316986084\n",
            "step 6488: loss = 2.3776581287384033\n",
            "step 6489: loss = 2.4845058917999268\n",
            "step 6490: loss = 2.563901901245117\n",
            "step 6491: loss = 2.232537031173706\n",
            "step 6492: loss = 2.5339815616607666\n",
            "step 6493: loss = 2.7211716175079346\n",
            "step 6494: loss = 2.6554365158081055\n",
            "step 6495: loss = 2.625087261199951\n",
            "step 6496: loss = 2.584681510925293\n",
            "step 6497: loss = 2.3670427799224854\n",
            "step 6498: loss = 2.7326407432556152\n",
            "step 6499: loss = 2.5639865398406982\n",
            "step 6500: loss = 2.45369291305542\n",
            "step 6501: loss = 2.540022134780884\n",
            "step 6502: loss = 2.5041415691375732\n",
            "step 6503: loss = 2.613152027130127\n",
            "step 6504: loss = 2.578376531600952\n",
            "step 6505: loss = 2.6481211185455322\n",
            "step 6506: loss = 2.285630702972412\n",
            "step 6507: loss = 2.6493287086486816\n",
            "step 6508: loss = 2.582387685775757\n",
            "step 6509: loss = 2.9340763092041016\n",
            "step 6510: loss = 2.3889882564544678\n",
            "step 6511: loss = 2.7296814918518066\n",
            "step 6512: loss = 2.3604092597961426\n",
            "step 6513: loss = 2.4727742671966553\n",
            "step 6514: loss = 2.768467903137207\n",
            "step 6515: loss = 2.5776255130767822\n",
            "step 6516: loss = 2.7314321994781494\n",
            "step 6517: loss = 2.594719409942627\n",
            "step 6518: loss = 2.612791061401367\n",
            "step 6519: loss = 2.371305227279663\n",
            "step 6520: loss = 2.4705312252044678\n",
            "step 6521: loss = 2.527737855911255\n",
            "step 6522: loss = 2.563170909881592\n",
            "step 6523: loss = 2.558864116668701\n",
            "step 6524: loss = 2.514277219772339\n",
            "step 6525: loss = 2.379621982574463\n",
            "step 6526: loss = 2.5166351795196533\n",
            "step 6527: loss = 2.5024116039276123\n",
            "step 6528: loss = 2.6257946491241455\n",
            "step 6529: loss = 2.526189088821411\n",
            "step 6530: loss = 2.532775402069092\n",
            "step 6531: loss = 2.571916341781616\n",
            "step 6532: loss = 2.742727041244507\n",
            "step 6533: loss = 2.8431637287139893\n",
            "step 6534: loss = 2.655532121658325\n",
            "step 6535: loss = 2.521707773208618\n",
            "step 6536: loss = 2.615478754043579\n",
            "step 6537: loss = 2.373777389526367\n",
            "step 6538: loss = 2.7286744117736816\n",
            "step 6539: loss = 2.4801084995269775\n",
            "step 6540: loss = 2.3089778423309326\n",
            "step 6541: loss = 2.673644781112671\n",
            "step 6542: loss = 2.4195170402526855\n",
            "step 6543: loss = 2.275832414627075\n",
            "step 6544: loss = 2.3470869064331055\n",
            "step 6545: loss = 2.632152795791626\n",
            "step 6546: loss = 2.487433910369873\n",
            "step 6547: loss = 2.472869873046875\n",
            "step 6548: loss = 2.2382071018218994\n",
            "step 6549: loss = 2.577972888946533\n",
            "step 6550: loss = 2.5059783458709717\n",
            "step 6551: loss = 2.389979600906372\n",
            "step 6552: loss = 2.729022741317749\n",
            "step 6553: loss = 2.319978952407837\n",
            "step 6554: loss = 2.413963794708252\n",
            "step 6555: loss = 2.6542553901672363\n",
            "step 6556: loss = 2.7884366512298584\n",
            "step 6557: loss = 2.4138269424438477\n",
            "step 6558: loss = 2.595327377319336\n",
            "step 6559: loss = 2.7805299758911133\n",
            "step 6560: loss = 2.4282944202423096\n",
            "step 6561: loss = 2.475390672683716\n",
            "step 6562: loss = 2.596951723098755\n",
            "step 6563: loss = 2.5019571781158447\n",
            "step 6564: loss = 2.570542097091675\n",
            "step 6565: loss = 2.4764726161956787\n",
            "step 6566: loss = 2.3371639251708984\n",
            "step 6567: loss = 2.2587897777557373\n",
            "step 6568: loss = 2.9105188846588135\n",
            "step 6569: loss = 2.548234701156616\n",
            "step 6570: loss = 2.6384329795837402\n",
            "step 6571: loss = 2.412886619567871\n",
            "step 6572: loss = 2.6629185676574707\n",
            "step 6573: loss = 2.6195716857910156\n",
            "step 6574: loss = 2.2986533641815186\n",
            "step 6575: loss = 2.699662446975708\n",
            "step 6576: loss = 2.350242853164673\n",
            "step 6577: loss = 2.468658685684204\n",
            "step 6578: loss = 2.5285897254943848\n",
            "step 6579: loss = 2.700739860534668\n",
            "step 6580: loss = 2.4750773906707764\n",
            "step 6581: loss = 2.6046149730682373\n",
            "step 6582: loss = 2.6359033584594727\n",
            "step 6583: loss = 2.6031863689422607\n",
            "step 6584: loss = 2.3751060962677\n",
            "step 6585: loss = 2.227083444595337\n",
            "step 6586: loss = 2.368232250213623\n",
            "step 6587: loss = 2.544945478439331\n",
            "step 6588: loss = 2.7481048107147217\n",
            "step 6589: loss = 2.730395793914795\n",
            "step 6590: loss = 2.498915910720825\n",
            "step 6591: loss = 2.6768875122070312\n",
            "step 6592: loss = 2.417320966720581\n",
            "step 6593: loss = 2.490027904510498\n",
            "step 6594: loss = 2.53922438621521\n",
            "step 6595: loss = 2.4664571285247803\n",
            "step 6596: loss = 2.4191884994506836\n",
            "step 6597: loss = 2.536470890045166\n",
            "step 6598: loss = 2.577691078186035\n",
            "step 6599: loss = 2.687164545059204\n",
            "step 6600: loss = 2.2743418216705322\n",
            "step 6601: loss = 2.565960645675659\n",
            "step 6602: loss = 2.48160457611084\n",
            "step 6603: loss = 2.5582447052001953\n",
            "step 6604: loss = 2.253833770751953\n",
            "step 6605: loss = 2.6132986545562744\n",
            "step 6606: loss = 2.6208574771881104\n",
            "step 6607: loss = 2.5400681495666504\n",
            "step 6608: loss = 2.7165725231170654\n",
            "step 6609: loss = 2.7797906398773193\n",
            "step 6610: loss = 2.7050678730010986\n",
            "step 6611: loss = 2.5575006008148193\n",
            "step 6612: loss = 2.3854620456695557\n",
            "step 6613: loss = 2.3652727603912354\n",
            "step 6614: loss = 2.7087631225585938\n",
            "step 6615: loss = 2.469228744506836\n",
            "step 6616: loss = 2.7358968257904053\n",
            "step 6617: loss = 2.3284997940063477\n",
            "step 6618: loss = 2.6384947299957275\n",
            "step 6619: loss = 2.6834378242492676\n",
            "step 6620: loss = 2.7205238342285156\n",
            "step 6621: loss = 2.59987473487854\n",
            "step 6622: loss = 2.5004098415374756\n",
            "step 6623: loss = 2.2888684272766113\n",
            "step 6624: loss = 2.537651300430298\n",
            "step 6625: loss = 2.553938627243042\n",
            "step 6626: loss = 2.5754036903381348\n",
            "step 6627: loss = 2.5473926067352295\n",
            "step 6628: loss = 2.7792129516601562\n",
            "step 6629: loss = 2.8038675785064697\n",
            "step 6630: loss = 2.643780469894409\n",
            "step 6631: loss = 2.3114349842071533\n",
            "step 6632: loss = 2.582899808883667\n",
            "step 6633: loss = 2.5947916507720947\n",
            "step 6634: loss = 2.612030506134033\n",
            "step 6635: loss = 2.530026435852051\n",
            "step 6636: loss = 2.5335757732391357\n",
            "step 6637: loss = 2.5323848724365234\n",
            "step 6638: loss = 2.795267105102539\n",
            "step 6639: loss = 2.5964863300323486\n",
            "step 6640: loss = 2.8481802940368652\n",
            "step 6641: loss = 2.5796546936035156\n",
            "step 6642: loss = 2.5052731037139893\n",
            "step 6643: loss = 2.470257043838501\n",
            "step 6644: loss = 2.3441660404205322\n",
            "step 6645: loss = 2.564497232437134\n",
            "step 6646: loss = 2.5619800090789795\n",
            "step 6647: loss = 2.732783079147339\n",
            "step 6648: loss = 2.224337339401245\n",
            "step 6649: loss = 2.6917405128479004\n",
            "step 6650: loss = 2.4241364002227783\n",
            "step 6651: loss = 2.464609146118164\n",
            "step 6652: loss = 2.594630479812622\n",
            "step 6653: loss = 2.7525413036346436\n",
            "step 6654: loss = 3.0092358589172363\n",
            "step 6655: loss = 2.761838912963867\n",
            "step 6656: loss = 2.787065029144287\n",
            "step 6657: loss = 2.8382513523101807\n",
            "step 6658: loss = 2.692997932434082\n",
            "step 6659: loss = 2.70524001121521\n",
            "step 6660: loss = 2.503692150115967\n",
            "step 6661: loss = 2.525949001312256\n",
            "step 6662: loss = 2.519174814224243\n",
            "step 6663: loss = 2.691178798675537\n",
            "step 6664: loss = 2.4322128295898438\n",
            "step 6665: loss = 2.6408374309539795\n",
            "step 6666: loss = 2.5936501026153564\n",
            "step 6667: loss = 2.5697665214538574\n",
            "step 6668: loss = 2.526334285736084\n",
            "step 6669: loss = 2.5249674320220947\n",
            "step 6670: loss = 2.5683441162109375\n",
            "step 6671: loss = 2.648757219314575\n",
            "step 6672: loss = 2.485518217086792\n",
            "step 6673: loss = 2.4807374477386475\n",
            "step 6674: loss = 2.716590642929077\n",
            "step 6675: loss = 2.4830548763275146\n",
            "step 6676: loss = 2.4497759342193604\n",
            "step 6677: loss = 2.6572890281677246\n",
            "step 6678: loss = 2.3459620475769043\n",
            "step 6679: loss = 2.521672487258911\n",
            "step 6680: loss = 2.453011989593506\n",
            "step 6681: loss = 2.6680405139923096\n",
            "step 6682: loss = 2.384153366088867\n",
            "step 6683: loss = 2.60703182220459\n",
            "step 6684: loss = 2.4281058311462402\n",
            "step 6685: loss = 2.3520143032073975\n",
            "step 6686: loss = 2.6642837524414062\n",
            "step 6687: loss = 2.4658477306365967\n",
            "step 6688: loss = 2.453742027282715\n",
            "step 6689: loss = 2.5695841312408447\n",
            "step 6690: loss = 2.4921555519104004\n",
            "step 6691: loss = 2.7640397548675537\n",
            "step 6692: loss = 2.5146379470825195\n",
            "step 6693: loss = 2.605466365814209\n",
            "step 6694: loss = 2.6720879077911377\n",
            "step 6695: loss = 2.672053098678589\n",
            "step 6696: loss = 2.5660653114318848\n",
            "step 6697: loss = 2.6822941303253174\n",
            "step 6698: loss = 2.6663548946380615\n",
            "step 6699: loss = 2.6344590187072754\n",
            "step 6700: loss = 2.3870656490325928\n",
            "step 6701: loss = 2.5040314197540283\n",
            "step 6702: loss = 2.5046818256378174\n",
            "step 6703: loss = 2.7631101608276367\n",
            "step 6704: loss = 2.6530115604400635\n",
            "step 6705: loss = 2.734917163848877\n",
            "step 6706: loss = 2.3309648036956787\n",
            "step 6707: loss = 2.5471301078796387\n",
            "step 6708: loss = 2.7471375465393066\n",
            "step 6709: loss = 2.5962183475494385\n",
            "step 6710: loss = 2.541630506515503\n",
            "step 6711: loss = 2.404792547225952\n",
            "step 6712: loss = 2.8877854347229004\n",
            "step 6713: loss = 2.6076467037200928\n",
            "step 6714: loss = 2.8087172508239746\n",
            "step 6715: loss = 2.7065269947052\n",
            "step 6716: loss = 2.7250218391418457\n",
            "step 6717: loss = 2.8095643520355225\n",
            "step 6718: loss = 2.253319263458252\n",
            "step 6719: loss = 2.638059139251709\n",
            "step 6720: loss = 2.554762363433838\n",
            "step 6721: loss = 2.625993490219116\n",
            "step 6722: loss = 2.6260769367218018\n",
            "step 6723: loss = 2.630441188812256\n",
            "step 6724: loss = 2.8732478618621826\n",
            "step 6725: loss = 2.627749443054199\n",
            "step 6726: loss = 2.586991548538208\n",
            "step 6727: loss = 2.2729551792144775\n",
            "step 6728: loss = 2.5540249347686768\n",
            "step 6729: loss = 2.5936548709869385\n",
            "step 6730: loss = 2.6028683185577393\n",
            "step 6731: loss = 2.518953323364258\n",
            "step 6732: loss = 2.40423321723938\n",
            "step 6733: loss = 2.503356695175171\n",
            "step 6734: loss = 2.5776102542877197\n",
            "step 6735: loss = 2.4699809551239014\n",
            "step 6736: loss = 2.4954686164855957\n",
            "step 6737: loss = 2.5597915649414062\n",
            "step 6738: loss = 2.477681875228882\n",
            "step 6739: loss = 2.47891902923584\n",
            "step 6740: loss = 2.4711508750915527\n",
            "step 6741: loss = 2.577869415283203\n",
            "step 6742: loss = 2.6684093475341797\n",
            "step 6743: loss = 2.612812042236328\n",
            "step 6744: loss = 2.7025346755981445\n",
            "step 6745: loss = 2.6632211208343506\n",
            "step 6746: loss = 2.59057354927063\n",
            "step 6747: loss = 2.517514228820801\n",
            "step 6748: loss = 2.3524043560028076\n",
            "step 6749: loss = 2.514688730239868\n",
            "step 6750: loss = 2.64411997795105\n",
            "step 6751: loss = 2.5964479446411133\n",
            "step 6752: loss = 2.654083251953125\n",
            "step 6753: loss = 2.748993396759033\n",
            "step 6754: loss = 2.3375654220581055\n",
            "step 6755: loss = 2.7321794033050537\n",
            "step 6756: loss = 2.3876848220825195\n",
            "step 6757: loss = 2.7237181663513184\n",
            "step 6758: loss = 2.3219876289367676\n",
            "step 6759: loss = 2.7666680812835693\n",
            "step 6760: loss = 2.375629186630249\n",
            "step 6761: loss = 2.7433712482452393\n",
            "step 6762: loss = 2.537165880203247\n",
            "step 6763: loss = 2.394519329071045\n",
            "step 6764: loss = 2.9478769302368164\n",
            "step 6765: loss = 2.4519009590148926\n",
            "step 6766: loss = 2.743502616882324\n",
            "step 6767: loss = 2.362617254257202\n",
            "step 6768: loss = 2.6720457077026367\n",
            "step 6769: loss = 2.5894503593444824\n",
            "step 6770: loss = 2.9909539222717285\n",
            "step 6771: loss = 2.6794443130493164\n",
            "step 6772: loss = 2.7241146564483643\n",
            "step 6773: loss = 2.6598660945892334\n",
            "step 6774: loss = 2.7256410121917725\n",
            "step 6775: loss = 2.696096420288086\n",
            "step 6776: loss = 2.5023272037506104\n",
            "step 6777: loss = 2.4780054092407227\n",
            "step 6778: loss = 2.7575998306274414\n",
            "step 6779: loss = 2.72851300239563\n",
            "step 6780: loss = 2.5239834785461426\n",
            "step 6781: loss = 2.7647645473480225\n",
            "step 6782: loss = 2.5389630794525146\n",
            "step 6783: loss = 2.3640518188476562\n",
            "step 6784: loss = 2.64355206489563\n",
            "step 6785: loss = 2.397733688354492\n",
            "step 6786: loss = 2.6061272621154785\n",
            "step 6787: loss = 2.590648889541626\n",
            "step 6788: loss = 2.5421040058135986\n",
            "step 6789: loss = 2.648624897003174\n",
            "step 6790: loss = 2.7129340171813965\n",
            "step 6791: loss = 2.5812466144561768\n",
            "step 6792: loss = 2.4630119800567627\n",
            "step 6793: loss = 2.522031307220459\n",
            "step 6794: loss = 2.416139841079712\n",
            "step 6795: loss = 2.7268059253692627\n",
            "step 6796: loss = 2.805818796157837\n",
            "step 6797: loss = 2.9140207767486572\n",
            "step 6798: loss = 2.4649574756622314\n",
            "step 6799: loss = 2.567540168762207\n",
            "step 6800: loss = 2.4391696453094482\n",
            "step 6801: loss = 2.8010027408599854\n",
            "step 6802: loss = 2.6903204917907715\n",
            "step 6803: loss = 2.758300304412842\n",
            "step 6804: loss = 2.682563066482544\n",
            "step 6805: loss = 2.8581604957580566\n",
            "step 6806: loss = 2.5783467292785645\n",
            "step 6807: loss = 2.5447371006011963\n",
            "step 6808: loss = 2.748546600341797\n",
            "step 6809: loss = 2.538221597671509\n",
            "step 6810: loss = 2.8500237464904785\n",
            "step 6811: loss = 2.8480007648468018\n",
            "step 6812: loss = 2.5464985370635986\n",
            "step 6813: loss = 2.787813901901245\n",
            "step 6814: loss = 2.4279611110687256\n",
            "step 6815: loss = 2.6268913745880127\n",
            "step 6816: loss = 2.5215096473693848\n",
            "step 6817: loss = 2.235982656478882\n",
            "step 6818: loss = 2.608025550842285\n",
            "step 6819: loss = 2.7924742698669434\n",
            "step 6820: loss = 2.9658803939819336\n",
            "step 6821: loss = 2.512174606323242\n",
            "step 6822: loss = 2.722839117050171\n",
            "step 6823: loss = 2.7157065868377686\n",
            "step 6824: loss = 2.56795072555542\n",
            "step 6825: loss = 2.55155086517334\n",
            "step 6826: loss = 2.518979549407959\n",
            "step 6827: loss = 2.4707069396972656\n",
            "step 6828: loss = 2.5771610736846924\n",
            "step 6829: loss = 2.5535807609558105\n",
            "step 6830: loss = 2.5203394889831543\n",
            "step 6831: loss = 2.755392074584961\n",
            "step 6832: loss = 2.684507131576538\n",
            "step 6833: loss = 2.817202568054199\n",
            "step 6834: loss = 2.356483221054077\n",
            "step 6835: loss = 2.531067371368408\n",
            "step 6836: loss = 2.9816465377807617\n",
            "step 6837: loss = 2.3549020290374756\n",
            "step 6838: loss = 3.058901786804199\n",
            "step 6839: loss = 2.3376176357269287\n",
            "step 6840: loss = 2.839777946472168\n",
            "step 6841: loss = 2.9571917057037354\n",
            "step 6842: loss = 2.631413459777832\n",
            "step 6843: loss = 2.492243528366089\n",
            "step 6844: loss = 2.6383309364318848\n",
            "step 6845: loss = 2.8088295459747314\n",
            "step 6846: loss = 2.5323240756988525\n",
            "step 6847: loss = 2.525444984436035\n",
            "step 6848: loss = 2.6030619144439697\n",
            "step 6849: loss = 2.510072946548462\n",
            "step 6850: loss = 2.8258514404296875\n",
            "step 6851: loss = 2.518993377685547\n",
            "step 6852: loss = 2.742525815963745\n",
            "step 6853: loss = 2.584871530532837\n",
            "step 6854: loss = 2.737582206726074\n",
            "step 6855: loss = 2.5084543228149414\n",
            "step 6856: loss = 2.6510367393493652\n",
            "step 6857: loss = 2.4174113273620605\n",
            "step 6858: loss = 2.905252456665039\n",
            "step 6859: loss = 2.6913974285125732\n",
            "step 6860: loss = 2.4658026695251465\n",
            "step 6861: loss = 2.520036220550537\n",
            "step 6862: loss = 2.4968326091766357\n",
            "step 6863: loss = 2.598757266998291\n",
            "step 6864: loss = 2.8655126094818115\n",
            "step 6865: loss = 2.5070512294769287\n",
            "step 6866: loss = 2.5931007862091064\n",
            "step 6867: loss = 2.8101389408111572\n",
            "step 6868: loss = 2.4775450229644775\n",
            "step 6869: loss = 2.740668296813965\n",
            "step 6870: loss = 2.7569572925567627\n",
            "step 6871: loss = 3.0134451389312744\n",
            "step 6872: loss = 2.3740861415863037\n",
            "step 6873: loss = 2.508396625518799\n",
            "step 6874: loss = 2.677273750305176\n",
            "step 6875: loss = 2.3634135723114014\n",
            "step 6876: loss = 2.6406409740448\n",
            "step 6877: loss = 2.5755183696746826\n",
            "step 6878: loss = 2.608774185180664\n",
            "step 6879: loss = 2.727405309677124\n",
            "step 6880: loss = 2.440323829650879\n",
            "step 6881: loss = 2.7301864624023438\n",
            "step 6882: loss = 2.269362211227417\n",
            "step 6883: loss = 2.4154865741729736\n",
            "step 6884: loss = 2.38919734954834\n",
            "step 6885: loss = 2.660287380218506\n",
            "step 6886: loss = 2.7855844497680664\n",
            "step 6887: loss = 2.435072183609009\n",
            "step 6888: loss = 2.3282063007354736\n",
            "step 6889: loss = 2.843918800354004\n",
            "step 6890: loss = 2.687800168991089\n",
            "step 6891: loss = 2.564135789871216\n",
            "step 6892: loss = 2.683868408203125\n",
            "step 6893: loss = 2.4101643562316895\n",
            "step 6894: loss = 2.6621220111846924\n",
            "step 6895: loss = 2.659518003463745\n",
            "step 6896: loss = 2.5250155925750732\n",
            "step 6897: loss = 2.7413785457611084\n",
            "step 6898: loss = 2.8209385871887207\n",
            "step 6899: loss = 2.692655086517334\n",
            "step 6900: loss = 2.7647452354431152\n",
            "step 6901: loss = 2.39717960357666\n",
            "step 6902: loss = 2.496246814727783\n",
            "step 6903: loss = 2.7474513053894043\n",
            "step 6904: loss = 2.6726768016815186\n",
            "step 6905: loss = 2.6981406211853027\n",
            "step 6906: loss = 2.396660089492798\n",
            "step 6907: loss = 2.4360671043395996\n",
            "step 6908: loss = 2.361544370651245\n",
            "step 6909: loss = 2.572303295135498\n",
            "step 6910: loss = 2.6303787231445312\n",
            "step 6911: loss = 2.6908586025238037\n",
            "step 6912: loss = 2.7222416400909424\n",
            "step 6913: loss = 2.4166481494903564\n",
            "step 6914: loss = 2.5851352214813232\n",
            "step 6915: loss = 2.7143194675445557\n",
            "step 6916: loss = 2.820802927017212\n",
            "step 6917: loss = 2.7219929695129395\n",
            "step 6918: loss = 2.433830738067627\n",
            "step 6919: loss = 2.4881668090820312\n",
            "step 6920: loss = 2.5409047603607178\n",
            "step 6921: loss = 2.7026455402374268\n",
            "step 6922: loss = 2.423409938812256\n",
            "step 6923: loss = 2.5655500888824463\n",
            "step 6924: loss = 2.6862096786499023\n",
            "step 6925: loss = 2.5223286151885986\n",
            "step 6926: loss = 2.6082050800323486\n",
            "step 6927: loss = 2.3362233638763428\n",
            "step 6928: loss = 2.8028900623321533\n",
            "step 6929: loss = 2.7343814373016357\n",
            "step 6930: loss = 2.4774115085601807\n",
            "step 6931: loss = 2.5522146224975586\n",
            "step 6932: loss = 2.6758694648742676\n",
            "step 6933: loss = 2.7976062297821045\n",
            "step 6934: loss = 2.919603109359741\n",
            "step 6935: loss = 2.6968042850494385\n",
            "step 6936: loss = 2.754425525665283\n",
            "step 6937: loss = 2.805095911026001\n",
            "step 6938: loss = 2.264625072479248\n",
            "step 6939: loss = 2.5277397632598877\n",
            "step 6940: loss = 2.5899391174316406\n",
            "step 6941: loss = 2.590393304824829\n",
            "step 6942: loss = 2.6514506340026855\n",
            "step 6943: loss = 2.9020638465881348\n",
            "step 6944: loss = 2.576026439666748\n",
            "step 6945: loss = 2.6973185539245605\n",
            "step 6946: loss = 2.270075798034668\n",
            "step 6947: loss = 2.4103035926818848\n",
            "step 6948: loss = 2.419858932495117\n",
            "step 6949: loss = 2.6082346439361572\n",
            "step 6950: loss = 2.5624215602874756\n",
            "step 6951: loss = 2.6937572956085205\n",
            "step 6952: loss = 2.8457257747650146\n",
            "step 6953: loss = 2.5211782455444336\n",
            "step 6954: loss = 2.6319870948791504\n",
            "step 6955: loss = 2.829350233078003\n",
            "step 6956: loss = 2.774522066116333\n",
            "step 6957: loss = 2.5268359184265137\n",
            "step 6958: loss = 2.6959147453308105\n",
            "step 6959: loss = 2.4422998428344727\n",
            "step 6960: loss = 2.522019624710083\n",
            "step 6961: loss = 2.4691221714019775\n",
            "step 6962: loss = 2.5178747177124023\n",
            "step 6963: loss = 2.7023141384124756\n",
            "step 6964: loss = 2.7524423599243164\n",
            "step 6965: loss = 2.3641505241394043\n",
            "step 6966: loss = 2.4999148845672607\n",
            "step 6967: loss = 2.5655412673950195\n",
            "step 6968: loss = 2.4257571697235107\n",
            "step 6969: loss = 2.8297834396362305\n",
            "step 6970: loss = 2.7518622875213623\n",
            "step 6971: loss = 2.716564178466797\n",
            "step 6972: loss = 2.75838041305542\n",
            "step 6973: loss = 2.5710034370422363\n",
            "step 6974: loss = 2.918752670288086\n",
            "step 6975: loss = 2.4092533588409424\n",
            "step 6976: loss = 2.548520088195801\n",
            "step 6977: loss = 2.623525381088257\n",
            "step 6978: loss = 2.504007339477539\n",
            "step 6979: loss = 2.665090560913086\n",
            "step 6980: loss = 2.8002400398254395\n",
            "step 6981: loss = 2.4897964000701904\n",
            "step 6982: loss = 2.7782399654388428\n",
            "step 6983: loss = 2.325155019760132\n",
            "step 6984: loss = 2.6993021965026855\n",
            "step 6985: loss = 2.6642637252807617\n",
            "step 6986: loss = 2.778217315673828\n",
            "step 6987: loss = 2.569365978240967\n",
            "step 6988: loss = 2.6727497577667236\n",
            "step 6989: loss = 2.7861316204071045\n",
            "step 6990: loss = 2.512664556503296\n",
            "step 6991: loss = 2.488751173019409\n",
            "step 6992: loss = 2.3553338050842285\n",
            "step 6993: loss = 2.6835272312164307\n",
            "step 6994: loss = 2.610354423522949\n",
            "step 6995: loss = 2.863313913345337\n",
            "step 6996: loss = 2.8778131008148193\n",
            "step 6997: loss = 2.7400736808776855\n",
            "step 6998: loss = 2.4931397438049316\n",
            "step 6999: loss = 2.8821535110473633\n",
            "step 7000: loss = 2.384000301361084\n",
            "step 7001: loss = 2.422667980194092\n",
            "step 7002: loss = 2.7937700748443604\n",
            "step 7003: loss = 2.410005807876587\n",
            "step 7004: loss = 2.7528700828552246\n",
            "step 7005: loss = 2.5386970043182373\n",
            "step 7006: loss = 2.8956470489501953\n",
            "step 7007: loss = 2.524176836013794\n",
            "step 7008: loss = 2.6774909496307373\n",
            "step 7009: loss = 2.53269362449646\n",
            "step 7010: loss = 2.633450508117676\n",
            "step 7011: loss = 2.7111144065856934\n",
            "step 7012: loss = 2.8651132583618164\n",
            "step 7013: loss = 2.6657402515411377\n",
            "step 7014: loss = 2.7015883922576904\n",
            "step 7015: loss = 2.8774116039276123\n",
            "step 7016: loss = 2.5368685722351074\n",
            "step 7017: loss = 2.540828227996826\n",
            "step 7018: loss = 2.723940849304199\n",
            "step 7019: loss = 2.549495220184326\n",
            "step 7020: loss = 2.6943519115448\n",
            "step 7021: loss = 2.6821796894073486\n",
            "step 7022: loss = 2.7591934204101562\n",
            "step 7023: loss = 2.305046319961548\n",
            "step 7024: loss = 2.533116340637207\n",
            "step 7025: loss = 2.7890725135803223\n",
            "step 7026: loss = 2.5908024311065674\n",
            "step 7027: loss = 2.3884291648864746\n",
            "step 7028: loss = 2.8237948417663574\n",
            "step 7029: loss = 2.6050665378570557\n",
            "step 7030: loss = 2.6274452209472656\n",
            "step 7031: loss = 2.5661697387695312\n",
            "step 7032: loss = 2.753804922103882\n",
            "step 7033: loss = 2.7396786212921143\n",
            "step 7034: loss = 2.801374912261963\n",
            "step 7035: loss = 2.8200931549072266\n",
            "step 7036: loss = 2.6797571182250977\n",
            "step 7037: loss = 3.109779119491577\n",
            "step 7038: loss = 2.4339599609375\n",
            "step 7039: loss = 2.416966438293457\n",
            "step 7040: loss = 2.6000702381134033\n",
            "step 7041: loss = 2.521120548248291\n",
            "step 7042: loss = 2.6260106563568115\n",
            "step 7043: loss = 2.80016827583313\n",
            "step 7044: loss = 2.4666221141815186\n",
            "step 7045: loss = 2.5253853797912598\n",
            "step 7046: loss = 2.549887180328369\n",
            "step 7047: loss = 2.666574001312256\n",
            "step 7048: loss = 2.5656843185424805\n",
            "step 7049: loss = 2.474360942840576\n",
            "step 7050: loss = 2.6974687576293945\n",
            "step 7051: loss = 2.6943414211273193\n",
            "step 7052: loss = 2.6232175827026367\n",
            "step 7053: loss = 2.4885733127593994\n",
            "step 7054: loss = 2.4468584060668945\n",
            "step 7055: loss = 2.5187501907348633\n",
            "step 7056: loss = 2.452740430831909\n",
            "step 7057: loss = 2.684873580932617\n",
            "step 7058: loss = 2.7442591190338135\n",
            "step 7059: loss = 2.667135238647461\n",
            "step 7060: loss = 2.7386558055877686\n",
            "step 7061: loss = 2.6789753437042236\n",
            "step 7062: loss = 2.6748101711273193\n",
            "step 7063: loss = 2.373427629470825\n",
            "step 7064: loss = 2.3951971530914307\n",
            "step 7065: loss = 2.6093618869781494\n",
            "step 7066: loss = 2.505232810974121\n",
            "step 7067: loss = 2.4267897605895996\n",
            "step 7068: loss = 2.6832032203674316\n",
            "step 7069: loss = 2.51765513420105\n",
            "step 7070: loss = 2.869675397872925\n",
            "step 7071: loss = 2.7461400032043457\n",
            "step 7072: loss = 2.523106575012207\n",
            "step 7073: loss = 2.526254177093506\n",
            "step 7074: loss = 2.606349468231201\n",
            "step 7075: loss = 2.453944444656372\n",
            "step 7076: loss = 2.8403213024139404\n",
            "step 7077: loss = 2.6516239643096924\n",
            "step 7078: loss = 2.61785888671875\n",
            "step 7079: loss = 2.4598848819732666\n",
            "step 7080: loss = 2.6597461700439453\n",
            "step 7081: loss = 2.7250049114227295\n",
            "step 7082: loss = 2.689622640609741\n",
            "step 7083: loss = 2.6576356887817383\n",
            "step 7084: loss = 2.6921768188476562\n",
            "step 7085: loss = 2.707979917526245\n",
            "step 7086: loss = 2.7935080528259277\n",
            "step 7087: loss = 2.5951263904571533\n",
            "step 7088: loss = 2.7834670543670654\n",
            "step 7089: loss = 2.5138792991638184\n",
            "step 7090: loss = 2.9108362197875977\n",
            "step 7091: loss = 2.8639039993286133\n",
            "step 7092: loss = 2.728950262069702\n",
            "step 7093: loss = 2.7319185733795166\n",
            "step 7094: loss = 2.6891896724700928\n",
            "step 7095: loss = 2.7399089336395264\n",
            "step 7096: loss = 2.6275217533111572\n",
            "step 7097: loss = 2.5841658115386963\n",
            "step 7098: loss = 2.73875093460083\n",
            "step 7099: loss = 2.6654961109161377\n",
            "step 7100: loss = 2.723956823348999\n",
            "step 7101: loss = 2.620265007019043\n",
            "step 7102: loss = 2.3948562145233154\n",
            "step 7103: loss = 2.29368257522583\n",
            "step 7104: loss = 2.6075050830841064\n",
            "step 7105: loss = 2.812732219696045\n",
            "step 7106: loss = 2.594078302383423\n",
            "step 7107: loss = 2.3964669704437256\n",
            "step 7108: loss = 2.624237060546875\n",
            "step 7109: loss = 2.712012529373169\n",
            "step 7110: loss = 2.5412497520446777\n",
            "step 7111: loss = 2.3742592334747314\n",
            "step 7112: loss = 2.711298942565918\n",
            "step 7113: loss = 2.5728325843811035\n",
            "step 7114: loss = 2.2856857776641846\n",
            "step 7115: loss = 2.7066845893859863\n",
            "step 7116: loss = 2.986863851547241\n",
            "step 7117: loss = 2.4897940158843994\n",
            "step 7118: loss = 2.680605888366699\n",
            "step 7119: loss = 2.558486223220825\n",
            "step 7120: loss = 2.612440824508667\n",
            "step 7121: loss = 2.725524663925171\n",
            "step 7122: loss = 2.52144455909729\n",
            "step 7123: loss = 2.7405717372894287\n",
            "step 7124: loss = 2.562239170074463\n",
            "step 7125: loss = 2.687624454498291\n",
            "step 7126: loss = 2.5758345127105713\n",
            "step 7127: loss = 2.6724236011505127\n",
            "step 7128: loss = 2.6205976009368896\n",
            "step 7129: loss = 2.939703941345215\n",
            "step 7130: loss = 2.5424227714538574\n",
            "step 7131: loss = 2.496782064437866\n",
            "step 7132: loss = 2.6838717460632324\n",
            "step 7133: loss = 2.8159542083740234\n",
            "step 7134: loss = 2.486506223678589\n",
            "step 7135: loss = 2.568873643875122\n",
            "step 7136: loss = 2.5049140453338623\n",
            "step 7137: loss = 2.487828016281128\n",
            "step 7138: loss = 2.6370129585266113\n",
            "step 7139: loss = 2.6103129386901855\n",
            "step 7140: loss = 2.64245867729187\n",
            "step 7141: loss = 2.642698287963867\n",
            "step 7142: loss = 2.5009331703186035\n",
            "step 7143: loss = 2.753481864929199\n",
            "step 7144: loss = 2.611837863922119\n",
            "step 7145: loss = 2.346891164779663\n",
            "step 7146: loss = 2.671085834503174\n",
            "step 7147: loss = 2.6451103687286377\n",
            "step 7148: loss = 2.693049907684326\n",
            "step 7149: loss = 2.6533570289611816\n",
            "step 7150: loss = 2.594827651977539\n",
            "step 7151: loss = 2.6636769771575928\n",
            "step 7152: loss = 2.9797942638397217\n",
            "step 7153: loss = 2.4162065982818604\n",
            "step 7154: loss = 2.6462063789367676\n",
            "step 7155: loss = 2.6160709857940674\n",
            "step 7156: loss = 3.0392565727233887\n",
            "step 7157: loss = 2.62353253364563\n",
            "step 7158: loss = 2.6741068363189697\n",
            "step 7159: loss = 2.6138224601745605\n",
            "step 7160: loss = 2.732757329940796\n",
            "step 7161: loss = 2.618234395980835\n",
            "step 7162: loss = 2.673886299133301\n",
            "step 7163: loss = 2.562535285949707\n",
            "step 7164: loss = 2.821563482284546\n",
            "step 7165: loss = 2.7379603385925293\n",
            "step 7166: loss = 2.7535574436187744\n",
            "step 7167: loss = 2.9258933067321777\n",
            "step 7168: loss = 2.634681224822998\n",
            "step 7169: loss = 2.5435256958007812\n",
            "step 7170: loss = 2.5270745754241943\n",
            "step 7171: loss = 2.61541748046875\n",
            "step 7172: loss = 2.7240774631500244\n",
            "step 7173: loss = 2.714838981628418\n",
            "step 7174: loss = 2.752715826034546\n",
            "step 7175: loss = 2.9922034740448\n",
            "step 7176: loss = 2.6191468238830566\n",
            "step 7177: loss = 2.7996504306793213\n",
            "step 7178: loss = 2.814410924911499\n",
            "step 7179: loss = 2.861400842666626\n",
            "step 7180: loss = 2.6918036937713623\n",
            "step 7181: loss = 2.7472984790802\n",
            "step 7182: loss = 2.670393705368042\n",
            "step 7183: loss = 2.748084306716919\n",
            "step 7184: loss = 2.6752090454101562\n",
            "step 7185: loss = 2.733727216720581\n",
            "step 7186: loss = 2.450570583343506\n",
            "step 7187: loss = 2.708822011947632\n",
            "step 7188: loss = 2.64044189453125\n",
            "step 7189: loss = 2.37306547164917\n",
            "step 7190: loss = 2.580360174179077\n",
            "step 7191: loss = 2.4857900142669678\n",
            "step 7192: loss = 2.6358816623687744\n",
            "step 7193: loss = 2.750108003616333\n",
            "step 7194: loss = 2.783829927444458\n",
            "step 7195: loss = 2.759727954864502\n",
            "step 7196: loss = 2.634596586227417\n",
            "step 7197: loss = 2.674821615219116\n",
            "step 7198: loss = 2.935399055480957\n",
            "step 7199: loss = 2.835176706314087\n",
            "step 7200: loss = 2.8511838912963867\n",
            "step 7201: loss = 2.616255283355713\n",
            "step 7202: loss = 2.331655979156494\n",
            "step 7203: loss = 2.564305305480957\n",
            "step 7204: loss = 2.559631586074829\n",
            "step 7205: loss = 2.600262403488159\n",
            "step 7206: loss = 2.754145383834839\n",
            "step 7207: loss = 2.4807138442993164\n",
            "step 7208: loss = 2.6146345138549805\n",
            "step 7209: loss = 2.711644411087036\n",
            "step 7210: loss = 2.621461868286133\n",
            "step 7211: loss = 2.4612374305725098\n",
            "step 7212: loss = 2.712139844894409\n",
            "step 7213: loss = 2.682375907897949\n",
            "step 7214: loss = 2.548111915588379\n",
            "step 7215: loss = 2.809164047241211\n",
            "step 7216: loss = 2.4590017795562744\n",
            "step 7217: loss = 2.7533442974090576\n",
            "step 7218: loss = 2.238175868988037\n",
            "step 7219: loss = 2.649080991744995\n",
            "step 7220: loss = 2.514105796813965\n",
            "step 7221: loss = 2.6182870864868164\n",
            "step 7222: loss = 2.5426175594329834\n",
            "step 7223: loss = 2.554039478302002\n",
            "step 7224: loss = 2.4471471309661865\n",
            "step 7225: loss = 2.5367431640625\n",
            "step 7226: loss = 2.5779449939727783\n",
            "step 7227: loss = 2.918264627456665\n",
            "step 7228: loss = 2.5927538871765137\n",
            "step 7229: loss = 2.342195987701416\n",
            "step 7230: loss = 2.7038028240203857\n",
            "step 7231: loss = 2.717015266418457\n",
            "step 7232: loss = 2.5590267181396484\n",
            "step 7233: loss = 2.7772433757781982\n",
            "step 7234: loss = 2.450732707977295\n",
            "step 7235: loss = 2.550401210784912\n",
            "step 7236: loss = 2.593677520751953\n",
            "step 7237: loss = 2.4965641498565674\n",
            "step 7238: loss = 2.748599052429199\n",
            "step 7239: loss = 2.6679866313934326\n",
            "step 7240: loss = 2.4316182136535645\n",
            "step 7241: loss = 2.630910873413086\n",
            "step 7242: loss = 2.497960090637207\n",
            "step 7243: loss = 2.596247434616089\n",
            "step 7244: loss = 2.6420130729675293\n",
            "step 7245: loss = 2.428966999053955\n",
            "step 7246: loss = 2.4955313205718994\n",
            "step 7247: loss = 2.695753574371338\n",
            "step 7248: loss = 2.585381269454956\n",
            "step 7249: loss = 2.664405584335327\n",
            "step 7250: loss = 2.7376248836517334\n",
            "step 7251: loss = 2.4362988471984863\n",
            "step 7252: loss = 2.661393165588379\n",
            "step 7253: loss = 2.4801251888275146\n",
            "step 7254: loss = 2.6813461780548096\n",
            "step 7255: loss = 2.360557794570923\n",
            "step 7256: loss = 2.9040629863739014\n",
            "step 7257: loss = 2.7129056453704834\n",
            "step 7258: loss = 2.6126134395599365\n",
            "step 7259: loss = 2.90889573097229\n",
            "step 7260: loss = 2.5181732177734375\n",
            "step 7261: loss = 2.656350612640381\n",
            "step 7262: loss = 2.9230241775512695\n",
            "step 7263: loss = 2.682133197784424\n",
            "step 7264: loss = 2.5810461044311523\n",
            "step 7265: loss = 2.682429790496826\n",
            "step 7266: loss = 2.6846811771392822\n",
            "step 7267: loss = 2.52120304107666\n",
            "step 7268: loss = 2.732456922531128\n",
            "step 7269: loss = 2.888155937194824\n",
            "step 7270: loss = 2.628671884536743\n",
            "step 7271: loss = 2.5859785079956055\n",
            "step 7272: loss = 2.6453890800476074\n",
            "step 7273: loss = 2.6912007331848145\n",
            "step 7274: loss = 2.5878400802612305\n",
            "step 7275: loss = 2.527949094772339\n",
            "step 7276: loss = 2.7689805030822754\n",
            "step 7277: loss = 2.356647253036499\n",
            "step 7278: loss = 2.689389944076538\n",
            "step 7279: loss = 2.5567002296447754\n",
            "step 7280: loss = 2.6462149620056152\n",
            "step 7281: loss = 2.5160133838653564\n",
            "step 7282: loss = 2.64597487449646\n",
            "step 7283: loss = 2.665376663208008\n",
            "step 7284: loss = 2.5820810794830322\n",
            "step 7285: loss = 2.3920576572418213\n",
            "step 7286: loss = 2.7078604698181152\n",
            "step 7287: loss = 2.631549596786499\n",
            "step 7288: loss = 2.7457571029663086\n",
            "step 7289: loss = 2.510251760482788\n",
            "step 7290: loss = 2.32637095451355\n",
            "step 7291: loss = 2.8018569946289062\n",
            "step 7292: loss = 2.7257039546966553\n",
            "step 7293: loss = 2.49861741065979\n",
            "step 7294: loss = 2.759795665740967\n",
            "step 7295: loss = 2.53707218170166\n",
            "step 7296: loss = 2.423532724380493\n",
            "step 7297: loss = 2.6347036361694336\n",
            "step 7298: loss = 2.6317365169525146\n",
            "step 7299: loss = 2.624122381210327\n",
            "step 7300: loss = 2.9055402278900146\n",
            "step 7301: loss = 2.3850669860839844\n",
            "step 7302: loss = 2.8263354301452637\n",
            "step 7303: loss = 2.6660375595092773\n",
            "step 7304: loss = 2.6108791828155518\n",
            "step 7305: loss = 2.438499927520752\n",
            "step 7306: loss = 2.586263656616211\n",
            "step 7307: loss = 2.568565607070923\n",
            "step 7308: loss = 2.77872633934021\n",
            "step 7309: loss = 2.549898147583008\n",
            "step 7310: loss = 3.1968085765838623\n",
            "step 7311: loss = 2.8082211017608643\n",
            "step 7312: loss = 2.5622901916503906\n",
            "step 7313: loss = 2.672118902206421\n",
            "step 7314: loss = 2.7042016983032227\n",
            "step 7315: loss = 2.6298985481262207\n",
            "step 7316: loss = 2.8927645683288574\n",
            "step 7317: loss = 2.7490007877349854\n",
            "step 7318: loss = 2.658780097961426\n",
            "step 7319: loss = 2.7373788356781006\n",
            "step 7320: loss = 2.7242324352264404\n",
            "step 7321: loss = 2.749915361404419\n",
            "step 7322: loss = 2.586682081222534\n",
            "step 7323: loss = 2.6456801891326904\n",
            "step 7324: loss = 2.583763837814331\n",
            "step 7325: loss = 2.5967767238616943\n",
            "step 7326: loss = 2.491868495941162\n",
            "step 7327: loss = 2.635915517807007\n",
            "step 7328: loss = 2.8756062984466553\n",
            "step 7329: loss = 2.4802865982055664\n",
            "step 7330: loss = 2.4855871200561523\n",
            "step 7331: loss = 2.7798280715942383\n",
            "step 7332: loss = 2.798006772994995\n",
            "step 7333: loss = 2.6800241470336914\n",
            "step 7334: loss = 2.495495319366455\n",
            "step 7335: loss = 2.695051670074463\n",
            "step 7336: loss = 2.5025522708892822\n",
            "step 7337: loss = 2.730656623840332\n",
            "step 7338: loss = 2.5374231338500977\n",
            "step 7339: loss = 2.9093141555786133\n",
            "step 7340: loss = 2.614733934402466\n",
            "step 7341: loss = 2.6291844844818115\n",
            "step 7342: loss = 2.5355942249298096\n",
            "step 7343: loss = 2.470360040664673\n",
            "step 7344: loss = 2.734168291091919\n",
            "step 7345: loss = 2.517362356185913\n",
            "step 7346: loss = 2.591284990310669\n",
            "step 7347: loss = 2.5959343910217285\n",
            "step 7348: loss = 2.6759631633758545\n",
            "step 7349: loss = 2.4682135581970215\n",
            "step 7350: loss = 2.4551429748535156\n",
            "step 7351: loss = 2.6750526428222656\n",
            "step 7352: loss = 2.6594228744506836\n",
            "step 7353: loss = 2.8932442665100098\n",
            "step 7354: loss = 2.556596517562866\n",
            "step 7355: loss = 2.773937463760376\n",
            "step 7356: loss = 2.635650157928467\n",
            "step 7357: loss = 2.588710069656372\n",
            "step 7358: loss = 2.814628839492798\n",
            "step 7359: loss = 2.638922691345215\n",
            "step 7360: loss = 2.758704662322998\n",
            "step 7361: loss = 2.3464319705963135\n",
            "step 7362: loss = 2.5864386558532715\n",
            "step 7363: loss = 2.5867936611175537\n",
            "step 7364: loss = 2.609708547592163\n",
            "step 7365: loss = 2.5409131050109863\n",
            "step 7366: loss = 2.6753621101379395\n",
            "step 7367: loss = 2.5050718784332275\n",
            "step 7368: loss = 2.5144083499908447\n",
            "step 7369: loss = 2.7521376609802246\n",
            "step 7370: loss = 2.67642879486084\n",
            "step 7371: loss = 2.5384585857391357\n",
            "step 7372: loss = 2.7295804023742676\n",
            "step 7373: loss = 2.755298614501953\n",
            "step 7374: loss = 2.532226800918579\n",
            "step 7375: loss = 2.6604483127593994\n",
            "step 7376: loss = 2.49898099899292\n",
            "step 7377: loss = 2.6966588497161865\n",
            "step 7378: loss = 2.9686155319213867\n",
            "step 7379: loss = 2.6179935932159424\n",
            "step 7380: loss = 2.82525372505188\n",
            "step 7381: loss = 2.7643702030181885\n",
            "step 7382: loss = 2.4782934188842773\n",
            "step 7383: loss = 2.899611711502075\n",
            "step 7384: loss = 2.672318458557129\n",
            "step 7385: loss = 2.633634328842163\n",
            "step 7386: loss = 2.520324230194092\n",
            "step 7387: loss = 2.8578569889068604\n",
            "step 7388: loss = 2.7532238960266113\n",
            "step 7389: loss = 2.634965419769287\n",
            "step 7390: loss = 2.65346622467041\n",
            "step 7391: loss = 2.905148506164551\n",
            "step 7392: loss = 2.579885959625244\n",
            "step 7393: loss = 2.571226119995117\n",
            "step 7394: loss = 2.5035505294799805\n",
            "step 7395: loss = 2.3637502193450928\n",
            "step 7396: loss = 2.6883981227874756\n",
            "step 7397: loss = 2.8983891010284424\n",
            "step 7398: loss = 2.620398998260498\n",
            "step 7399: loss = 2.309995412826538\n",
            "step 7400: loss = 2.943118095397949\n",
            "step 7401: loss = 2.6656312942504883\n",
            "step 7402: loss = 2.837038516998291\n",
            "step 7403: loss = 2.6713807582855225\n",
            "step 7404: loss = 2.5458741188049316\n",
            "step 7405: loss = 2.82433819770813\n",
            "step 7406: loss = 2.721425771713257\n",
            "step 7407: loss = 2.8631091117858887\n",
            "step 7408: loss = 2.3576431274414062\n",
            "step 7409: loss = 2.542584180831909\n",
            "step 7410: loss = 2.587763547897339\n",
            "step 7411: loss = 2.5809788703918457\n",
            "step 7412: loss = 2.8082892894744873\n",
            "step 7413: loss = 2.496493101119995\n",
            "step 7414: loss = 2.554950714111328\n",
            "step 7415: loss = 2.7486701011657715\n",
            "step 7416: loss = 2.6433029174804688\n",
            "step 7417: loss = 2.6559860706329346\n",
            "step 7418: loss = 2.765627861022949\n",
            "step 7419: loss = 2.66634202003479\n",
            "step 7420: loss = 2.6490564346313477\n",
            "step 7421: loss = 2.8388681411743164\n",
            "step 7422: loss = 2.5691447257995605\n",
            "step 7423: loss = 2.6818931102752686\n",
            "step 7424: loss = 2.923607587814331\n",
            "step 7425: loss = 2.571625232696533\n",
            "step 7426: loss = 2.647331476211548\n",
            "step 7427: loss = 2.7471160888671875\n",
            "step 7428: loss = 2.337777614593506\n",
            "step 7429: loss = 2.9520955085754395\n",
            "step 7430: loss = 2.671539306640625\n",
            "step 7431: loss = 2.8047778606414795\n",
            "step 7432: loss = 2.634176731109619\n",
            "step 7433: loss = 2.7083959579467773\n",
            "step 7434: loss = 2.917222023010254\n",
            "step 7435: loss = 2.680664539337158\n",
            "step 7436: loss = 2.512042284011841\n",
            "step 7437: loss = 2.712221622467041\n",
            "step 7438: loss = 2.388380289077759\n",
            "step 7439: loss = 2.6296255588531494\n",
            "step 7440: loss = 2.656459093093872\n",
            "step 7441: loss = 2.5890183448791504\n",
            "step 7442: loss = 2.6386184692382812\n",
            "step 7443: loss = 2.490771532058716\n",
            "step 7444: loss = 2.7102537155151367\n",
            "step 7445: loss = 2.4745590686798096\n",
            "step 7446: loss = 2.8731558322906494\n",
            "step 7447: loss = 2.891522169113159\n",
            "step 7448: loss = 2.8289241790771484\n",
            "step 7449: loss = 2.555806875228882\n",
            "step 7450: loss = 2.375333547592163\n",
            "step 7451: loss = 2.813044309616089\n",
            "step 7452: loss = 2.442974090576172\n",
            "step 7453: loss = 2.458573579788208\n",
            "step 7454: loss = 2.938868284225464\n",
            "step 7455: loss = 2.740305185317993\n",
            "step 7456: loss = 2.7835590839385986\n",
            "step 7457: loss = 2.355163812637329\n",
            "step 7458: loss = 2.7675247192382812\n",
            "step 7459: loss = 2.6646509170532227\n",
            "step 7460: loss = 2.63859486579895\n",
            "step 7461: loss = 2.609870672225952\n",
            "step 7462: loss = 2.697791576385498\n",
            "step 7463: loss = 2.530137777328491\n",
            "step 7464: loss = 2.86008358001709\n",
            "step 7465: loss = 2.8458738327026367\n",
            "step 7466: loss = 2.435082197189331\n",
            "step 7467: loss = 2.688997268676758\n",
            "step 7468: loss = 2.6355297565460205\n",
            "step 7469: loss = 2.7043097019195557\n",
            "step 7470: loss = 2.6412012577056885\n",
            "step 7471: loss = 2.577314853668213\n",
            "step 7472: loss = 3.022057294845581\n",
            "step 7473: loss = 2.4667694568634033\n",
            "step 7474: loss = 2.4487411975860596\n",
            "step 7475: loss = 2.792083263397217\n",
            "step 7476: loss = 2.607978105545044\n",
            "step 7477: loss = 2.3548696041107178\n",
            "step 7478: loss = 2.5535337924957275\n",
            "step 7479: loss = 2.53676176071167\n",
            "step 7480: loss = 2.5447258949279785\n",
            "step 7481: loss = 2.4329721927642822\n",
            "step 7482: loss = 2.559896230697632\n",
            "step 7483: loss = 2.692713975906372\n",
            "step 7484: loss = 2.609400749206543\n",
            "step 7485: loss = 2.7754602432250977\n",
            "step 7486: loss = 2.638455629348755\n",
            "step 7487: loss = 2.635235548019409\n",
            "step 7488: loss = 2.584477186203003\n",
            "step 7489: loss = 2.558962821960449\n",
            "step 7490: loss = 2.4885151386260986\n",
            "step 7491: loss = 2.688934087753296\n",
            "step 7492: loss = 2.5554354190826416\n",
            "step 7493: loss = 3.1344521045684814\n",
            "step 7494: loss = 2.7890255451202393\n",
            "step 7495: loss = 3.087803840637207\n",
            "step 7496: loss = 2.550153970718384\n",
            "step 7497: loss = 2.720729112625122\n",
            "step 7498: loss = 2.6804428100585938\n",
            "step 7499: loss = 2.6701598167419434\n",
            "step 7500: loss = 2.694098949432373\n",
            "step 7501: loss = 2.695371627807617\n",
            "step 7502: loss = 2.589261531829834\n",
            "step 7503: loss = 2.8283097743988037\n",
            "step 7504: loss = 2.699772357940674\n",
            "step 7505: loss = 2.4972822666168213\n",
            "step 7506: loss = 2.7427990436553955\n",
            "step 7507: loss = 2.811659574508667\n",
            "step 7508: loss = 2.628964424133301\n",
            "step 7509: loss = 2.8608009815216064\n",
            "step 7510: loss = 2.927790641784668\n",
            "step 7511: loss = 2.6696372032165527\n",
            "step 7512: loss = 2.66607403755188\n",
            "step 7513: loss = 2.674741744995117\n",
            "step 7514: loss = 2.6916208267211914\n",
            "step 7515: loss = 2.802675247192383\n",
            "step 7516: loss = 2.654268741607666\n",
            "step 7517: loss = 2.4601752758026123\n",
            "step 7518: loss = 2.720353603363037\n",
            "step 7519: loss = 2.512540578842163\n",
            "step 7520: loss = 2.9927375316619873\n",
            "step 7521: loss = 2.9722118377685547\n",
            "step 7522: loss = 2.6852498054504395\n",
            "step 7523: loss = 2.621823310852051\n",
            "step 7524: loss = 2.57361102104187\n",
            "step 7525: loss = 2.7068793773651123\n",
            "step 7526: loss = 2.6130120754241943\n",
            "step 7527: loss = 2.510655403137207\n",
            "step 7528: loss = 2.553395986557007\n",
            "step 7529: loss = 2.4714467525482178\n",
            "step 7530: loss = 2.633676290512085\n",
            "step 7531: loss = 2.3764264583587646\n",
            "step 7532: loss = 2.5774545669555664\n",
            "step 7533: loss = 2.5594065189361572\n",
            "step 7534: loss = 2.651162624359131\n",
            "step 7535: loss = 2.6900508403778076\n",
            "step 7536: loss = 2.481710910797119\n",
            "step 7537: loss = 2.8190183639526367\n",
            "step 7538: loss = 2.6000313758850098\n",
            "step 7539: loss = 2.6806111335754395\n",
            "step 7540: loss = 2.671614646911621\n",
            "step 7541: loss = 2.5184011459350586\n",
            "step 7542: loss = 2.57244873046875\n",
            "step 7543: loss = 2.772279739379883\n",
            "step 7544: loss = 2.660033702850342\n",
            "step 7545: loss = 2.5220186710357666\n",
            "step 7546: loss = 2.8740437030792236\n",
            "step 7547: loss = 2.664308786392212\n",
            "step 7548: loss = 2.4852824211120605\n",
            "step 7549: loss = 2.5287046432495117\n",
            "step 7550: loss = 2.6663899421691895\n",
            "step 7551: loss = 2.8250205516815186\n",
            "step 7552: loss = 2.4993133544921875\n",
            "step 7553: loss = 2.7250173091888428\n",
            "step 7554: loss = 2.6006314754486084\n",
            "step 7555: loss = 2.6825268268585205\n",
            "step 7556: loss = 2.6374592781066895\n",
            "step 7557: loss = 2.5438196659088135\n",
            "step 7558: loss = 2.6948623657226562\n",
            "step 7559: loss = 2.8696789741516113\n",
            "step 7560: loss = 2.6544888019561768\n",
            "step 7561: loss = 2.792026996612549\n",
            "step 7562: loss = 2.575402021408081\n",
            "step 7563: loss = 2.5559821128845215\n",
            "step 7564: loss = 2.6140031814575195\n",
            "step 7565: loss = 2.695972204208374\n",
            "step 7566: loss = 3.0367817878723145\n",
            "step 7567: loss = 2.74989652633667\n",
            "step 7568: loss = 2.6331748962402344\n",
            "step 7569: loss = 2.479552745819092\n",
            "step 7570: loss = 2.714742660522461\n",
            "step 7571: loss = 2.771057367324829\n",
            "step 7572: loss = 2.626394271850586\n",
            "step 7573: loss = 2.7556543350219727\n",
            "step 7574: loss = 2.7223329544067383\n",
            "step 7575: loss = 2.6814396381378174\n",
            "step 7576: loss = 2.6132097244262695\n",
            "step 7577: loss = 2.350146770477295\n",
            "step 7578: loss = 2.8693385124206543\n",
            "step 7579: loss = 2.5740556716918945\n",
            "step 7580: loss = 2.8185617923736572\n",
            "step 7581: loss = 2.8702218532562256\n",
            "step 7582: loss = 2.653740882873535\n",
            "step 7583: loss = 2.7727434635162354\n",
            "step 7584: loss = 2.8150038719177246\n",
            "step 7585: loss = 2.7110681533813477\n",
            "step 7586: loss = 2.529630661010742\n",
            "step 7587: loss = 2.7045340538024902\n",
            "step 7588: loss = 2.7245442867279053\n",
            "step 7589: loss = 2.951725482940674\n",
            "step 7590: loss = 2.6071298122406006\n",
            "step 7591: loss = 2.8410332202911377\n",
            "step 7592: loss = 2.635317802429199\n",
            "step 7593: loss = 2.800062417984009\n",
            "step 7594: loss = 2.837812662124634\n",
            "step 7595: loss = 2.6154026985168457\n",
            "step 7596: loss = 2.889456272125244\n",
            "step 7597: loss = 2.467193841934204\n",
            "step 7598: loss = 2.786562442779541\n",
            "step 7599: loss = 2.662803888320923\n",
            "step 7600: loss = 2.578502655029297\n",
            "step 7601: loss = 2.6030092239379883\n",
            "step 7602: loss = 2.637340784072876\n",
            "step 7603: loss = 2.708662986755371\n",
            "step 7604: loss = 2.8893256187438965\n",
            "step 7605: loss = 2.4786317348480225\n",
            "step 7606: loss = 2.69378662109375\n",
            "step 7607: loss = 2.4589779376983643\n",
            "step 7608: loss = 2.67758846282959\n",
            "step 7609: loss = 2.689758777618408\n",
            "step 7610: loss = 2.56965970993042\n",
            "step 7611: loss = 2.7380146980285645\n",
            "step 7612: loss = 2.5882651805877686\n",
            "step 7613: loss = 2.692897081375122\n",
            "step 7614: loss = 2.8972504138946533\n",
            "step 7615: loss = 2.562700033187866\n",
            "step 7616: loss = 2.6611976623535156\n",
            "step 7617: loss = 2.9347822666168213\n",
            "step 7618: loss = 2.6166725158691406\n",
            "step 7619: loss = 2.9040815830230713\n",
            "step 7620: loss = 2.860034942626953\n",
            "step 7621: loss = 2.6018505096435547\n",
            "step 7622: loss = 2.84395170211792\n",
            "step 7623: loss = 2.8146567344665527\n",
            "step 7624: loss = 2.588773488998413\n",
            "step 7625: loss = 2.8026795387268066\n",
            "step 7626: loss = 2.710383176803589\n",
            "step 7627: loss = 2.9743123054504395\n",
            "step 7628: loss = 2.499063491821289\n",
            "step 7629: loss = 2.5026063919067383\n",
            "step 7630: loss = 2.676088571548462\n",
            "step 7631: loss = 2.535883903503418\n",
            "step 7632: loss = 2.6850123405456543\n",
            "step 7633: loss = 2.4341890811920166\n",
            "step 7634: loss = 2.5103681087493896\n",
            "step 7635: loss = 2.5732338428497314\n",
            "step 7636: loss = 2.762890577316284\n",
            "step 7637: loss = 2.516166925430298\n",
            "step 7638: loss = 3.0476701259613037\n",
            "step 7639: loss = 2.940981149673462\n",
            "step 7640: loss = 2.573000431060791\n",
            "step 7641: loss = 2.726780652999878\n",
            "step 7642: loss = 2.535107374191284\n",
            "step 7643: loss = 2.6433050632476807\n",
            "step 7644: loss = 2.6137216091156006\n",
            "step 7645: loss = 2.55291748046875\n",
            "step 7646: loss = 2.9039981365203857\n",
            "step 7647: loss = 2.414787769317627\n",
            "step 7648: loss = 2.6453802585601807\n",
            "step 7649: loss = 2.8281867504119873\n",
            "step 7650: loss = 2.489497184753418\n",
            "step 7651: loss = 2.8167619705200195\n",
            "step 7652: loss = 2.651186466217041\n",
            "step 7653: loss = 2.394394636154175\n",
            "step 7654: loss = 2.8542888164520264\n",
            "step 7655: loss = 2.803349018096924\n",
            "step 7656: loss = 2.391387939453125\n",
            "step 7657: loss = 2.63783597946167\n",
            "step 7658: loss = 2.696641206741333\n",
            "step 7659: loss = 2.6001505851745605\n",
            "step 7660: loss = 2.5406601428985596\n",
            "step 7661: loss = 2.597543239593506\n",
            "step 7662: loss = 2.538370132446289\n",
            "step 7663: loss = 2.6706321239471436\n",
            "step 7664: loss = 2.684109926223755\n",
            "step 7665: loss = 2.493149995803833\n",
            "step 7666: loss = 2.9090540409088135\n",
            "step 7667: loss = 2.592257499694824\n",
            "step 7668: loss = 2.4727625846862793\n",
            "step 7669: loss = 2.741100311279297\n",
            "step 7670: loss = 2.828458547592163\n",
            "step 7671: loss = 2.6747677326202393\n",
            "step 7672: loss = 2.841806411743164\n",
            "step 7673: loss = 2.6563451290130615\n",
            "step 7674: loss = 2.814578056335449\n",
            "step 7675: loss = 2.7528796195983887\n",
            "step 7676: loss = 2.387542724609375\n",
            "step 7677: loss = 2.6629316806793213\n",
            "step 7678: loss = 2.5490787029266357\n",
            "step 7679: loss = 2.6789193153381348\n",
            "step 7680: loss = 2.3956639766693115\n",
            "step 7681: loss = 2.617924213409424\n",
            "step 7682: loss = 2.564155340194702\n",
            "step 7683: loss = 2.4749772548675537\n",
            "step 7684: loss = 2.6098294258117676\n",
            "step 7685: loss = 2.6011250019073486\n",
            "step 7686: loss = 2.6918587684631348\n",
            "step 7687: loss = 2.497314691543579\n",
            "step 7688: loss = 2.6821982860565186\n",
            "step 7689: loss = 2.482754707336426\n",
            "step 7690: loss = 2.28910231590271\n",
            "step 7691: loss = 2.694166660308838\n",
            "step 7692: loss = 2.5847370624542236\n",
            "step 7693: loss = 2.600924491882324\n",
            "step 7694: loss = 2.4241206645965576\n",
            "step 7695: loss = 2.7158167362213135\n",
            "step 7696: loss = 2.9923758506774902\n",
            "step 7697: loss = 2.5032622814178467\n",
            "step 7698: loss = 2.87017822265625\n",
            "step 7699: loss = 2.6362597942352295\n",
            "step 7700: loss = 2.7584946155548096\n",
            "step 7701: loss = 2.7901628017425537\n",
            "step 7702: loss = 2.702932834625244\n",
            "step 7703: loss = 2.5104849338531494\n",
            "step 7704: loss = 2.5233476161956787\n",
            "step 7705: loss = 2.642923593521118\n",
            "step 7706: loss = 2.598025321960449\n",
            "step 7707: loss = 2.7791895866394043\n",
            "step 7708: loss = 2.6165897846221924\n",
            "step 7709: loss = 2.744574785232544\n",
            "step 7710: loss = 2.423248767852783\n",
            "step 7711: loss = 2.5015454292297363\n",
            "step 7712: loss = 2.736314058303833\n",
            "step 7713: loss = 2.912264823913574\n",
            "step 7714: loss = 2.3090317249298096\n",
            "step 7715: loss = 2.3646860122680664\n",
            "step 7716: loss = 2.4138994216918945\n",
            "step 7717: loss = 2.5151302814483643\n",
            "step 7718: loss = 2.4607760906219482\n",
            "step 7719: loss = 2.510923147201538\n",
            "step 7720: loss = 2.385310649871826\n",
            "step 7721: loss = 2.8061110973358154\n",
            "step 7722: loss = 2.6711339950561523\n",
            "step 7723: loss = 2.634098768234253\n",
            "step 7724: loss = 2.5861053466796875\n",
            "step 7725: loss = 2.6331627368927\n",
            "step 7726: loss = 2.599699020385742\n",
            "step 7727: loss = 2.6049580574035645\n",
            "step 7728: loss = 2.407896041870117\n",
            "step 7729: loss = 2.2489311695098877\n",
            "step 7730: loss = 2.6288840770721436\n",
            "step 7731: loss = 3.0666415691375732\n",
            "step 7732: loss = 2.793179750442505\n",
            "step 7733: loss = 2.688652992248535\n",
            "step 7734: loss = 2.88470721244812\n",
            "step 7735: loss = 2.498677968978882\n",
            "step 7736: loss = 2.573169469833374\n",
            "step 7737: loss = 2.748919725418091\n",
            "step 7738: loss = 2.8028881549835205\n",
            "step 7739: loss = 2.787201404571533\n",
            "step 7740: loss = 2.3581581115722656\n",
            "step 7741: loss = 2.7887394428253174\n",
            "step 7742: loss = 2.601914167404175\n",
            "step 7743: loss = 2.5468966960906982\n",
            "step 7744: loss = 2.6631128787994385\n",
            "step 7745: loss = 2.5273258686065674\n",
            "step 7746: loss = 2.722628355026245\n",
            "step 7747: loss = 2.7325172424316406\n",
            "step 7748: loss = 2.3802835941314697\n",
            "step 7749: loss = 2.531632661819458\n",
            "step 7750: loss = 2.7528457641601562\n",
            "step 7751: loss = 2.628997325897217\n",
            "step 7752: loss = 2.610360860824585\n",
            "step 7753: loss = 2.756770372390747\n",
            "step 7754: loss = 2.645348310470581\n",
            "step 7755: loss = 2.779874086380005\n",
            "step 7756: loss = 2.764329433441162\n",
            "step 7757: loss = 2.7065412998199463\n",
            "step 7758: loss = 2.382112979888916\n",
            "step 7759: loss = 2.640191078186035\n",
            "step 7760: loss = 2.6131269931793213\n",
            "step 7761: loss = 2.5532314777374268\n",
            "step 7762: loss = 2.727969169616699\n",
            "step 7763: loss = 2.663724184036255\n",
            "step 7764: loss = 2.530881881713867\n",
            "step 7765: loss = 2.561194896697998\n",
            "step 7766: loss = 2.6899940967559814\n",
            "step 7767: loss = 2.9842617511749268\n",
            "step 7768: loss = 2.324319839477539\n",
            "step 7769: loss = 2.756992816925049\n",
            "step 7770: loss = 2.8952372074127197\n",
            "step 7771: loss = 2.4673662185668945\n",
            "step 7772: loss = 2.585385799407959\n",
            "step 7773: loss = 2.7990856170654297\n",
            "step 7774: loss = 2.620407819747925\n",
            "step 7775: loss = 2.6806423664093018\n",
            "step 7776: loss = 2.7829010486602783\n",
            "step 7777: loss = 2.674873113632202\n",
            "step 7778: loss = 2.6782782077789307\n",
            "step 7779: loss = 2.9092843532562256\n",
            "step 7780: loss = 2.791522741317749\n",
            "step 7781: loss = 2.6124777793884277\n",
            "step 7782: loss = 2.9052388668060303\n",
            "step 7783: loss = 2.608206272125244\n",
            "step 7784: loss = 2.813598155975342\n",
            "step 7785: loss = 2.504611015319824\n",
            "step 7786: loss = 2.721794843673706\n",
            "step 7787: loss = 2.7292065620422363\n",
            "step 7788: loss = 2.898848533630371\n",
            "step 7789: loss = 2.557617664337158\n",
            "step 7790: loss = 2.576451301574707\n",
            "step 7791: loss = 2.2824277877807617\n",
            "step 7792: loss = 2.6691067218780518\n",
            "step 7793: loss = 2.7968127727508545\n",
            "step 7794: loss = 2.822063684463501\n",
            "step 7795: loss = 2.6407768726348877\n",
            "step 7796: loss = 2.632338523864746\n",
            "step 7797: loss = 2.619049549102783\n",
            "step 7798: loss = 2.5976128578186035\n",
            "step 7799: loss = 2.576261281967163\n",
            "step 7800: loss = 2.5331978797912598\n",
            "step 7801: loss = 2.79953670501709\n",
            "step 7802: loss = 2.7004222869873047\n",
            "step 7803: loss = 2.5388343334198\n",
            "step 7804: loss = 2.7980213165283203\n",
            "step 7805: loss = 2.9152708053588867\n",
            "step 7806: loss = 2.657808780670166\n",
            "step 7807: loss = 2.5198888778686523\n",
            "step 7808: loss = 2.6772408485412598\n",
            "step 7809: loss = 2.9212749004364014\n",
            "step 7810: loss = 2.768867254257202\n",
            "Finish epoch 5\n",
            "New model saved, minimum loss: 2.6075070143967065 \n",
            "\n",
            "step 7811: loss = 2.2179198265075684\n",
            "step 7812: loss = 2.2755074501037598\n",
            "step 7813: loss = 2.2292251586914062\n",
            "step 7814: loss = 2.106559991836548\n",
            "step 7815: loss = 2.240432024002075\n",
            "step 7816: loss = 2.3697433471679688\n",
            "step 7817: loss = 2.02567982673645\n",
            "step 7818: loss = 2.2903339862823486\n",
            "step 7819: loss = 2.295063018798828\n",
            "step 7820: loss = 2.341244697570801\n",
            "step 7821: loss = 2.0489695072174072\n",
            "step 7822: loss = 2.2854504585266113\n",
            "step 7823: loss = 2.4843738079071045\n",
            "step 7824: loss = 2.2822344303131104\n",
            "step 7825: loss = 2.036412477493286\n",
            "step 7826: loss = 2.074897289276123\n",
            "step 7827: loss = 2.179300308227539\n",
            "step 7828: loss = 2.0522356033325195\n",
            "step 7829: loss = 2.3988537788391113\n",
            "step 7830: loss = 2.0593011379241943\n",
            "step 7831: loss = 1.8912791013717651\n",
            "step 7832: loss = 2.3571360111236572\n",
            "step 7833: loss = 1.9335620403289795\n",
            "step 7834: loss = 2.003870725631714\n",
            "step 7835: loss = 2.1568918228149414\n",
            "step 7836: loss = 2.223393440246582\n",
            "step 7837: loss = 2.5554726123809814\n",
            "step 7838: loss = 2.259934186935425\n",
            "step 7839: loss = 2.090257406234741\n",
            "step 7840: loss = 2.284717082977295\n",
            "step 7841: loss = 2.38773250579834\n",
            "step 7842: loss = 2.179624557495117\n",
            "step 7843: loss = 2.020899534225464\n",
            "step 7844: loss = 2.23384165763855\n",
            "step 7845: loss = 2.041011333465576\n",
            "step 7846: loss = 2.2483949661254883\n",
            "step 7847: loss = 2.2321321964263916\n",
            "step 7848: loss = 2.2117555141448975\n",
            "step 7849: loss = 2.017244815826416\n",
            "step 7850: loss = 2.200136423110962\n",
            "step 7851: loss = 2.093388319015503\n",
            "step 7852: loss = 2.354189157485962\n",
            "step 7853: loss = 2.210407018661499\n",
            "step 7854: loss = 2.184267997741699\n",
            "step 7855: loss = 2.191917657852173\n",
            "step 7856: loss = 2.082911729812622\n",
            "step 7857: loss = 2.0834672451019287\n",
            "step 7858: loss = 2.3867251873016357\n",
            "step 7859: loss = 2.1584291458129883\n",
            "step 7860: loss = 2.284008264541626\n",
            "step 7861: loss = 2.11506724357605\n",
            "step 7862: loss = 1.976406455039978\n",
            "step 7863: loss = 2.1442883014678955\n",
            "step 7864: loss = 2.5098955631256104\n",
            "step 7865: loss = 2.143327236175537\n",
            "step 7866: loss = 2.260157585144043\n",
            "step 7867: loss = 2.1696791648864746\n",
            "step 7868: loss = 2.217885732650757\n",
            "step 7869: loss = 1.9596703052520752\n",
            "step 7870: loss = 2.1813266277313232\n",
            "step 7871: loss = 2.0073254108428955\n",
            "step 7872: loss = 2.1475577354431152\n",
            "step 7873: loss = 2.120162010192871\n",
            "step 7874: loss = 2.1771748065948486\n",
            "step 7875: loss = 2.224327564239502\n",
            "step 7876: loss = 2.2620272636413574\n",
            "step 7877: loss = 2.43168306350708\n",
            "step 7878: loss = 2.1699538230895996\n",
            "step 7879: loss = 2.284119129180908\n",
            "step 7880: loss = 2.306485891342163\n",
            "step 7881: loss = 1.9953010082244873\n",
            "step 7882: loss = 1.9939022064208984\n",
            "step 7883: loss = 2.1337454319000244\n",
            "step 7884: loss = 2.344639539718628\n",
            "step 7885: loss = 2.132168769836426\n",
            "step 7886: loss = 2.2032289505004883\n",
            "step 7887: loss = 2.305840492248535\n",
            "step 7888: loss = 1.9788140058517456\n",
            "step 7889: loss = 2.138402223587036\n",
            "step 7890: loss = 1.8896980285644531\n",
            "step 7891: loss = 2.287961006164551\n",
            "step 7892: loss = 2.1112098693847656\n",
            "step 7893: loss = 2.166734457015991\n",
            "step 7894: loss = 2.3544764518737793\n",
            "step 7895: loss = 2.178313970565796\n",
            "step 7896: loss = 2.2040531635284424\n",
            "step 7897: loss = 2.1629300117492676\n",
            "step 7898: loss = 2.3709733486175537\n",
            "step 7899: loss = 2.251464366912842\n",
            "step 7900: loss = 2.305878162384033\n",
            "step 7901: loss = 2.1994471549987793\n",
            "step 7902: loss = 2.3846967220306396\n",
            "step 7903: loss = 2.2387166023254395\n",
            "step 7904: loss = 2.341597318649292\n",
            "step 7905: loss = 2.29854679107666\n",
            "step 7906: loss = 2.0568418502807617\n",
            "step 7907: loss = 2.139159917831421\n",
            "step 7908: loss = 2.4401986598968506\n",
            "step 7909: loss = 2.3177921772003174\n",
            "step 7910: loss = 2.0725135803222656\n",
            "step 7911: loss = 2.0941758155822754\n",
            "step 7912: loss = 2.087311029434204\n",
            "step 7913: loss = 2.0607118606567383\n",
            "step 7914: loss = 2.372267246246338\n",
            "step 7915: loss = 2.5065834522247314\n",
            "step 7916: loss = 2.075127124786377\n",
            "step 7917: loss = 2.061109781265259\n",
            "step 7918: loss = 2.1695995330810547\n",
            "step 7919: loss = 2.550682783126831\n",
            "step 7920: loss = 2.2045605182647705\n",
            "step 7921: loss = 2.0692527294158936\n",
            "step 7922: loss = 2.2782132625579834\n",
            "step 7923: loss = 2.288572072982788\n",
            "step 7924: loss = 2.259403944015503\n",
            "step 7925: loss = 2.25618314743042\n",
            "step 7926: loss = 2.399303674697876\n",
            "step 7927: loss = 2.186077833175659\n",
            "step 7928: loss = 2.451143980026245\n",
            "step 7929: loss = 2.1935813426971436\n",
            "step 7930: loss = 2.244271993637085\n",
            "step 7931: loss = 2.1933693885803223\n",
            "step 7932: loss = 2.089820623397827\n",
            "step 7933: loss = 2.148665189743042\n",
            "step 7934: loss = 2.4416863918304443\n",
            "step 7935: loss = 2.527968168258667\n",
            "step 7936: loss = 2.4914815425872803\n",
            "step 7937: loss = 2.476301908493042\n",
            "step 7938: loss = 2.1146042346954346\n",
            "step 7939: loss = 2.095463752746582\n",
            "step 7940: loss = 2.2940471172332764\n",
            "step 7941: loss = 2.1696255207061768\n",
            "step 7942: loss = 2.2046210765838623\n",
            "step 7943: loss = 2.3720591068267822\n",
            "step 7944: loss = 2.2088019847869873\n",
            "step 7945: loss = 2.1432266235351562\n",
            "step 7946: loss = 2.0098114013671875\n",
            "step 7947: loss = 2.2477900981903076\n",
            "step 7948: loss = 2.31300687789917\n",
            "step 7949: loss = 2.0723252296447754\n",
            "step 7950: loss = 2.2812042236328125\n",
            "step 7951: loss = 2.2631211280822754\n",
            "step 7952: loss = 2.2296290397644043\n",
            "step 7953: loss = 2.183259963989258\n",
            "step 7954: loss = 2.2147090435028076\n",
            "step 7955: loss = 2.4130215644836426\n",
            "step 7956: loss = 2.555223226547241\n",
            "step 7957: loss = 2.0543456077575684\n",
            "step 7958: loss = 2.203580856323242\n",
            "step 7959: loss = 2.329819440841675\n",
            "step 7960: loss = 2.0471584796905518\n",
            "step 7961: loss = 2.223843574523926\n",
            "step 7962: loss = 2.1781346797943115\n",
            "step 7963: loss = 2.2991724014282227\n",
            "step 7964: loss = 2.1160435676574707\n",
            "step 7965: loss = 2.015425205230713\n",
            "step 7966: loss = 1.9914618730545044\n",
            "step 7967: loss = 2.1655397415161133\n",
            "step 7968: loss = 2.334101915359497\n",
            "step 7969: loss = 2.108001232147217\n",
            "step 7970: loss = 2.4751360416412354\n",
            "step 7971: loss = 2.545734167098999\n",
            "step 7972: loss = 2.3217275142669678\n",
            "step 7973: loss = 2.398618459701538\n",
            "step 7974: loss = 2.354381561279297\n",
            "step 7975: loss = 2.1044206619262695\n",
            "step 7976: loss = 2.1711361408233643\n",
            "step 7977: loss = 2.2736546993255615\n",
            "step 7978: loss = 2.1044485569000244\n",
            "step 7979: loss = 2.121342182159424\n",
            "step 7980: loss = 2.276916980743408\n",
            "step 7981: loss = 2.213921070098877\n",
            "step 7982: loss = 2.1953823566436768\n",
            "step 7983: loss = 2.288001298904419\n",
            "step 7984: loss = 2.0933780670166016\n",
            "step 7985: loss = 2.23842453956604\n",
            "step 7986: loss = 2.1772348880767822\n",
            "step 7987: loss = 2.496295928955078\n",
            "step 7988: loss = 2.139235496520996\n",
            "step 7989: loss = 2.1766223907470703\n",
            "step 7990: loss = 2.1197738647460938\n",
            "step 7991: loss = 2.004633903503418\n",
            "step 7992: loss = 2.0514028072357178\n",
            "step 7993: loss = 2.1088614463806152\n",
            "step 7994: loss = 2.4415442943573\n",
            "step 7995: loss = 2.434174060821533\n",
            "step 7996: loss = 2.0832810401916504\n",
            "step 7997: loss = 2.3031582832336426\n",
            "step 7998: loss = 2.1379127502441406\n",
            "step 7999: loss = 2.254829168319702\n",
            "step 8000: loss = 2.164893865585327\n",
            "step 8001: loss = 2.4044978618621826\n",
            "step 8002: loss = 2.3451457023620605\n",
            "step 8003: loss = 2.174349546432495\n",
            "step 8004: loss = 2.3692615032196045\n",
            "step 8005: loss = 2.23685884475708\n",
            "step 8006: loss = 2.135298252105713\n",
            "step 8007: loss = 2.3948919773101807\n",
            "step 8008: loss = 2.249501943588257\n",
            "step 8009: loss = 2.145552635192871\n",
            "step 8010: loss = 2.4687702655792236\n",
            "step 8011: loss = 2.137432336807251\n",
            "step 8012: loss = 2.489847183227539\n",
            "step 8013: loss = 2.1732981204986572\n",
            "step 8014: loss = 2.3835980892181396\n",
            "step 8015: loss = 2.416576623916626\n",
            "step 8016: loss = 1.8883112668991089\n",
            "step 8017: loss = 2.5908989906311035\n",
            "step 8018: loss = 2.1618690490722656\n",
            "step 8019: loss = 2.457841396331787\n",
            "step 8020: loss = 2.291099786758423\n",
            "step 8021: loss = 2.2194015979766846\n",
            "step 8022: loss = 2.1267759799957275\n",
            "step 8023: loss = 2.419508457183838\n",
            "step 8024: loss = 2.201174020767212\n",
            "step 8025: loss = 2.194689989089966\n",
            "step 8026: loss = 2.2547926902770996\n",
            "step 8027: loss = 2.112121343612671\n",
            "step 8028: loss = 2.2672269344329834\n",
            "step 8029: loss = 2.4196937084198\n",
            "step 8030: loss = 2.3682796955108643\n",
            "step 8031: loss = 2.525754690170288\n",
            "step 8032: loss = 2.3556935787200928\n",
            "step 8033: loss = 2.084604024887085\n",
            "step 8034: loss = 2.2844104766845703\n",
            "step 8035: loss = 2.243657112121582\n",
            "step 8036: loss = 2.1660044193267822\n",
            "step 8037: loss = 2.3584372997283936\n",
            "step 8038: loss = 2.4022819995880127\n",
            "step 8039: loss = 2.177105665206909\n",
            "step 8040: loss = 2.2439606189727783\n",
            "step 8041: loss = 2.0440475940704346\n",
            "step 8042: loss = 2.278082847595215\n",
            "step 8043: loss = 2.1139936447143555\n",
            "step 8044: loss = 2.069666624069214\n",
            "step 8045: loss = 2.0927860736846924\n",
            "step 8046: loss = 2.4116294384002686\n",
            "step 8047: loss = 2.350632667541504\n",
            "step 8048: loss = 2.122645616531372\n",
            "step 8049: loss = 2.215226650238037\n",
            "step 8050: loss = 2.149824380874634\n",
            "step 8051: loss = 2.207404851913452\n",
            "step 8052: loss = 2.0675177574157715\n",
            "step 8053: loss = 2.209876775741577\n",
            "step 8054: loss = 2.332615375518799\n",
            "step 8055: loss = 2.362760543823242\n",
            "step 8056: loss = 2.1510579586029053\n",
            "step 8057: loss = 2.297452449798584\n",
            "step 8058: loss = 1.9274263381958008\n",
            "step 8059: loss = 2.547102928161621\n",
            "step 8060: loss = 2.557691812515259\n",
            "step 8061: loss = 2.227944850921631\n",
            "step 8062: loss = 2.4687235355377197\n",
            "step 8063: loss = 2.2457354068756104\n",
            "step 8064: loss = 2.1724374294281006\n",
            "step 8065: loss = 2.2132275104522705\n",
            "step 8066: loss = 2.220749855041504\n",
            "step 8067: loss = 2.0972981452941895\n",
            "step 8068: loss = 2.0497512817382812\n",
            "step 8069: loss = 2.2600691318511963\n",
            "step 8070: loss = 2.324669361114502\n",
            "step 8071: loss = 2.339547872543335\n",
            "step 8072: loss = 2.268829345703125\n",
            "step 8073: loss = 2.221801996231079\n",
            "step 8074: loss = 2.214895486831665\n",
            "step 8075: loss = 2.3319923877716064\n",
            "step 8076: loss = 2.2812774181365967\n",
            "step 8077: loss = 2.2669193744659424\n",
            "step 8078: loss = 1.9827948808670044\n",
            "step 8079: loss = 2.4057819843292236\n",
            "step 8080: loss = 2.1802260875701904\n",
            "step 8081: loss = 2.3966143131256104\n",
            "step 8082: loss = 2.2228047847747803\n",
            "step 8083: loss = 2.3078293800354004\n",
            "step 8084: loss = 2.283207416534424\n",
            "step 8085: loss = 2.327397346496582\n",
            "step 8086: loss = 2.3625805377960205\n",
            "step 8087: loss = 2.271923780441284\n",
            "step 8088: loss = 2.5210788249969482\n",
            "step 8089: loss = 2.289456605911255\n",
            "step 8090: loss = 2.299443006515503\n",
            "step 8091: loss = 2.1906521320343018\n",
            "step 8092: loss = 2.5924296379089355\n",
            "step 8093: loss = 2.313843250274658\n",
            "step 8094: loss = 2.155137538909912\n",
            "step 8095: loss = 2.2979238033294678\n",
            "step 8096: loss = 2.1994144916534424\n",
            "step 8097: loss = 2.2107911109924316\n",
            "step 8098: loss = 2.0789294242858887\n",
            "step 8099: loss = 2.2140114307403564\n",
            "step 8100: loss = 2.3660941123962402\n",
            "step 8101: loss = 2.562748432159424\n",
            "step 8102: loss = 2.4265384674072266\n",
            "step 8103: loss = 2.146721601486206\n",
            "step 8104: loss = 2.367431163787842\n",
            "step 8105: loss = 2.1054792404174805\n",
            "step 8106: loss = 2.201523542404175\n",
            "step 8107: loss = 2.265641689300537\n",
            "step 8108: loss = 2.3505430221557617\n",
            "step 8109: loss = 2.2288527488708496\n",
            "step 8110: loss = 2.2139639854431152\n",
            "step 8111: loss = 2.353713035583496\n",
            "step 8112: loss = 2.4132814407348633\n",
            "step 8113: loss = 2.483682155609131\n",
            "step 8114: loss = 2.545071840286255\n",
            "step 8115: loss = 2.052647829055786\n",
            "step 8116: loss = 2.547725200653076\n",
            "step 8117: loss = 2.232192277908325\n",
            "step 8118: loss = 2.4083518981933594\n",
            "step 8119: loss = 2.172499895095825\n",
            "step 8120: loss = 2.1970338821411133\n",
            "step 8121: loss = 2.2046308517456055\n",
            "step 8122: loss = 2.2010338306427\n",
            "step 8123: loss = 2.046936511993408\n",
            "step 8124: loss = 2.2878944873809814\n",
            "step 8125: loss = 2.197958469390869\n",
            "step 8126: loss = 2.170275926589966\n",
            "step 8127: loss = 2.247833728790283\n",
            "step 8128: loss = 2.0960474014282227\n",
            "step 8129: loss = 2.1166224479675293\n",
            "step 8130: loss = 2.414029836654663\n",
            "step 8131: loss = 2.3008651733398438\n",
            "step 8132: loss = 2.3507847785949707\n",
            "step 8133: loss = 2.4264540672302246\n",
            "step 8134: loss = 2.3951053619384766\n",
            "step 8135: loss = 2.453094959259033\n",
            "step 8136: loss = 2.486304521560669\n",
            "step 8137: loss = 2.2943921089172363\n",
            "step 8138: loss = 2.211310863494873\n",
            "step 8139: loss = 2.08732533454895\n",
            "step 8140: loss = 2.1652750968933105\n",
            "step 8141: loss = 2.418073892593384\n",
            "step 8142: loss = 2.2344980239868164\n",
            "step 8143: loss = 2.230065107345581\n",
            "step 8144: loss = 2.221791982650757\n",
            "step 8145: loss = 2.177734613418579\n",
            "step 8146: loss = 2.4117021560668945\n",
            "step 8147: loss = 2.3905277252197266\n",
            "step 8148: loss = 2.4046483039855957\n",
            "step 8149: loss = 2.180426597595215\n",
            "step 8150: loss = 2.1731598377227783\n",
            "step 8151: loss = 2.1668272018432617\n",
            "step 8152: loss = 2.2187867164611816\n",
            "step 8153: loss = 2.4805500507354736\n",
            "step 8154: loss = 2.1163766384124756\n",
            "step 8155: loss = 2.168060302734375\n",
            "step 8156: loss = 2.1488702297210693\n",
            "step 8157: loss = 2.2428481578826904\n",
            "step 8158: loss = 2.2647924423217773\n",
            "step 8159: loss = 2.360875368118286\n",
            "step 8160: loss = 2.2913825511932373\n",
            "step 8161: loss = 2.2937076091766357\n",
            "step 8162: loss = 2.4046294689178467\n",
            "step 8163: loss = 2.281205415725708\n",
            "step 8164: loss = 2.3161544799804688\n",
            "step 8165: loss = 2.2055909633636475\n",
            "step 8166: loss = 2.3462295532226562\n",
            "step 8167: loss = 2.4907617568969727\n",
            "step 8168: loss = 2.439984083175659\n",
            "step 8169: loss = 2.151338577270508\n",
            "step 8170: loss = 2.117973804473877\n",
            "step 8171: loss = 2.485369920730591\n",
            "step 8172: loss = 2.208228349685669\n",
            "step 8173: loss = 2.3966004848480225\n",
            "step 8174: loss = 2.2070953845977783\n",
            "step 8175: loss = 2.4061286449432373\n",
            "step 8176: loss = 2.289058208465576\n",
            "step 8177: loss = 2.4669432640075684\n",
            "step 8178: loss = 2.1587986946105957\n",
            "step 8179: loss = 2.5016732215881348\n",
            "step 8180: loss = 2.417212963104248\n",
            "step 8181: loss = 2.3909215927124023\n",
            "step 8182: loss = 2.3126111030578613\n",
            "step 8183: loss = 2.260143995285034\n",
            "step 8184: loss = 2.204166889190674\n",
            "step 8185: loss = 2.425525426864624\n",
            "step 8186: loss = 2.3109843730926514\n",
            "step 8187: loss = 2.289003610610962\n",
            "step 8188: loss = 2.623908042907715\n",
            "step 8189: loss = 2.2776434421539307\n",
            "step 8190: loss = 2.435073137283325\n",
            "step 8191: loss = 2.3354785442352295\n",
            "step 8192: loss = 2.3632147312164307\n",
            "step 8193: loss = 2.220998764038086\n",
            "step 8194: loss = 2.0173075199127197\n",
            "step 8195: loss = 2.1782987117767334\n",
            "step 8196: loss = 2.482999563217163\n",
            "step 8197: loss = 2.4252538681030273\n",
            "step 8198: loss = 2.375849723815918\n",
            "step 8199: loss = 2.1030075550079346\n",
            "step 8200: loss = 2.388005256652832\n",
            "step 8201: loss = 2.1836156845092773\n",
            "step 8202: loss = 2.535379409790039\n",
            "step 8203: loss = 2.4552769660949707\n",
            "step 8204: loss = 2.4668397903442383\n",
            "step 8205: loss = 2.2116432189941406\n",
            "step 8206: loss = 2.286067008972168\n",
            "step 8207: loss = 2.183837413787842\n",
            "step 8208: loss = 2.4234228134155273\n",
            "step 8209: loss = 2.415792942047119\n",
            "step 8210: loss = 2.49658203125\n",
            "step 8211: loss = 2.2648682594299316\n",
            "step 8212: loss = 2.279794692993164\n",
            "step 8213: loss = 2.1763832569122314\n",
            "step 8214: loss = 2.5161330699920654\n",
            "step 8215: loss = 2.2007110118865967\n",
            "step 8216: loss = 2.221175193786621\n",
            "step 8217: loss = 2.115534782409668\n",
            "step 8218: loss = 2.076084613800049\n",
            "step 8219: loss = 2.4748525619506836\n",
            "step 8220: loss = 2.3463642597198486\n",
            "step 8221: loss = 2.543159246444702\n",
            "step 8222: loss = 2.4476704597473145\n",
            "step 8223: loss = 2.685673236846924\n",
            "step 8224: loss = 2.372481346130371\n",
            "step 8225: loss = 2.2166316509246826\n",
            "step 8226: loss = 2.0868282318115234\n",
            "step 8227: loss = 2.2383270263671875\n",
            "step 8228: loss = 1.9953649044036865\n",
            "step 8229: loss = 2.461514472961426\n",
            "step 8230: loss = 2.5672850608825684\n",
            "step 8231: loss = 2.578181266784668\n",
            "step 8232: loss = 2.1680586338043213\n",
            "step 8233: loss = 2.2744009494781494\n",
            "step 8234: loss = 2.3841934204101562\n",
            "step 8235: loss = 2.4982972145080566\n",
            "step 8236: loss = 2.155870199203491\n",
            "step 8237: loss = 2.337031602859497\n",
            "step 8238: loss = 2.274329900741577\n",
            "step 8239: loss = 2.485621929168701\n",
            "step 8240: loss = 2.358762502670288\n",
            "step 8241: loss = 2.6715681552886963\n",
            "step 8242: loss = 2.145939826965332\n",
            "step 8243: loss = 2.6798319816589355\n",
            "step 8244: loss = 1.9767391681671143\n",
            "step 8245: loss = 2.2489843368530273\n",
            "step 8246: loss = 2.6043968200683594\n",
            "step 8247: loss = 2.2741055488586426\n",
            "step 8248: loss = 2.2679786682128906\n",
            "step 8249: loss = 2.378507375717163\n",
            "step 8250: loss = 2.408191204071045\n",
            "step 8251: loss = 2.22222900390625\n",
            "step 8252: loss = 2.3033607006073\n",
            "step 8253: loss = 2.507131814956665\n",
            "step 8254: loss = 2.315593957901001\n",
            "step 8255: loss = 2.5439486503601074\n",
            "step 8256: loss = 2.482105016708374\n",
            "step 8257: loss = 2.1809957027435303\n",
            "step 8258: loss = 2.193974494934082\n",
            "step 8259: loss = 2.5158915519714355\n",
            "step 8260: loss = 2.1745903491973877\n",
            "step 8261: loss = 2.4470279216766357\n",
            "step 8262: loss = 2.5625576972961426\n",
            "step 8263: loss = 2.2410268783569336\n",
            "step 8264: loss = 2.439450740814209\n",
            "step 8265: loss = 2.265005350112915\n",
            "step 8266: loss = 2.2280731201171875\n",
            "step 8267: loss = 2.383399486541748\n",
            "step 8268: loss = 2.064318895339966\n",
            "step 8269: loss = 2.525050401687622\n",
            "step 8270: loss = 2.442901611328125\n",
            "step 8271: loss = 2.2569992542266846\n",
            "step 8272: loss = 2.4204165935516357\n",
            "step 8273: loss = 2.165314197540283\n",
            "step 8274: loss = 2.18341064453125\n",
            "step 8275: loss = 2.144258975982666\n",
            "step 8276: loss = 2.1661648750305176\n",
            "step 8277: loss = 2.386523962020874\n",
            "step 8278: loss = 2.361706495285034\n",
            "step 8279: loss = 2.2935450077056885\n",
            "step 8280: loss = 2.0136380195617676\n",
            "step 8281: loss = 2.4032106399536133\n",
            "step 8282: loss = 2.3023157119750977\n",
            "step 8283: loss = 2.258307456970215\n",
            "step 8284: loss = 2.3930346965789795\n",
            "step 8285: loss = 2.24076771736145\n",
            "step 8286: loss = 2.135586738586426\n",
            "step 8287: loss = 2.202817916870117\n",
            "step 8288: loss = 2.3248040676116943\n",
            "step 8289: loss = 2.3633668422698975\n",
            "step 8290: loss = 2.2745585441589355\n",
            "step 8291: loss = 2.3167619705200195\n",
            "step 8292: loss = 2.1792497634887695\n",
            "step 8293: loss = 2.298248767852783\n",
            "step 8294: loss = 2.281794786453247\n",
            "step 8295: loss = 2.4514551162719727\n",
            "step 8296: loss = 2.0641708374023438\n",
            "step 8297: loss = 2.1413841247558594\n",
            "step 8298: loss = 2.498201847076416\n",
            "step 8299: loss = 2.2913379669189453\n",
            "step 8300: loss = 2.1907830238342285\n",
            "step 8301: loss = 2.2861738204956055\n",
            "step 8302: loss = 2.3118183612823486\n",
            "step 8303: loss = 2.388235092163086\n",
            "step 8304: loss = 2.637305498123169\n",
            "step 8305: loss = 2.2080976963043213\n",
            "step 8306: loss = 2.3107097148895264\n",
            "step 8307: loss = 2.1183502674102783\n",
            "step 8308: loss = 2.173776626586914\n",
            "step 8309: loss = 2.585684299468994\n",
            "step 8310: loss = 2.6513705253601074\n",
            "step 8311: loss = 2.2065844535827637\n",
            "step 8312: loss = 2.4257946014404297\n",
            "step 8313: loss = 2.3734066486358643\n",
            "step 8314: loss = 2.3700764179229736\n",
            "step 8315: loss = 2.291316509246826\n",
            "step 8316: loss = 2.371260404586792\n",
            "step 8317: loss = 2.337719678878784\n",
            "step 8318: loss = 2.3977532386779785\n",
            "step 8319: loss = 2.2272772789001465\n",
            "step 8320: loss = 2.2466373443603516\n",
            "step 8321: loss = 2.451082229614258\n",
            "step 8322: loss = 2.1569859981536865\n",
            "step 8323: loss = 2.3740477561950684\n",
            "step 8324: loss = 2.316270351409912\n",
            "step 8325: loss = 2.550543785095215\n",
            "step 8326: loss = 2.779857873916626\n",
            "step 8327: loss = 2.4052252769470215\n",
            "step 8328: loss = 2.352149248123169\n",
            "step 8329: loss = 2.389737606048584\n",
            "step 8330: loss = 2.2646126747131348\n",
            "step 8331: loss = 2.112095355987549\n",
            "step 8332: loss = 2.2609353065490723\n",
            "step 8333: loss = 2.30177640914917\n",
            "step 8334: loss = 2.248110055923462\n",
            "step 8335: loss = 2.144533395767212\n",
            "step 8336: loss = 2.50514554977417\n",
            "step 8337: loss = 2.4313535690307617\n",
            "step 8338: loss = 2.2336037158966064\n",
            "step 8339: loss = 2.2022759914398193\n",
            "step 8340: loss = 2.1448142528533936\n",
            "step 8341: loss = 2.443002223968506\n",
            "step 8342: loss = 2.241450071334839\n",
            "step 8343: loss = 2.253710985183716\n",
            "step 8344: loss = 2.292182207107544\n",
            "step 8345: loss = 2.1646370887756348\n",
            "step 8346: loss = 2.2834198474884033\n",
            "step 8347: loss = 2.5972187519073486\n",
            "step 8348: loss = 2.256998300552368\n",
            "step 8349: loss = 2.205857753753662\n",
            "step 8350: loss = 2.352055311203003\n",
            "step 8351: loss = 2.45139479637146\n",
            "step 8352: loss = 2.3884787559509277\n",
            "step 8353: loss = 2.524897575378418\n",
            "step 8354: loss = 2.3295743465423584\n",
            "step 8355: loss = 2.3886520862579346\n",
            "step 8356: loss = 2.2708864212036133\n",
            "step 8357: loss = 2.414154291152954\n",
            "step 8358: loss = 2.284071207046509\n",
            "step 8359: loss = 2.4195826053619385\n",
            "step 8360: loss = 2.481700897216797\n",
            "step 8361: loss = 2.2247824668884277\n",
            "step 8362: loss = 2.4034488201141357\n",
            "step 8363: loss = 2.1897356510162354\n",
            "step 8364: loss = 2.108426332473755\n",
            "step 8365: loss = 2.1568593978881836\n",
            "step 8366: loss = 2.3956825733184814\n",
            "step 8367: loss = 2.3099231719970703\n",
            "step 8368: loss = 2.2468864917755127\n",
            "step 8369: loss = 2.399291753768921\n",
            "step 8370: loss = 2.2449851036071777\n",
            "step 8371: loss = 2.346931219100952\n",
            "step 8372: loss = 2.581920623779297\n",
            "step 8373: loss = 2.5353424549102783\n",
            "step 8374: loss = 2.2200727462768555\n",
            "step 8375: loss = 2.401458263397217\n",
            "step 8376: loss = 2.2795190811157227\n",
            "step 8377: loss = 2.3217945098876953\n",
            "step 8378: loss = 2.131657600402832\n",
            "step 8379: loss = 2.4830257892608643\n",
            "step 8380: loss = 2.22895884513855\n",
            "step 8381: loss = 2.4091007709503174\n",
            "step 8382: loss = 2.031787872314453\n",
            "step 8383: loss = 2.279834032058716\n",
            "step 8384: loss = 2.4867303371429443\n",
            "step 8385: loss = 2.288438320159912\n",
            "step 8386: loss = 2.4026777744293213\n",
            "step 8387: loss = 2.4396543502807617\n",
            "step 8388: loss = 2.5408666133880615\n",
            "step 8389: loss = 2.425895929336548\n",
            "step 8390: loss = 2.324601173400879\n",
            "step 8391: loss = 2.271674633026123\n",
            "step 8392: loss = 2.322911500930786\n",
            "step 8393: loss = 2.363398790359497\n",
            "step 8394: loss = 2.5256459712982178\n",
            "step 8395: loss = 2.6367483139038086\n",
            "step 8396: loss = 2.3670918941497803\n",
            "step 8397: loss = 2.3147661685943604\n",
            "step 8398: loss = 2.344724178314209\n",
            "step 8399: loss = 2.372157573699951\n",
            "step 8400: loss = 2.635305643081665\n",
            "step 8401: loss = 2.5028491020202637\n",
            "step 8402: loss = 2.294853687286377\n",
            "step 8403: loss = 2.5633292198181152\n",
            "step 8404: loss = 2.256086587905884\n",
            "step 8405: loss = 2.6161234378814697\n",
            "step 8406: loss = 2.4747893810272217\n",
            "step 8407: loss = 2.4722628593444824\n",
            "step 8408: loss = 2.040506362915039\n",
            "step 8409: loss = 2.087609052658081\n",
            "step 8410: loss = 2.4312353134155273\n",
            "step 8411: loss = 2.3144071102142334\n",
            "step 8412: loss = 2.4052114486694336\n",
            "step 8413: loss = 2.23516583442688\n",
            "step 8414: loss = 2.3296051025390625\n",
            "step 8415: loss = 2.720442056655884\n",
            "step 8416: loss = 2.2417478561401367\n",
            "step 8417: loss = 2.263209342956543\n",
            "step 8418: loss = 2.741133689880371\n",
            "step 8419: loss = 2.3995611667633057\n",
            "step 8420: loss = 2.2311809062957764\n",
            "step 8421: loss = 2.312011957168579\n",
            "step 8422: loss = 2.663249969482422\n",
            "step 8423: loss = 2.376857042312622\n",
            "step 8424: loss = 2.3166186809539795\n",
            "step 8425: loss = 2.2454147338867188\n",
            "step 8426: loss = 2.3561341762542725\n",
            "step 8427: loss = 2.4548139572143555\n",
            "step 8428: loss = 2.593818187713623\n",
            "step 8429: loss = 2.445216655731201\n",
            "step 8430: loss = 2.5451996326446533\n",
            "step 8431: loss = 2.1128041744232178\n",
            "step 8432: loss = 2.5062553882598877\n",
            "step 8433: loss = 2.5574278831481934\n",
            "step 8434: loss = 2.294830083847046\n",
            "step 8435: loss = 2.4314169883728027\n",
            "step 8436: loss = 2.226435899734497\n",
            "step 8437: loss = 2.508573055267334\n",
            "step 8438: loss = 2.420560598373413\n",
            "step 8439: loss = 2.47470760345459\n",
            "step 8440: loss = 2.217655897140503\n",
            "step 8441: loss = 2.1766254901885986\n",
            "step 8442: loss = 2.381734848022461\n",
            "step 8443: loss = 2.6410012245178223\n",
            "step 8444: loss = 2.272461414337158\n",
            "step 8445: loss = 2.148109197616577\n",
            "step 8446: loss = 2.1898720264434814\n",
            "step 8447: loss = 2.4709033966064453\n",
            "step 8448: loss = 2.194552421569824\n",
            "step 8449: loss = 2.194772720336914\n",
            "step 8450: loss = 2.3958046436309814\n",
            "step 8451: loss = 2.3221843242645264\n",
            "step 8452: loss = 2.446387529373169\n",
            "step 8453: loss = 2.267791509628296\n",
            "step 8454: loss = 2.1017768383026123\n",
            "step 8455: loss = 2.0877559185028076\n",
            "step 8456: loss = 2.403827667236328\n",
            "step 8457: loss = 2.220104217529297\n",
            "step 8458: loss = 2.2061445713043213\n",
            "step 8459: loss = 2.3432395458221436\n",
            "step 8460: loss = 2.2483396530151367\n",
            "step 8461: loss = 2.28849720954895\n",
            "step 8462: loss = 2.5090675354003906\n",
            "step 8463: loss = 2.431236743927002\n",
            "step 8464: loss = 2.4463517665863037\n",
            "step 8465: loss = 2.6425435543060303\n",
            "step 8466: loss = 2.4391379356384277\n",
            "step 8467: loss = 2.334213972091675\n",
            "step 8468: loss = 2.3469371795654297\n",
            "step 8469: loss = 2.6084439754486084\n",
            "step 8470: loss = 2.3749754428863525\n",
            "step 8471: loss = 2.3623170852661133\n",
            "step 8472: loss = 2.5914416313171387\n",
            "step 8473: loss = 2.6510727405548096\n",
            "step 8474: loss = 2.1531057357788086\n",
            "step 8475: loss = 1.9277154207229614\n",
            "step 8476: loss = 2.3205759525299072\n",
            "step 8477: loss = 2.3583357334136963\n",
            "step 8478: loss = 2.243750810623169\n",
            "step 8479: loss = 2.4912874698638916\n",
            "step 8480: loss = 2.2265758514404297\n",
            "step 8481: loss = 2.1815967559814453\n",
            "step 8482: loss = 2.323129415512085\n",
            "step 8483: loss = 2.4652233123779297\n",
            "step 8484: loss = 2.271636962890625\n",
            "step 8485: loss = 2.3923373222351074\n",
            "step 8486: loss = 2.0682644844055176\n",
            "step 8487: loss = 2.3956754207611084\n",
            "step 8488: loss = 2.2755017280578613\n",
            "step 8489: loss = 2.2268807888031006\n",
            "step 8490: loss = 2.609745979309082\n",
            "step 8491: loss = 2.348938465118408\n",
            "step 8492: loss = 2.5251107215881348\n",
            "step 8493: loss = 2.3003153800964355\n",
            "step 8494: loss = 2.1450753211975098\n",
            "step 8495: loss = 2.3704118728637695\n",
            "step 8496: loss = 2.605933904647827\n",
            "step 8497: loss = 2.0011610984802246\n",
            "step 8498: loss = 2.223008871078491\n",
            "step 8499: loss = 2.264094352722168\n",
            "step 8500: loss = 2.275296926498413\n",
            "step 8501: loss = 2.3931474685668945\n",
            "step 8502: loss = 2.1875205039978027\n",
            "step 8503: loss = 2.1499216556549072\n",
            "step 8504: loss = 2.359971523284912\n",
            "step 8505: loss = 2.2856552600860596\n",
            "step 8506: loss = 2.4035747051239014\n",
            "step 8507: loss = 2.3835835456848145\n",
            "step 8508: loss = 2.3812804222106934\n",
            "step 8509: loss = 2.439335346221924\n",
            "step 8510: loss = 2.534601926803589\n",
            "step 8511: loss = 2.4737746715545654\n",
            "step 8512: loss = 2.3333680629730225\n",
            "step 8513: loss = 2.3067879676818848\n",
            "step 8514: loss = 2.627295732498169\n",
            "step 8515: loss = 2.0246052742004395\n",
            "step 8516: loss = 2.2054107189178467\n",
            "step 8517: loss = 2.211230754852295\n",
            "step 8518: loss = 2.2593226432800293\n",
            "step 8519: loss = 2.3209452629089355\n",
            "step 8520: loss = 2.4473814964294434\n",
            "step 8521: loss = 2.549358606338501\n",
            "step 8522: loss = 2.4961485862731934\n",
            "step 8523: loss = 2.3598268032073975\n",
            "step 8524: loss = 2.096742868423462\n",
            "step 8525: loss = 2.143531084060669\n",
            "step 8526: loss = 2.330803871154785\n",
            "step 8527: loss = 2.2467894554138184\n",
            "step 8528: loss = 2.293327808380127\n",
            "step 8529: loss = 2.277235269546509\n",
            "step 8530: loss = 2.262272596359253\n",
            "step 8531: loss = 2.106444835662842\n",
            "step 8532: loss = 2.454190969467163\n",
            "step 8533: loss = 2.732804775238037\n",
            "step 8534: loss = 2.619296073913574\n",
            "step 8535: loss = 2.3419337272644043\n",
            "step 8536: loss = 2.395967483520508\n",
            "step 8537: loss = 2.429572582244873\n",
            "step 8538: loss = 2.5859739780426025\n",
            "step 8539: loss = 2.361847162246704\n",
            "step 8540: loss = 2.571885347366333\n",
            "step 8541: loss = 2.3899333477020264\n",
            "step 8542: loss = 2.3092799186706543\n",
            "step 8543: loss = 2.059238910675049\n",
            "step 8544: loss = 2.2850148677825928\n",
            "step 8545: loss = 2.435004949569702\n",
            "step 8546: loss = 2.2990365028381348\n",
            "step 8547: loss = 2.439185619354248\n",
            "step 8548: loss = 2.474046468734741\n",
            "step 8549: loss = 2.4088973999023438\n",
            "step 8550: loss = 2.3217294216156006\n",
            "step 8551: loss = 2.3279013633728027\n",
            "step 8552: loss = 2.290064573287964\n",
            "step 8553: loss = 2.544954538345337\n",
            "step 8554: loss = 2.133249044418335\n",
            "step 8555: loss = 2.2502269744873047\n",
            "step 8556: loss = 2.2502665519714355\n",
            "step 8557: loss = 2.479116678237915\n",
            "step 8558: loss = 2.119795322418213\n",
            "step 8559: loss = 2.5143518447875977\n",
            "step 8560: loss = 2.4786102771759033\n",
            "step 8561: loss = 2.359494209289551\n",
            "step 8562: loss = 2.59700608253479\n",
            "step 8563: loss = 2.3832499980926514\n",
            "step 8564: loss = 2.4188344478607178\n",
            "step 8565: loss = 2.174835681915283\n",
            "step 8566: loss = 2.248443841934204\n",
            "step 8567: loss = 2.3292088508605957\n",
            "step 8568: loss = 2.3339767456054688\n",
            "step 8569: loss = 2.2170372009277344\n",
            "step 8570: loss = 2.49851393699646\n",
            "step 8571: loss = 2.1039395332336426\n",
            "step 8572: loss = 2.4342193603515625\n",
            "step 8573: loss = 2.364642381668091\n",
            "step 8574: loss = 2.270707607269287\n",
            "step 8575: loss = 2.160179376602173\n",
            "step 8576: loss = 2.301114320755005\n",
            "step 8577: loss = 2.4831185340881348\n",
            "step 8578: loss = 2.160022497177124\n",
            "step 8579: loss = 2.6014347076416016\n",
            "step 8580: loss = 2.1187498569488525\n",
            "step 8581: loss = 2.2346386909484863\n",
            "step 8582: loss = 2.5214452743530273\n",
            "step 8583: loss = 2.490030288696289\n",
            "step 8584: loss = 2.4780995845794678\n",
            "step 8585: loss = 2.4635863304138184\n",
            "step 8586: loss = 2.429276704788208\n",
            "step 8587: loss = 2.172283411026001\n",
            "step 8588: loss = 2.5421435832977295\n",
            "step 8589: loss = 2.36979603767395\n",
            "step 8590: loss = 2.4250996112823486\n",
            "step 8591: loss = 2.3336899280548096\n",
            "step 8592: loss = 2.3767662048339844\n",
            "step 8593: loss = 2.1319100856781006\n",
            "step 8594: loss = 2.4925825595855713\n",
            "step 8595: loss = 2.303673505783081\n",
            "step 8596: loss = 2.4438884258270264\n",
            "step 8597: loss = 2.4079365730285645\n",
            "step 8598: loss = 2.3027865886688232\n",
            "step 8599: loss = 2.2583017349243164\n",
            "step 8600: loss = 2.4294028282165527\n",
            "step 8601: loss = 2.377037286758423\n",
            "step 8602: loss = 2.4256045818328857\n",
            "step 8603: loss = 2.462076187133789\n",
            "step 8604: loss = 1.9880647659301758\n",
            "step 8605: loss = 2.3283884525299072\n",
            "step 8606: loss = 2.2301430702209473\n",
            "step 8607: loss = 2.477910041809082\n",
            "step 8608: loss = 2.5859732627868652\n",
            "step 8609: loss = 2.1595468521118164\n",
            "step 8610: loss = 2.2668769359588623\n",
            "step 8611: loss = 2.463974714279175\n",
            "step 8612: loss = 2.3741307258605957\n",
            "step 8613: loss = 2.249413251876831\n",
            "step 8614: loss = 2.1903553009033203\n",
            "step 8615: loss = 2.29144549369812\n",
            "step 8616: loss = 2.3711674213409424\n",
            "step 8617: loss = 2.192647933959961\n",
            "step 8618: loss = 2.1309874057769775\n",
            "step 8619: loss = 2.5057520866394043\n",
            "step 8620: loss = 2.2872297763824463\n",
            "step 8621: loss = 2.268411874771118\n",
            "step 8622: loss = 2.293952226638794\n",
            "step 8623: loss = 2.2166991233825684\n",
            "step 8624: loss = 2.3897860050201416\n",
            "step 8625: loss = 2.537524938583374\n",
            "step 8626: loss = 2.635782480239868\n",
            "step 8627: loss = 2.3432958126068115\n",
            "step 8628: loss = 2.4753057956695557\n",
            "step 8629: loss = 2.3119289875030518\n",
            "step 8630: loss = 2.0576045513153076\n",
            "step 8631: loss = 2.2108187675476074\n",
            "step 8632: loss = 2.2467446327209473\n",
            "step 8633: loss = 2.3666841983795166\n",
            "step 8634: loss = 2.2003839015960693\n",
            "step 8635: loss = 2.4736392498016357\n",
            "step 8636: loss = 2.3918814659118652\n",
            "step 8637: loss = 2.262624740600586\n",
            "step 8638: loss = 2.0525670051574707\n",
            "step 8639: loss = 2.350870370864868\n",
            "step 8640: loss = 2.1855790615081787\n",
            "step 8641: loss = 2.42781400680542\n",
            "step 8642: loss = 2.5214624404907227\n",
            "step 8643: loss = 2.8137943744659424\n",
            "step 8644: loss = 2.3525214195251465\n",
            "step 8645: loss = 2.225921869277954\n",
            "step 8646: loss = 2.31640362739563\n",
            "step 8647: loss = 2.7016103267669678\n",
            "step 8648: loss = 2.401085138320923\n",
            "step 8649: loss = 2.6274893283843994\n",
            "step 8650: loss = 2.318528413772583\n",
            "step 8651: loss = 2.463345766067505\n",
            "step 8652: loss = 2.6259701251983643\n",
            "step 8653: loss = 2.435068368911743\n",
            "step 8654: loss = 2.1969285011291504\n",
            "step 8655: loss = 2.266416549682617\n",
            "step 8656: loss = 2.1173160076141357\n",
            "step 8657: loss = 2.2295799255371094\n",
            "step 8658: loss = 2.2740590572357178\n",
            "step 8659: loss = 2.373502731323242\n",
            "step 8660: loss = 2.33994460105896\n",
            "step 8661: loss = 2.1046087741851807\n",
            "step 8662: loss = 2.144195556640625\n",
            "step 8663: loss = 2.2296786308288574\n",
            "step 8664: loss = 2.245993137359619\n",
            "step 8665: loss = 2.526543378829956\n",
            "step 8666: loss = 2.1629605293273926\n",
            "step 8667: loss = 2.3985321521759033\n",
            "step 8668: loss = 2.5745954513549805\n",
            "step 8669: loss = 2.1997365951538086\n",
            "step 8670: loss = 2.3073606491088867\n",
            "step 8671: loss = 2.287127733230591\n",
            "step 8672: loss = 2.4519479274749756\n",
            "step 8673: loss = 2.561399221420288\n",
            "step 8674: loss = 2.5531222820281982\n",
            "step 8675: loss = 2.168046712875366\n",
            "step 8676: loss = 2.4211533069610596\n",
            "step 8677: loss = 2.1398227214813232\n",
            "step 8678: loss = 2.568004608154297\n",
            "step 8679: loss = 2.0995092391967773\n",
            "step 8680: loss = 2.5063822269439697\n",
            "step 8681: loss = 2.421839952468872\n",
            "step 8682: loss = 2.5179171562194824\n",
            "step 8683: loss = 2.3218958377838135\n",
            "step 8684: loss = 2.3788952827453613\n",
            "step 8685: loss = 2.3266959190368652\n",
            "step 8686: loss = 2.2924373149871826\n",
            "step 8687: loss = 2.4365217685699463\n",
            "step 8688: loss = 2.249457597732544\n",
            "step 8689: loss = 1.987777590751648\n",
            "step 8690: loss = 2.4126782417297363\n",
            "step 8691: loss = 2.463095188140869\n",
            "step 8692: loss = 2.2148640155792236\n",
            "step 8693: loss = 2.225558280944824\n",
            "step 8694: loss = 2.5956380367279053\n",
            "step 8695: loss = 2.395104169845581\n",
            "step 8696: loss = 2.4152605533599854\n",
            "step 8697: loss = 2.5370895862579346\n",
            "step 8698: loss = 2.221951484680176\n",
            "step 8699: loss = 2.622086524963379\n",
            "step 8700: loss = 2.404338836669922\n",
            "step 8701: loss = 2.5829081535339355\n",
            "step 8702: loss = 2.3409013748168945\n",
            "step 8703: loss = 2.3186163902282715\n",
            "step 8704: loss = 2.2935307025909424\n",
            "step 8705: loss = 2.2414212226867676\n",
            "step 8706: loss = 2.4062352180480957\n",
            "step 8707: loss = 2.3523659706115723\n",
            "step 8708: loss = 2.420172691345215\n",
            "step 8709: loss = 2.5639243125915527\n",
            "step 8710: loss = 2.4080770015716553\n",
            "step 8711: loss = 2.363812208175659\n",
            "step 8712: loss = 2.325753688812256\n",
            "step 8713: loss = 2.3156299591064453\n",
            "step 8714: loss = 2.4478726387023926\n",
            "step 8715: loss = 2.2734649181365967\n",
            "step 8716: loss = 2.3103573322296143\n",
            "step 8717: loss = 2.2699124813079834\n",
            "step 8718: loss = 2.1450250148773193\n",
            "step 8719: loss = 2.1978368759155273\n",
            "step 8720: loss = 2.5543854236602783\n",
            "step 8721: loss = 2.5271244049072266\n",
            "step 8722: loss = 2.5631511211395264\n",
            "step 8723: loss = 2.5408530235290527\n",
            "step 8724: loss = 2.4743082523345947\n",
            "step 8725: loss = 2.456965923309326\n",
            "step 8726: loss = 2.3125617504119873\n",
            "step 8727: loss = 2.4229159355163574\n",
            "step 8728: loss = 2.3821706771850586\n",
            "step 8729: loss = 2.224308490753174\n",
            "step 8730: loss = 2.777825355529785\n",
            "step 8731: loss = 2.326935291290283\n",
            "step 8732: loss = 2.324995756149292\n",
            "step 8733: loss = 2.223146677017212\n",
            "step 8734: loss = 2.3867275714874268\n",
            "step 8735: loss = 2.368435859680176\n",
            "step 8736: loss = 2.393710136413574\n",
            "step 8737: loss = 2.460569143295288\n",
            "step 8738: loss = 2.5933163166046143\n",
            "step 8739: loss = 2.306424140930176\n",
            "step 8740: loss = 2.4324710369110107\n",
            "step 8741: loss = 2.2403652667999268\n",
            "step 8742: loss = 2.3356287479400635\n",
            "step 8743: loss = 2.2689106464385986\n",
            "step 8744: loss = 2.2635674476623535\n",
            "step 8745: loss = 2.3687233924865723\n",
            "step 8746: loss = 2.3424179553985596\n",
            "step 8747: loss = 2.3896217346191406\n",
            "step 8748: loss = 2.3970706462860107\n",
            "step 8749: loss = 2.450322151184082\n",
            "step 8750: loss = 2.3445205688476562\n",
            "step 8751: loss = 2.1606497764587402\n",
            "step 8752: loss = 2.688356637954712\n",
            "step 8753: loss = 2.348717451095581\n",
            "step 8754: loss = 2.290342330932617\n",
            "step 8755: loss = 2.518686532974243\n",
            "step 8756: loss = 2.348686695098877\n",
            "step 8757: loss = 2.675539970397949\n",
            "step 8758: loss = 2.3648147583007812\n",
            "step 8759: loss = 2.3440394401550293\n",
            "step 8760: loss = 2.6554365158081055\n",
            "step 8761: loss = 2.48214054107666\n",
            "step 8762: loss = 2.641582489013672\n",
            "step 8763: loss = 2.4946095943450928\n",
            "step 8764: loss = 2.2472639083862305\n",
            "step 8765: loss = 2.175966262817383\n",
            "step 8766: loss = 2.1734468936920166\n",
            "step 8767: loss = 2.1854941844940186\n",
            "step 8768: loss = 2.625840187072754\n",
            "step 8769: loss = 2.2465546131134033\n",
            "step 8770: loss = 2.3618369102478027\n",
            "step 8771: loss = 2.533618927001953\n",
            "step 8772: loss = 2.2379796504974365\n",
            "step 8773: loss = 2.3622851371765137\n",
            "step 8774: loss = 2.411325216293335\n",
            "step 8775: loss = 2.4637186527252197\n",
            "step 8776: loss = 2.4467766284942627\n",
            "step 8777: loss = 2.2928221225738525\n",
            "step 8778: loss = 2.4202113151550293\n",
            "step 8779: loss = 2.410400390625\n",
            "step 8780: loss = 2.518592596054077\n",
            "step 8781: loss = 2.4391419887542725\n",
            "step 8782: loss = 2.4687769412994385\n",
            "step 8783: loss = 2.4212934970855713\n",
            "step 8784: loss = 2.3321926593780518\n",
            "step 8785: loss = 2.395005226135254\n",
            "step 8786: loss = 2.3296406269073486\n",
            "step 8787: loss = 2.39910888671875\n",
            "step 8788: loss = 2.2483482360839844\n",
            "step 8789: loss = 2.4699928760528564\n",
            "step 8790: loss = 2.4841067790985107\n",
            "step 8791: loss = 2.192152500152588\n",
            "step 8792: loss = 2.4005753993988037\n",
            "step 8793: loss = 2.2833945751190186\n",
            "step 8794: loss = 2.270130157470703\n",
            "step 8795: loss = 2.3929717540740967\n",
            "step 8796: loss = 2.3395774364471436\n",
            "step 8797: loss = 2.151265859603882\n",
            "step 8798: loss = 2.6307241916656494\n",
            "step 8799: loss = 2.210886001586914\n",
            "step 8800: loss = 2.252568483352661\n",
            "step 8801: loss = 2.6829094886779785\n",
            "step 8802: loss = 2.316408634185791\n",
            "step 8803: loss = 2.521752119064331\n",
            "step 8804: loss = 2.274430274963379\n",
            "step 8805: loss = 2.3622310161590576\n",
            "step 8806: loss = 2.232975959777832\n",
            "step 8807: loss = 2.0925471782684326\n",
            "step 8808: loss = 2.3107783794403076\n",
            "step 8809: loss = 2.5866973400115967\n",
            "step 8810: loss = 2.5901827812194824\n",
            "step 8811: loss = 2.1465561389923096\n",
            "step 8812: loss = 2.4158053398132324\n",
            "step 8813: loss = 2.3027172088623047\n",
            "step 8814: loss = 2.274254560470581\n",
            "step 8815: loss = 2.5300395488739014\n",
            "step 8816: loss = 2.4722471237182617\n",
            "step 8817: loss = 2.144386053085327\n",
            "step 8818: loss = 2.6706650257110596\n",
            "step 8819: loss = 2.4456441402435303\n",
            "step 8820: loss = 2.338012218475342\n",
            "step 8821: loss = 2.4855401515960693\n",
            "step 8822: loss = 2.4429092407226562\n",
            "step 8823: loss = 2.5309953689575195\n",
            "step 8824: loss = 2.1692047119140625\n",
            "step 8825: loss = 2.3617560863494873\n",
            "step 8826: loss = 2.23752760887146\n",
            "step 8827: loss = 2.2797157764434814\n",
            "step 8828: loss = 2.541748523712158\n",
            "step 8829: loss = 2.2956440448760986\n",
            "step 8830: loss = 2.435938835144043\n",
            "step 8831: loss = 2.0902748107910156\n",
            "step 8832: loss = 2.5222771167755127\n",
            "step 8833: loss = 2.3269569873809814\n",
            "step 8834: loss = 2.4272947311401367\n",
            "step 8835: loss = 2.2495522499084473\n",
            "step 8836: loss = 2.2469260692596436\n",
            "step 8837: loss = 2.407846689224243\n",
            "step 8838: loss = 2.6392629146575928\n",
            "step 8839: loss = 2.2248048782348633\n",
            "step 8840: loss = 2.2850029468536377\n",
            "step 8841: loss = 2.2746551036834717\n",
            "step 8842: loss = 2.4902472496032715\n",
            "step 8843: loss = 2.4279797077178955\n",
            "step 8844: loss = 2.301208257675171\n",
            "step 8845: loss = 2.2272937297821045\n",
            "step 8846: loss = 2.4656119346618652\n",
            "step 8847: loss = 2.03826642036438\n",
            "step 8848: loss = 2.413764715194702\n",
            "step 8849: loss = 2.5322377681732178\n",
            "step 8850: loss = 2.2896976470947266\n",
            "step 8851: loss = 2.299617052078247\n",
            "step 8852: loss = 2.5110087394714355\n",
            "step 8853: loss = 2.4647891521453857\n",
            "step 8854: loss = 2.587656259536743\n",
            "step 8855: loss = 2.444953441619873\n",
            "step 8856: loss = 2.521010398864746\n",
            "step 8857: loss = 2.4024956226348877\n",
            "step 8858: loss = 2.3624980449676514\n",
            "step 8859: loss = 2.3602705001831055\n",
            "step 8860: loss = 2.255035161972046\n",
            "step 8861: loss = 2.362713575363159\n",
            "step 8862: loss = 2.488030433654785\n",
            "step 8863: loss = 2.185581922531128\n",
            "step 8864: loss = 2.048333168029785\n",
            "step 8865: loss = 2.585860252380371\n",
            "step 8866: loss = 2.553823709487915\n",
            "step 8867: loss = 2.1525375843048096\n",
            "step 8868: loss = 2.411355495452881\n",
            "step 8869: loss = 2.619539737701416\n",
            "step 8870: loss = 2.4190921783447266\n",
            "step 8871: loss = 2.3655269145965576\n",
            "step 8872: loss = 2.4038333892822266\n",
            "step 8873: loss = 2.4556643962860107\n",
            "step 8874: loss = 2.2972021102905273\n",
            "step 8875: loss = 2.263934850692749\n",
            "step 8876: loss = 2.6937201023101807\n",
            "step 8877: loss = 2.280435800552368\n",
            "step 8878: loss = 2.3134350776672363\n",
            "step 8879: loss = 2.4834907054901123\n",
            "step 8880: loss = 2.335711717605591\n",
            "step 8881: loss = 2.5225555896759033\n",
            "step 8882: loss = 2.2021641731262207\n",
            "step 8883: loss = 2.516209363937378\n",
            "step 8884: loss = 2.3991944789886475\n",
            "step 8885: loss = 2.283506393432617\n",
            "step 8886: loss = 2.508322238922119\n",
            "step 8887: loss = 2.4750969409942627\n",
            "step 8888: loss = 2.4390130043029785\n",
            "step 8889: loss = 2.719796657562256\n",
            "step 8890: loss = 2.2386233806610107\n",
            "step 8891: loss = 2.3574929237365723\n",
            "step 8892: loss = 2.307542324066162\n",
            "step 8893: loss = 2.4389405250549316\n",
            "step 8894: loss = 2.32541561126709\n",
            "step 8895: loss = 2.087223768234253\n",
            "step 8896: loss = 2.479785680770874\n",
            "step 8897: loss = 2.510772228240967\n",
            "step 8898: loss = 2.5701117515563965\n",
            "step 8899: loss = 2.601083278656006\n",
            "step 8900: loss = 2.320218324661255\n",
            "step 8901: loss = 2.485865592956543\n",
            "step 8902: loss = 2.2897238731384277\n",
            "step 8903: loss = 2.409665822982788\n",
            "step 8904: loss = 2.452131986618042\n",
            "step 8905: loss = 2.1912083625793457\n",
            "step 8906: loss = 2.3694655895233154\n",
            "step 8907: loss = 2.3442184925079346\n",
            "step 8908: loss = 2.5741569995880127\n",
            "step 8909: loss = 2.5838115215301514\n",
            "step 8910: loss = 2.137392044067383\n",
            "step 8911: loss = 2.664180040359497\n",
            "step 8912: loss = 2.4366848468780518\n",
            "step 8913: loss = 2.2376811504364014\n",
            "step 8914: loss = 2.3479549884796143\n",
            "step 8915: loss = 2.363462209701538\n",
            "step 8916: loss = 2.3494057655334473\n",
            "step 8917: loss = 2.192969560623169\n",
            "step 8918: loss = 2.1252148151397705\n",
            "step 8919: loss = 2.380812883377075\n",
            "step 8920: loss = 2.3572425842285156\n",
            "step 8921: loss = 2.567861795425415\n",
            "step 8922: loss = 2.4582347869873047\n",
            "step 8923: loss = 2.3937745094299316\n",
            "step 8924: loss = 2.1856393814086914\n",
            "step 8925: loss = 2.3616397380828857\n",
            "step 8926: loss = 2.603947162628174\n",
            "step 8927: loss = 2.1404950618743896\n",
            "step 8928: loss = 2.4432826042175293\n",
            "step 8929: loss = 2.5022504329681396\n",
            "step 8930: loss = 2.256474018096924\n",
            "step 8931: loss = 2.3521108627319336\n",
            "step 8932: loss = 2.1144003868103027\n",
            "step 8933: loss = 2.338639974594116\n",
            "step 8934: loss = 2.2566514015197754\n",
            "step 8935: loss = 2.4113385677337646\n",
            "step 8936: loss = 2.5225648880004883\n",
            "step 8937: loss = 2.3586721420288086\n",
            "step 8938: loss = 2.454080104827881\n",
            "step 8939: loss = 2.4792754650115967\n",
            "step 8940: loss = 2.503943920135498\n",
            "step 8941: loss = 2.503368854522705\n",
            "step 8942: loss = 2.3692080974578857\n",
            "step 8943: loss = 2.393883466720581\n",
            "step 8944: loss = 2.410022020339966\n",
            "step 8945: loss = 2.333920478820801\n",
            "step 8946: loss = 2.2485713958740234\n",
            "step 8947: loss = 2.4088990688323975\n",
            "step 8948: loss = 2.206433057785034\n",
            "step 8949: loss = 2.7081804275512695\n",
            "step 8950: loss = 2.5041136741638184\n",
            "step 8951: loss = 2.6330504417419434\n",
            "step 8952: loss = 2.5480809211730957\n",
            "step 8953: loss = 2.320042610168457\n",
            "step 8954: loss = 2.4254186153411865\n",
            "step 8955: loss = 2.364426374435425\n",
            "step 8956: loss = 2.2658164501190186\n",
            "step 8957: loss = 2.4009954929351807\n",
            "step 8958: loss = 2.3900275230407715\n",
            "step 8959: loss = 2.235344409942627\n",
            "step 8960: loss = 2.49882435798645\n",
            "step 8961: loss = 2.641061782836914\n",
            "step 8962: loss = 2.504056930541992\n",
            "step 8963: loss = 2.8849594593048096\n",
            "step 8964: loss = 2.603518009185791\n",
            "step 8965: loss = 2.357367753982544\n",
            "step 8966: loss = 2.62073016166687\n",
            "step 8967: loss = 2.3059310913085938\n",
            "step 8968: loss = 2.4979538917541504\n",
            "step 8969: loss = 2.1684060096740723\n",
            "step 8970: loss = 2.656740188598633\n",
            "step 8971: loss = 2.6488449573516846\n",
            "step 8972: loss = 2.3018081188201904\n",
            "step 8973: loss = 2.0476675033569336\n",
            "step 8974: loss = 2.348449468612671\n",
            "step 8975: loss = 2.576904296875\n",
            "step 8976: loss = 2.354088544845581\n",
            "step 8977: loss = 2.588998794555664\n",
            "step 8978: loss = 2.6163158416748047\n",
            "step 8979: loss = 2.401027202606201\n",
            "step 8980: loss = 2.516089677810669\n",
            "step 8981: loss = 2.163656234741211\n",
            "step 8982: loss = 2.2175424098968506\n",
            "step 8983: loss = 2.720412254333496\n",
            "step 8984: loss = 2.3950302600860596\n",
            "step 8985: loss = 2.505117177963257\n",
            "step 8986: loss = 2.213679552078247\n",
            "step 8987: loss = 2.449984312057495\n",
            "step 8988: loss = 2.3830480575561523\n",
            "step 8989: loss = 2.1782357692718506\n",
            "step 8990: loss = 2.495307445526123\n",
            "step 8991: loss = 2.270235538482666\n",
            "step 8992: loss = 2.2650513648986816\n",
            "step 8993: loss = 2.483917236328125\n",
            "step 8994: loss = 2.3911070823669434\n",
            "step 8995: loss = 2.3846216201782227\n",
            "step 8996: loss = 2.2044131755828857\n",
            "step 8997: loss = 2.5346240997314453\n",
            "step 8998: loss = 2.5213112831115723\n",
            "step 8999: loss = 2.5930302143096924\n",
            "step 9000: loss = 2.5277392864227295\n",
            "step 9001: loss = 2.3141753673553467\n",
            "step 9002: loss = 2.673203229904175\n",
            "step 9003: loss = 2.5220911502838135\n",
            "step 9004: loss = 2.5101351737976074\n",
            "step 9005: loss = 2.462986469268799\n",
            "step 9006: loss = 2.471975803375244\n",
            "step 9007: loss = 1.9505869150161743\n",
            "step 9008: loss = 2.3509039878845215\n",
            "step 9009: loss = 2.309905529022217\n",
            "step 9010: loss = 2.2736711502075195\n",
            "step 9011: loss = 2.225475549697876\n",
            "step 9012: loss = 2.742629051208496\n",
            "step 9013: loss = 2.473679542541504\n",
            "step 9014: loss = 2.239591598510742\n",
            "step 9015: loss = 2.1953699588775635\n",
            "step 9016: loss = 2.244802474975586\n",
            "step 9017: loss = 2.439122438430786\n",
            "step 9018: loss = 2.333645820617676\n",
            "step 9019: loss = 2.3215348720550537\n",
            "step 9020: loss = 2.243469715118408\n",
            "step 9021: loss = 2.3295483589172363\n",
            "step 9022: loss = 2.3055942058563232\n",
            "step 9023: loss = 2.250544786453247\n",
            "step 9024: loss = 2.131011486053467\n",
            "step 9025: loss = 2.366722822189331\n",
            "step 9026: loss = 2.2780091762542725\n",
            "step 9027: loss = 2.168590545654297\n",
            "step 9028: loss = 2.671058177947998\n",
            "step 9029: loss = 2.3638663291931152\n",
            "step 9030: loss = 2.371767997741699\n",
            "step 9031: loss = 2.2366271018981934\n",
            "step 9032: loss = 2.353273630142212\n",
            "step 9033: loss = 2.3744163513183594\n",
            "step 9034: loss = 2.189232587814331\n",
            "step 9035: loss = 2.5616278648376465\n",
            "step 9036: loss = 2.4005837440490723\n",
            "step 9037: loss = 2.508246421813965\n",
            "step 9038: loss = 2.381649971008301\n",
            "step 9039: loss = 2.1568551063537598\n",
            "step 9040: loss = 2.582624673843384\n",
            "step 9041: loss = 2.6082093715667725\n",
            "step 9042: loss = 2.2874557971954346\n",
            "step 9043: loss = 2.3876571655273438\n",
            "step 9044: loss = 2.3641364574432373\n",
            "step 9045: loss = 2.4310543537139893\n",
            "step 9046: loss = 2.4872825145721436\n",
            "step 9047: loss = 2.225440502166748\n",
            "step 9048: loss = 2.5373895168304443\n",
            "step 9049: loss = 2.2411248683929443\n",
            "step 9050: loss = 2.5268476009368896\n",
            "step 9051: loss = 2.520200252532959\n",
            "step 9052: loss = 2.4166970252990723\n",
            "step 9053: loss = 2.1438584327697754\n",
            "step 9054: loss = 2.54723858833313\n",
            "step 9055: loss = 2.2173867225646973\n",
            "step 9056: loss = 2.498842477798462\n",
            "step 9057: loss = 2.5251452922821045\n",
            "step 9058: loss = 2.188981533050537\n",
            "step 9059: loss = 2.4440722465515137\n",
            "step 9060: loss = 2.4050371646881104\n",
            "step 9061: loss = 2.3155274391174316\n",
            "step 9062: loss = 2.4105255603790283\n",
            "step 9063: loss = 2.5294299125671387\n",
            "step 9064: loss = 2.322397470474243\n",
            "step 9065: loss = 2.4238831996917725\n",
            "step 9066: loss = 2.4006845951080322\n",
            "step 9067: loss = 2.3478827476501465\n",
            "step 9068: loss = 2.273280382156372\n",
            "step 9069: loss = 2.5571706295013428\n",
            "step 9070: loss = 2.5893092155456543\n",
            "step 9071: loss = 2.3896877765655518\n",
            "step 9072: loss = 2.219956874847412\n",
            "step 9073: loss = 2.3810319900512695\n",
            "step 9074: loss = 2.3703789710998535\n",
            "step 9075: loss = 2.3991687297821045\n",
            "step 9076: loss = 2.5311601161956787\n",
            "step 9077: loss = 2.345076560974121\n",
            "step 9078: loss = 2.1997110843658447\n",
            "step 9079: loss = 2.483098268508911\n",
            "step 9080: loss = 2.7032296657562256\n",
            "step 9081: loss = 2.5315158367156982\n",
            "step 9082: loss = 2.5784828662872314\n",
            "step 9083: loss = 2.361095428466797\n",
            "step 9084: loss = 2.366208791732788\n",
            "step 9085: loss = 2.5088179111480713\n",
            "step 9086: loss = 2.430145025253296\n",
            "step 9087: loss = 2.7034084796905518\n",
            "step 9088: loss = 2.211118459701538\n",
            "step 9089: loss = 2.5317506790161133\n",
            "step 9090: loss = 2.6317663192749023\n",
            "step 9091: loss = 2.020486831665039\n",
            "step 9092: loss = 2.4697554111480713\n",
            "step 9093: loss = 2.598651647567749\n",
            "step 9094: loss = 2.392094612121582\n",
            "step 9095: loss = 2.263073205947876\n",
            "step 9096: loss = 2.4052186012268066\n",
            "step 9097: loss = 2.130359172821045\n",
            "step 9098: loss = 2.343385696411133\n",
            "step 9099: loss = 2.597757339477539\n",
            "step 9100: loss = 2.4213485717773438\n",
            "step 9101: loss = 2.332042694091797\n",
            "step 9102: loss = 2.3284006118774414\n",
            "step 9103: loss = 2.1442160606384277\n",
            "step 9104: loss = 2.575277328491211\n",
            "step 9105: loss = 2.6126017570495605\n",
            "step 9106: loss = 2.2933754920959473\n",
            "step 9107: loss = 2.372100830078125\n",
            "step 9108: loss = 2.557105779647827\n",
            "step 9109: loss = 2.5592384338378906\n",
            "step 9110: loss = 2.301013946533203\n",
            "step 9111: loss = 2.4081950187683105\n",
            "step 9112: loss = 2.431131362915039\n",
            "step 9113: loss = 2.2881858348846436\n",
            "step 9114: loss = 2.5382838249206543\n",
            "step 9115: loss = 2.5464513301849365\n",
            "step 9116: loss = 2.549226999282837\n",
            "step 9117: loss = 2.5030033588409424\n",
            "step 9118: loss = 2.605451822280884\n",
            "step 9119: loss = 2.3575246334075928\n",
            "step 9120: loss = 2.3385865688323975\n",
            "step 9121: loss = 2.440708875656128\n",
            "step 9122: loss = 2.4812116622924805\n",
            "step 9123: loss = 2.586183786392212\n",
            "step 9124: loss = 2.687629222869873\n",
            "step 9125: loss = 2.340080976486206\n",
            "step 9126: loss = 2.5916330814361572\n",
            "step 9127: loss = 2.3997068405151367\n",
            "step 9128: loss = 2.2998106479644775\n",
            "step 9129: loss = 2.683588743209839\n",
            "step 9130: loss = 2.3890316486358643\n",
            "step 9131: loss = 2.660785436630249\n",
            "step 9132: loss = 2.4878334999084473\n",
            "step 9133: loss = 2.4892640113830566\n",
            "step 9134: loss = 2.363694190979004\n",
            "step 9135: loss = 2.615821599960327\n",
            "step 9136: loss = 2.4402780532836914\n",
            "step 9137: loss = 2.4938435554504395\n",
            "step 9138: loss = 2.6424977779388428\n",
            "step 9139: loss = 2.3645527362823486\n",
            "step 9140: loss = 2.3097140789031982\n",
            "step 9141: loss = 2.2749223709106445\n",
            "step 9142: loss = 2.6683242321014404\n",
            "step 9143: loss = 2.417233943939209\n",
            "step 9144: loss = 2.5180575847625732\n",
            "step 9145: loss = 2.6220643520355225\n",
            "step 9146: loss = 2.6496334075927734\n",
            "step 9147: loss = 2.2464234828948975\n",
            "step 9148: loss = 2.3341128826141357\n",
            "step 9149: loss = 2.409349203109741\n",
            "step 9150: loss = 2.3670432567596436\n",
            "step 9151: loss = 2.4318079948425293\n",
            "step 9152: loss = 1.997802495956421\n",
            "step 9153: loss = 2.6233983039855957\n",
            "step 9154: loss = 2.770092248916626\n",
            "step 9155: loss = 2.539734363555908\n",
            "step 9156: loss = 2.4683773517608643\n",
            "step 9157: loss = 2.557698965072632\n",
            "step 9158: loss = 2.4589126110076904\n",
            "step 9159: loss = 2.56451416015625\n",
            "step 9160: loss = 2.254133701324463\n",
            "step 9161: loss = 2.5413854122161865\n",
            "step 9162: loss = 2.6227331161499023\n",
            "step 9163: loss = 2.582303524017334\n",
            "step 9164: loss = 2.6459126472473145\n",
            "step 9165: loss = 2.455260753631592\n",
            "step 9166: loss = 2.239103317260742\n",
            "step 9167: loss = 2.4172139167785645\n",
            "step 9168: loss = 2.495349884033203\n",
            "step 9169: loss = 2.2877655029296875\n",
            "step 9170: loss = 2.3341214656829834\n",
            "step 9171: loss = 2.7147438526153564\n",
            "step 9172: loss = 2.158215284347534\n",
            "step 9173: loss = 2.3922719955444336\n",
            "step 9174: loss = 2.6712257862091064\n",
            "step 9175: loss = 2.5035746097564697\n",
            "step 9176: loss = 2.0727038383483887\n",
            "step 9177: loss = 2.69564151763916\n",
            "step 9178: loss = 2.3573408126831055\n",
            "step 9179: loss = 2.4362196922302246\n",
            "step 9180: loss = 2.402543783187866\n",
            "step 9181: loss = 2.5777623653411865\n",
            "step 9182: loss = 2.5272319316864014\n",
            "step 9183: loss = 2.35684871673584\n",
            "step 9184: loss = 2.4993135929107666\n",
            "step 9185: loss = 2.322211265563965\n",
            "step 9186: loss = 2.188523530960083\n",
            "step 9187: loss = 2.069624900817871\n",
            "step 9188: loss = 2.3409006595611572\n",
            "step 9189: loss = 2.414822578430176\n",
            "step 9190: loss = 2.5539047718048096\n",
            "step 9191: loss = 2.383192300796509\n",
            "step 9192: loss = 2.457329511642456\n",
            "step 9193: loss = 2.472038984298706\n",
            "step 9194: loss = 2.5070767402648926\n",
            "step 9195: loss = 2.581300735473633\n",
            "step 9196: loss = 2.2924180030822754\n",
            "step 9197: loss = 2.5840165615081787\n",
            "step 9198: loss = 2.345513105392456\n",
            "step 9199: loss = 2.5537807941436768\n",
            "step 9200: loss = 2.3929786682128906\n",
            "step 9201: loss = 2.5799317359924316\n",
            "step 9202: loss = 2.2509326934814453\n",
            "step 9203: loss = 2.4571659564971924\n",
            "step 9204: loss = 2.627262830734253\n",
            "step 9205: loss = 2.532769203186035\n",
            "step 9206: loss = 2.6629064083099365\n",
            "step 9207: loss = 2.47061824798584\n",
            "step 9208: loss = 2.263364791870117\n",
            "step 9209: loss = 2.463545083999634\n",
            "step 9210: loss = 2.4041855335235596\n",
            "step 9211: loss = 2.3015642166137695\n",
            "step 9212: loss = 2.468000650405884\n",
            "step 9213: loss = 2.283421039581299\n",
            "step 9214: loss = 2.459468364715576\n",
            "step 9215: loss = 2.3135926723480225\n",
            "step 9216: loss = 2.631563663482666\n",
            "step 9217: loss = 2.570648431777954\n",
            "step 9218: loss = 2.092761516571045\n",
            "step 9219: loss = 2.7183451652526855\n",
            "step 9220: loss = 2.6044058799743652\n",
            "step 9221: loss = 2.229841947555542\n",
            "step 9222: loss = 2.4559130668640137\n",
            "step 9223: loss = 2.524082660675049\n",
            "step 9224: loss = 2.31437611579895\n",
            "step 9225: loss = 2.3730318546295166\n",
            "step 9226: loss = 2.4615776538848877\n",
            "step 9227: loss = 2.6027891635894775\n",
            "step 9228: loss = 2.1919984817504883\n",
            "step 9229: loss = 2.19461727142334\n",
            "step 9230: loss = 2.3667163848876953\n",
            "step 9231: loss = 2.317915201187134\n",
            "step 9232: loss = 2.433892250061035\n",
            "step 9233: loss = 2.4734549522399902\n",
            "step 9234: loss = 2.458527088165283\n",
            "step 9235: loss = 2.430419683456421\n",
            "step 9236: loss = 2.405689239501953\n",
            "step 9237: loss = 2.486599922180176\n",
            "step 9238: loss = 2.5417087078094482\n",
            "step 9239: loss = 2.4309370517730713\n",
            "step 9240: loss = 2.3131628036499023\n",
            "step 9241: loss = 2.471835136413574\n",
            "step 9242: loss = 2.334775924682617\n",
            "step 9243: loss = 2.4198338985443115\n",
            "step 9244: loss = 2.3631515502929688\n",
            "step 9245: loss = 2.408634901046753\n",
            "step 9246: loss = 2.4607584476470947\n",
            "step 9247: loss = 2.3166375160217285\n",
            "step 9248: loss = 2.4492151737213135\n",
            "step 9249: loss = 2.524348258972168\n",
            "step 9250: loss = 2.6078686714172363\n",
            "step 9251: loss = 2.330737590789795\n",
            "step 9252: loss = 2.4877843856811523\n",
            "step 9253: loss = 2.257412910461426\n",
            "step 9254: loss = 2.3325295448303223\n",
            "step 9255: loss = 2.488267421722412\n",
            "step 9256: loss = 2.5451974868774414\n",
            "step 9257: loss = 2.279132127761841\n",
            "step 9258: loss = 2.377715826034546\n",
            "step 9259: loss = 2.4638967514038086\n",
            "step 9260: loss = 2.484372615814209\n",
            "step 9261: loss = 2.073335886001587\n",
            "step 9262: loss = 1.8863908052444458\n",
            "step 9263: loss = 2.5283243656158447\n",
            "step 9264: loss = 2.510859727859497\n",
            "step 9265: loss = 2.398043155670166\n",
            "step 9266: loss = 2.6159396171569824\n",
            "step 9267: loss = 2.3040354251861572\n",
            "step 9268: loss = 2.5206384658813477\n",
            "step 9269: loss = 2.562192678451538\n",
            "step 9270: loss = 2.499220609664917\n",
            "step 9271: loss = 2.360215187072754\n",
            "step 9272: loss = 2.3748629093170166\n",
            "step 9273: loss = 2.397512435913086\n",
            "step 9274: loss = 2.375540256500244\n",
            "step 9275: loss = 2.395195960998535\n",
            "step 9276: loss = 2.5038397312164307\n",
            "step 9277: loss = 2.2909035682678223\n",
            "step 9278: loss = 2.4187397956848145\n",
            "step 9279: loss = 2.3553688526153564\n",
            "step 9280: loss = 2.391716957092285\n",
            "step 9281: loss = 2.4030768871307373\n",
            "step 9282: loss = 2.3117477893829346\n",
            "step 9283: loss = 2.429576873779297\n",
            "step 9284: loss = 2.2361814975738525\n",
            "step 9285: loss = 2.2499988079071045\n",
            "step 9286: loss = 2.190958023071289\n",
            "step 9287: loss = 2.3383781909942627\n",
            "step 9288: loss = 2.3466668128967285\n",
            "step 9289: loss = 2.7695751190185547\n",
            "step 9290: loss = 2.4117720127105713\n",
            "step 9291: loss = 2.4440665245056152\n",
            "step 9292: loss = 2.4177160263061523\n",
            "step 9293: loss = 2.4422497749328613\n",
            "step 9294: loss = 2.127207040786743\n",
            "step 9295: loss = 2.455409288406372\n",
            "step 9296: loss = 2.3895983695983887\n",
            "step 9297: loss = 2.6177241802215576\n",
            "step 9298: loss = 2.4465041160583496\n",
            "step 9299: loss = 2.41410756111145\n",
            "step 9300: loss = 2.496259927749634\n",
            "step 9301: loss = 2.1435439586639404\n",
            "step 9302: loss = 2.3892459869384766\n",
            "step 9303: loss = 2.5054993629455566\n",
            "step 9304: loss = 2.5671627521514893\n",
            "step 9305: loss = 2.6906352043151855\n",
            "step 9306: loss = 2.4557721614837646\n",
            "step 9307: loss = 2.4042158126831055\n",
            "step 9308: loss = 2.2230091094970703\n",
            "step 9309: loss = 2.336867570877075\n",
            "step 9310: loss = 2.289275646209717\n",
            "step 9311: loss = 2.366702079772949\n",
            "step 9312: loss = 2.3021512031555176\n",
            "step 9313: loss = 2.3150930404663086\n",
            "step 9314: loss = 2.411611557006836\n",
            "step 9315: loss = 2.4205782413482666\n",
            "step 9316: loss = 2.230046510696411\n",
            "step 9317: loss = 2.49983811378479\n",
            "step 9318: loss = 2.566591262817383\n",
            "step 9319: loss = 2.5111265182495117\n",
            "step 9320: loss = 2.289801836013794\n",
            "step 9321: loss = 2.6488893032073975\n",
            "step 9322: loss = 2.0792033672332764\n",
            "step 9323: loss = 2.4868574142456055\n",
            "step 9324: loss = 2.3766729831695557\n",
            "step 9325: loss = 2.46614408493042\n",
            "step 9326: loss = 2.7269396781921387\n",
            "step 9327: loss = 2.686061143875122\n",
            "step 9328: loss = 2.398550033569336\n",
            "step 9329: loss = 2.3564717769622803\n",
            "step 9330: loss = 2.4183671474456787\n",
            "step 9331: loss = 2.6213231086730957\n",
            "step 9332: loss = 2.4712109565734863\n",
            "step 9333: loss = 2.2346913814544678\n",
            "step 9334: loss = 2.3281893730163574\n",
            "step 9335: loss = 2.5175304412841797\n",
            "step 9336: loss = 2.3232126235961914\n",
            "step 9337: loss = 2.2742319107055664\n",
            "step 9338: loss = 2.330867052078247\n",
            "step 9339: loss = 2.1908340454101562\n",
            "step 9340: loss = 2.644437074661255\n",
            "step 9341: loss = 2.424236297607422\n",
            "step 9342: loss = 2.4366447925567627\n",
            "step 9343: loss = 2.361156463623047\n",
            "step 9344: loss = 2.484426259994507\n",
            "step 9345: loss = 2.280080795288086\n",
            "step 9346: loss = 2.539354085922241\n",
            "step 9347: loss = 2.6945722103118896\n",
            "step 9348: loss = 2.440814733505249\n",
            "step 9349: loss = 2.4946811199188232\n",
            "step 9350: loss = 2.148332357406616\n",
            "step 9351: loss = 2.3981549739837646\n",
            "step 9352: loss = 2.4187076091766357\n",
            "step 9353: loss = 2.548128604888916\n",
            "step 9354: loss = 2.5175719261169434\n",
            "step 9355: loss = 2.4369113445281982\n",
            "step 9356: loss = 2.4831202030181885\n",
            "step 9357: loss = 2.485107898712158\n",
            "step 9358: loss = 2.3948917388916016\n",
            "step 9359: loss = 2.3081202507019043\n",
            "step 9360: loss = 2.3734498023986816\n",
            "step 9361: loss = 2.375821590423584\n",
            "step 9362: loss = 2.530348539352417\n",
            "step 9363: loss = 2.38474440574646\n",
            "step 9364: loss = 2.436136245727539\n",
            "step 9365: loss = 2.5283946990966797\n",
            "step 9366: loss = 2.5583183765411377\n",
            "step 9367: loss = 2.4681222438812256\n",
            "step 9368: loss = 2.328552484512329\n",
            "step 9369: loss = 2.267554998397827\n",
            "step 9370: loss = 2.4900336265563965\n",
            "step 9371: loss = 2.2949578762054443\n",
            "step 9372: loss = 2.643421173095703\n",
            "Finish epoch 6\n",
            "New model saved, minimum loss: 2.3472600093113782 \n",
            "\n",
            "step 9373: loss = 2.091989278793335\n",
            "step 9374: loss = 1.8393293619155884\n",
            "step 9375: loss = 1.9641319513320923\n",
            "step 9376: loss = 1.8808825016021729\n",
            "step 9377: loss = 1.8840476274490356\n",
            "step 9378: loss = 1.9289294481277466\n",
            "step 9379: loss = 2.0255422592163086\n",
            "step 9380: loss = 1.8467137813568115\n",
            "step 9381: loss = 1.9030612707138062\n",
            "step 9382: loss = 1.8061938285827637\n",
            "step 9383: loss = 1.9904727935791016\n",
            "step 9384: loss = 1.90099036693573\n",
            "step 9385: loss = 1.9042649269104004\n",
            "step 9386: loss = 1.9206229448318481\n",
            "step 9387: loss = 1.9862346649169922\n",
            "step 9388: loss = 1.7863482236862183\n",
            "step 9389: loss = 1.9585084915161133\n",
            "step 9390: loss = 1.9342883825302124\n",
            "step 9391: loss = 2.012890100479126\n",
            "step 9392: loss = 2.114845037460327\n",
            "step 9393: loss = 1.6959296464920044\n",
            "step 9394: loss = 1.9106138944625854\n",
            "step 9395: loss = 1.818389892578125\n",
            "step 9396: loss = 1.8213452100753784\n",
            "step 9397: loss = 2.124375820159912\n",
            "step 9398: loss = 2.080787420272827\n",
            "step 9399: loss = 2.1571407318115234\n",
            "step 9400: loss = 2.186445951461792\n",
            "step 9401: loss = 1.977988362312317\n",
            "step 9402: loss = 2.073976993560791\n",
            "step 9403: loss = 1.9068199396133423\n",
            "step 9404: loss = 1.98202645778656\n",
            "step 9405: loss = 1.980316162109375\n",
            "step 9406: loss = 1.9314662218093872\n",
            "step 9407: loss = 1.9602530002593994\n",
            "step 9408: loss = 1.6814841032028198\n",
            "step 9409: loss = 1.986143708229065\n",
            "step 9410: loss = 1.8190934658050537\n",
            "step 9411: loss = 1.9798084497451782\n",
            "step 9412: loss = 2.1440699100494385\n",
            "step 9413: loss = 1.8487637042999268\n",
            "step 9414: loss = 1.787559151649475\n",
            "step 9415: loss = 1.8662723302841187\n",
            "step 9416: loss = 1.8432233333587646\n",
            "step 9417: loss = 1.8490478992462158\n",
            "step 9418: loss = 2.093174457550049\n",
            "step 9419: loss = 2.0811028480529785\n",
            "step 9420: loss = 2.0839903354644775\n",
            "step 9421: loss = 1.9990084171295166\n",
            "step 9422: loss = 2.042856454849243\n",
            "step 9423: loss = 1.9029154777526855\n",
            "step 9424: loss = 2.1665127277374268\n",
            "step 9425: loss = 2.112602472305298\n",
            "step 9426: loss = 1.7691587209701538\n",
            "step 9427: loss = 2.049985885620117\n",
            "step 9428: loss = 2.067812919616699\n",
            "step 9429: loss = 2.0888218879699707\n",
            "step 9430: loss = 1.9091640710830688\n",
            "step 9431: loss = 2.075756788253784\n",
            "step 9432: loss = 1.717926025390625\n",
            "step 9433: loss = 2.0789906978607178\n",
            "step 9434: loss = 2.0957019329071045\n",
            "step 9435: loss = 1.9971023797988892\n",
            "step 9436: loss = 1.683947205543518\n",
            "step 9437: loss = 2.160883665084839\n",
            "step 9438: loss = 2.1088531017303467\n",
            "step 9439: loss = 2.035449981689453\n",
            "step 9440: loss = 1.770140528678894\n",
            "step 9441: loss = 1.9996548891067505\n",
            "step 9442: loss = 2.195444107055664\n",
            "step 9443: loss = 1.8859020471572876\n",
            "step 9444: loss = 1.9574166536331177\n",
            "step 9445: loss = 2.2485876083374023\n",
            "step 9446: loss = 2.0596764087677\n",
            "step 9447: loss = 1.9398581981658936\n",
            "step 9448: loss = 2.0208747386932373\n",
            "step 9449: loss = 2.0432262420654297\n",
            "step 9450: loss = 2.4033267498016357\n",
            "step 9451: loss = 1.9618232250213623\n",
            "step 9452: loss = 1.780031681060791\n",
            "step 9453: loss = 1.981627106666565\n",
            "step 9454: loss = 2.251208543777466\n",
            "step 9455: loss = 1.8636884689331055\n",
            "step 9456: loss = 2.0022075176239014\n",
            "step 9457: loss = 1.9135898351669312\n",
            "step 9458: loss = 2.050384759902954\n",
            "step 9459: loss = 1.949720025062561\n",
            "step 9460: loss = 1.9614346027374268\n",
            "step 9461: loss = 2.0137574672698975\n",
            "step 9462: loss = 1.9130280017852783\n",
            "step 9463: loss = 1.8472645282745361\n",
            "step 9464: loss = 2.1319031715393066\n",
            "step 9465: loss = 2.0852537155151367\n",
            "step 9466: loss = 2.068150520324707\n",
            "step 9467: loss = 1.8071843385696411\n",
            "step 9468: loss = 1.8860334157943726\n",
            "step 9469: loss = 2.082071542739868\n",
            "step 9470: loss = 2.1395325660705566\n",
            "step 9471: loss = 1.8411895036697388\n",
            "step 9472: loss = 2.057253360748291\n",
            "step 9473: loss = 2.1996967792510986\n",
            "step 9474: loss = 2.150911808013916\n",
            "step 9475: loss = 1.9576280117034912\n",
            "step 9476: loss = 1.8313478231430054\n",
            "step 9477: loss = 1.9816521406173706\n",
            "step 9478: loss = 2.0437021255493164\n",
            "step 9479: loss = 2.114539861679077\n",
            "step 9480: loss = 1.849876046180725\n",
            "step 9481: loss = 1.998191237449646\n",
            "step 9482: loss = 2.080291271209717\n",
            "step 9483: loss = 1.921120285987854\n",
            "step 9484: loss = 1.9499754905700684\n",
            "step 9485: loss = 2.155507802963257\n",
            "step 9486: loss = 2.151522397994995\n",
            "step 9487: loss = 2.0094704627990723\n",
            "step 9488: loss = 1.9838582277297974\n",
            "step 9489: loss = 1.8870830535888672\n",
            "step 9490: loss = 2.0232737064361572\n",
            "step 9491: loss = 1.8175806999206543\n",
            "step 9492: loss = 1.9136461019515991\n",
            "step 9493: loss = 2.1415579319000244\n",
            "step 9494: loss = 2.020897626876831\n",
            "step 9495: loss = 1.9826245307922363\n",
            "step 9496: loss = 1.9632697105407715\n",
            "step 9497: loss = 1.9173715114593506\n",
            "step 9498: loss = 2.157209634780884\n",
            "step 9499: loss = 1.9756842851638794\n",
            "step 9500: loss = 1.9114091396331787\n",
            "step 9501: loss = 1.9331852197647095\n",
            "step 9502: loss = 2.0145251750946045\n",
            "step 9503: loss = 2.118772506713867\n",
            "step 9504: loss = 1.8591784238815308\n",
            "step 9505: loss = 1.9451112747192383\n",
            "step 9506: loss = 1.940510869026184\n",
            "step 9507: loss = 1.9760055541992188\n",
            "step 9508: loss = 2.0279157161712646\n",
            "step 9509: loss = 1.9755457639694214\n",
            "step 9510: loss = 1.8530617952346802\n",
            "step 9511: loss = 1.8435441255569458\n",
            "step 9512: loss = 1.8170863389968872\n",
            "step 9513: loss = 1.9864311218261719\n",
            "step 9514: loss = 2.1161558628082275\n",
            "step 9515: loss = 2.130878448486328\n",
            "step 9516: loss = 1.9506595134735107\n",
            "step 9517: loss = 2.1664857864379883\n",
            "step 9518: loss = 2.100358009338379\n",
            "step 9519: loss = 2.038262367248535\n",
            "step 9520: loss = 1.8898380994796753\n",
            "step 9521: loss = 1.9831770658493042\n",
            "step 9522: loss = 2.0155696868896484\n",
            "step 9523: loss = 1.9794092178344727\n",
            "step 9524: loss = 2.02227783203125\n",
            "step 9525: loss = 2.2351484298706055\n",
            "step 9526: loss = 2.052415609359741\n",
            "step 9527: loss = 2.0940356254577637\n",
            "step 9528: loss = 2.2481985092163086\n",
            "step 9529: loss = 2.1679060459136963\n",
            "step 9530: loss = 2.0423622131347656\n",
            "step 9531: loss = 2.067901134490967\n",
            "step 9532: loss = 1.9352715015411377\n",
            "step 9533: loss = 2.1360883712768555\n",
            "step 9534: loss = 1.8701751232147217\n",
            "step 9535: loss = 2.2285306453704834\n",
            "step 9536: loss = 1.951958417892456\n",
            "step 9537: loss = 2.1024038791656494\n",
            "step 9538: loss = 2.057771921157837\n",
            "step 9539: loss = 2.0513296127319336\n",
            "step 9540: loss = 1.8884609937667847\n",
            "step 9541: loss = 1.7633411884307861\n",
            "step 9542: loss = 2.181347131729126\n",
            "step 9543: loss = 1.9838820695877075\n",
            "step 9544: loss = 2.0065698623657227\n",
            "step 9545: loss = 2.276836395263672\n",
            "step 9546: loss = 2.199791669845581\n",
            "step 9547: loss = 1.8863354921340942\n",
            "step 9548: loss = 2.0812909603118896\n",
            "step 9549: loss = 2.254215717315674\n",
            "step 9550: loss = 1.851976752281189\n",
            "step 9551: loss = 1.977021336555481\n",
            "step 9552: loss = 2.0164053440093994\n",
            "step 9553: loss = 2.082087993621826\n",
            "step 9554: loss = 2.0942375659942627\n",
            "step 9555: loss = 2.0380654335021973\n",
            "step 9556: loss = 2.133483409881592\n",
            "step 9557: loss = 1.9898985624313354\n",
            "step 9558: loss = 1.9507116079330444\n",
            "step 9559: loss = 2.136943817138672\n",
            "step 9560: loss = 2.1155338287353516\n",
            "step 9561: loss = 2.0062942504882812\n",
            "step 9562: loss = 2.172389507293701\n",
            "step 9563: loss = 2.0516693592071533\n",
            "step 9564: loss = 2.2989468574523926\n",
            "step 9565: loss = 1.9940004348754883\n",
            "step 9566: loss = 2.164069890975952\n",
            "step 9567: loss = 1.9912382364273071\n",
            "step 9568: loss = 2.1362977027893066\n",
            "step 9569: loss = 2.053591012954712\n",
            "step 9570: loss = 2.0889973640441895\n",
            "step 9571: loss = 2.1279568672180176\n",
            "step 9572: loss = 2.0481228828430176\n",
            "step 9573: loss = 2.069791555404663\n",
            "step 9574: loss = 1.9504340887069702\n",
            "step 9575: loss = 2.300795078277588\n",
            "step 9576: loss = 1.9568285942077637\n",
            "step 9577: loss = 2.0971264839172363\n",
            "step 9578: loss = 1.989276647567749\n",
            "step 9579: loss = 1.8876608610153198\n",
            "step 9580: loss = 2.2010669708251953\n",
            "step 9581: loss = 2.0926945209503174\n",
            "step 9582: loss = 2.23384690284729\n",
            "step 9583: loss = 1.9251434803009033\n",
            "step 9584: loss = 2.156071662902832\n",
            "step 9585: loss = 1.9141861200332642\n",
            "step 9586: loss = 2.017282009124756\n",
            "step 9587: loss = 2.0105600357055664\n",
            "step 9588: loss = 1.821409821510315\n",
            "step 9589: loss = 2.1962757110595703\n",
            "step 9590: loss = 1.9129406213760376\n",
            "step 9591: loss = 1.9750778675079346\n",
            "step 9592: loss = 2.024853467941284\n",
            "step 9593: loss = 1.731606364250183\n",
            "step 9594: loss = 2.1048667430877686\n",
            "step 9595: loss = 1.9527314901351929\n",
            "step 9596: loss = 2.1753549575805664\n",
            "step 9597: loss = 2.0463857650756836\n",
            "step 9598: loss = 2.0307347774505615\n",
            "step 9599: loss = 1.9238868951797485\n",
            "step 9600: loss = 1.971510648727417\n",
            "step 9601: loss = 1.9194892644882202\n",
            "step 9602: loss = 1.941023349761963\n",
            "step 9603: loss = 1.902535319328308\n",
            "step 9604: loss = 2.3115272521972656\n",
            "step 9605: loss = 2.1112000942230225\n",
            "step 9606: loss = 2.0005483627319336\n",
            "step 9607: loss = 1.995070457458496\n",
            "step 9608: loss = 1.885945200920105\n",
            "step 9609: loss = 2.183303117752075\n",
            "step 9610: loss = 2.007390022277832\n",
            "step 9611: loss = 1.9998348951339722\n",
            "step 9612: loss = 1.9395167827606201\n",
            "step 9613: loss = 2.1023685932159424\n",
            "step 9614: loss = 2.005312919616699\n",
            "step 9615: loss = 1.9799473285675049\n",
            "step 9616: loss = 1.8865703344345093\n",
            "step 9617: loss = 2.128256320953369\n",
            "step 9618: loss = 2.024416208267212\n",
            "step 9619: loss = 2.0807580947875977\n",
            "step 9620: loss = 1.9860587120056152\n",
            "step 9621: loss = 1.9398853778839111\n",
            "step 9622: loss = 2.0924856662750244\n",
            "step 9623: loss = 1.9060550928115845\n",
            "step 9624: loss = 1.9518029689788818\n",
            "step 9625: loss = 1.7638309001922607\n",
            "step 9626: loss = 2.2019221782684326\n",
            "step 9627: loss = 1.9628922939300537\n",
            "step 9628: loss = 2.1115260124206543\n",
            "step 9629: loss = 2.015625\n",
            "step 9630: loss = 2.0154712200164795\n",
            "step 9631: loss = 2.154207229614258\n",
            "step 9632: loss = 2.2066400051116943\n",
            "step 9633: loss = 1.90193510055542\n",
            "step 9634: loss = 1.9983400106430054\n",
            "step 9635: loss = 1.8929139375686646\n",
            "step 9636: loss = 2.053168535232544\n",
            "step 9637: loss = 2.0145812034606934\n",
            "step 9638: loss = 2.1713507175445557\n",
            "step 9639: loss = 1.979332685470581\n",
            "step 9640: loss = 2.041907787322998\n",
            "step 9641: loss = 2.138698101043701\n",
            "step 9642: loss = 2.08733868598938\n",
            "step 9643: loss = 2.215327501296997\n",
            "step 9644: loss = 2.2074105739593506\n",
            "step 9645: loss = 1.9462810754776\n",
            "step 9646: loss = 2.0749709606170654\n",
            "step 9647: loss = 2.1709094047546387\n",
            "step 9648: loss = 1.8015127182006836\n",
            "step 9649: loss = 1.88971745967865\n",
            "step 9650: loss = 2.1114277839660645\n",
            "step 9651: loss = 2.1103193759918213\n",
            "step 9652: loss = 2.2666685581207275\n",
            "step 9653: loss = 2.073200225830078\n",
            "step 9654: loss = 2.0009562969207764\n",
            "step 9655: loss = 1.885553240776062\n",
            "step 9656: loss = 2.272653102874756\n",
            "step 9657: loss = 1.9557931423187256\n",
            "step 9658: loss = 1.9571083784103394\n",
            "step 9659: loss = 2.082420587539673\n",
            "step 9660: loss = 1.9399378299713135\n",
            "step 9661: loss = 2.0369672775268555\n",
            "step 9662: loss = 2.0877108573913574\n",
            "step 9663: loss = 2.0931999683380127\n",
            "step 9664: loss = 1.8568366765975952\n",
            "step 9665: loss = 2.0471649169921875\n",
            "step 9666: loss = 2.2215073108673096\n",
            "step 9667: loss = 2.2189793586730957\n",
            "step 9668: loss = 2.0639917850494385\n",
            "step 9669: loss = 2.039050579071045\n",
            "step 9670: loss = 2.1233174800872803\n",
            "step 9671: loss = 2.0628254413604736\n",
            "step 9672: loss = 1.99546217918396\n",
            "step 9673: loss = 2.106912136077881\n",
            "step 9674: loss = 1.8300061225891113\n",
            "step 9675: loss = 1.9909577369689941\n",
            "step 9676: loss = 2.047602891921997\n",
            "step 9677: loss = 1.9856981039047241\n",
            "step 9678: loss = 2.128946304321289\n",
            "step 9679: loss = 2.2061920166015625\n",
            "step 9680: loss = 1.9159202575683594\n",
            "step 9681: loss = 2.117708206176758\n",
            "step 9682: loss = 1.947719693183899\n",
            "step 9683: loss = 2.1443467140197754\n",
            "step 9684: loss = 1.733527421951294\n",
            "step 9685: loss = 1.8704456090927124\n",
            "step 9686: loss = 1.8254443407058716\n",
            "step 9687: loss = 1.9823042154312134\n",
            "step 9688: loss = 2.3587470054626465\n",
            "step 9689: loss = 2.2678961753845215\n",
            "step 9690: loss = 2.1398468017578125\n",
            "step 9691: loss = 2.1647329330444336\n",
            "step 9692: loss = 1.8733782768249512\n",
            "step 9693: loss = 2.0120532512664795\n",
            "step 9694: loss = 2.022648334503174\n",
            "step 9695: loss = 1.9926154613494873\n",
            "step 9696: loss = 1.9780761003494263\n",
            "step 9697: loss = 2.0667648315429688\n",
            "step 9698: loss = 1.9700579643249512\n",
            "step 9699: loss = 2.2442145347595215\n",
            "step 9700: loss = 2.1107723712921143\n",
            "step 9701: loss = 2.2424356937408447\n",
            "step 9702: loss = 2.009148597717285\n",
            "step 9703: loss = 1.871105432510376\n",
            "step 9704: loss = 2.2900640964508057\n",
            "step 9705: loss = 2.139084577560425\n",
            "step 9706: loss = 2.108311653137207\n",
            "step 9707: loss = 1.7417441606521606\n",
            "step 9708: loss = 2.1600494384765625\n",
            "step 9709: loss = 1.8590848445892334\n",
            "step 9710: loss = 2.2370519638061523\n",
            "step 9711: loss = 1.9556139707565308\n",
            "step 9712: loss = 2.0550038814544678\n",
            "step 9713: loss = 2.197160243988037\n",
            "step 9714: loss = 2.060793399810791\n",
            "step 9715: loss = 1.962892770767212\n",
            "step 9716: loss = 2.159654378890991\n",
            "step 9717: loss = 2.160146951675415\n",
            "step 9718: loss = 2.1386311054229736\n",
            "step 9719: loss = 2.2128705978393555\n",
            "step 9720: loss = 2.092341423034668\n",
            "step 9721: loss = 2.00740647315979\n",
            "step 9722: loss = 2.086796283721924\n",
            "step 9723: loss = 2.1485204696655273\n",
            "step 9724: loss = 2.1085877418518066\n",
            "step 9725: loss = 2.118694305419922\n",
            "step 9726: loss = 2.1365854740142822\n",
            "step 9727: loss = 2.0559914112091064\n",
            "step 9728: loss = 1.9992156028747559\n",
            "step 9729: loss = 2.0330023765563965\n",
            "step 9730: loss = 1.9282010793685913\n",
            "step 9731: loss = 2.109785795211792\n",
            "step 9732: loss = 2.1634252071380615\n",
            "step 9733: loss = 2.129572868347168\n",
            "step 9734: loss = 2.180065870285034\n",
            "step 9735: loss = 2.094691514968872\n",
            "step 9736: loss = 2.0960118770599365\n",
            "step 9737: loss = 1.9405730962753296\n",
            "step 9738: loss = 1.9137121438980103\n",
            "step 9739: loss = 1.98661470413208\n",
            "step 9740: loss = 2.2166852951049805\n",
            "step 9741: loss = 1.9624375104904175\n",
            "step 9742: loss = 1.9160308837890625\n",
            "step 9743: loss = 2.124711513519287\n",
            "step 9744: loss = 1.943608283996582\n",
            "step 9745: loss = 2.032146692276001\n",
            "step 9746: loss = 1.9517484903335571\n",
            "step 9747: loss = 2.068058729171753\n",
            "step 9748: loss = 2.2270631790161133\n",
            "step 9749: loss = 1.8610124588012695\n",
            "step 9750: loss = 2.258420944213867\n",
            "step 9751: loss = 2.0834434032440186\n",
            "step 9752: loss = 2.0346457958221436\n",
            "step 9753: loss = 2.1709110736846924\n",
            "step 9754: loss = 2.0140023231506348\n",
            "step 9755: loss = 2.1469383239746094\n",
            "step 9756: loss = 2.3306353092193604\n",
            "step 9757: loss = 2.1443967819213867\n",
            "step 9758: loss = 1.8953913450241089\n",
            "step 9759: loss = 2.1031904220581055\n",
            "step 9760: loss = 2.0410778522491455\n",
            "step 9761: loss = 1.9055026769638062\n",
            "step 9762: loss = 2.2737884521484375\n",
            "step 9763: loss = 1.9948996305465698\n",
            "step 9764: loss = 1.8729313611984253\n",
            "step 9765: loss = 2.198115825653076\n",
            "step 9766: loss = 2.3097939491271973\n",
            "step 9767: loss = 1.805046796798706\n",
            "step 9768: loss = 2.007092237472534\n",
            "step 9769: loss = 2.1394033432006836\n",
            "step 9770: loss = 2.1095340251922607\n",
            "step 9771: loss = 2.0433669090270996\n",
            "step 9772: loss = 1.988429307937622\n",
            "step 9773: loss = 2.19185209274292\n",
            "step 9774: loss = 1.939322590827942\n",
            "step 9775: loss = 2.1015803813934326\n",
            "step 9776: loss = 2.120872974395752\n",
            "step 9777: loss = 2.151294469833374\n",
            "step 9778: loss = 2.0558183193206787\n",
            "step 9779: loss = 1.9075992107391357\n",
            "step 9780: loss = 2.2286062240600586\n",
            "step 9781: loss = 2.390237331390381\n",
            "step 9782: loss = 2.056638479232788\n",
            "step 9783: loss = 2.058635950088501\n",
            "step 9784: loss = 2.2254910469055176\n",
            "step 9785: loss = 1.8640484809875488\n",
            "step 9786: loss = 2.2078254222869873\n",
            "step 9787: loss = 1.9478520154953003\n",
            "step 9788: loss = 2.053511142730713\n",
            "step 9789: loss = 2.0319793224334717\n",
            "step 9790: loss = 2.1252453327178955\n",
            "step 9791: loss = 1.954962968826294\n",
            "step 9792: loss = 2.0394680500030518\n",
            "step 9793: loss = 2.264613389968872\n",
            "step 9794: loss = 1.955236554145813\n",
            "step 9795: loss = 1.9208179712295532\n",
            "step 9796: loss = 2.090578079223633\n",
            "step 9797: loss = 1.9664174318313599\n",
            "step 9798: loss = 2.25303316116333\n",
            "step 9799: loss = 2.2508583068847656\n",
            "step 9800: loss = 2.202518939971924\n",
            "step 9801: loss = 2.265829086303711\n",
            "step 9802: loss = 2.4620254039764404\n",
            "step 9803: loss = 2.1271555423736572\n",
            "step 9804: loss = 2.1028854846954346\n",
            "step 9805: loss = 2.2391858100891113\n",
            "step 9806: loss = 1.978023648262024\n",
            "step 9807: loss = 2.1691665649414062\n",
            "step 9808: loss = 2.247201442718506\n",
            "step 9809: loss = 2.2381722927093506\n",
            "step 9810: loss = 2.2720868587493896\n",
            "step 9811: loss = 2.201211929321289\n",
            "step 9812: loss = 1.9004653692245483\n",
            "step 9813: loss = 2.1080071926116943\n",
            "step 9814: loss = 2.188371181488037\n",
            "step 9815: loss = 2.181877374649048\n",
            "step 9816: loss = 2.193953037261963\n",
            "step 9817: loss = 2.1071481704711914\n",
            "step 9818: loss = 2.0777182579040527\n",
            "step 9819: loss = 1.9965839385986328\n",
            "step 9820: loss = 2.0588796138763428\n",
            "step 9821: loss = 2.0775318145751953\n",
            "step 9822: loss = 2.047025203704834\n",
            "step 9823: loss = 2.042531967163086\n",
            "step 9824: loss = 1.8137283325195312\n",
            "step 9825: loss = 2.0011112689971924\n",
            "step 9826: loss = 2.0717387199401855\n",
            "step 9827: loss = 2.069197416305542\n",
            "step 9828: loss = 2.291501522064209\n",
            "step 9829: loss = 1.9088966846466064\n",
            "step 9830: loss = 2.172830104827881\n",
            "step 9831: loss = 2.118839740753174\n",
            "step 9832: loss = 2.076951265335083\n",
            "step 9833: loss = 2.149550199508667\n",
            "step 9834: loss = 2.093264102935791\n",
            "step 9835: loss = 2.3510146141052246\n",
            "step 9836: loss = 2.072067975997925\n",
            "step 9837: loss = 2.1804585456848145\n",
            "step 9838: loss = 2.3094258308410645\n",
            "step 9839: loss = 2.0528368949890137\n",
            "step 9840: loss = 2.3420186042785645\n",
            "step 9841: loss = 2.0371193885803223\n",
            "step 9842: loss = 2.198448657989502\n",
            "step 9843: loss = 2.1872384548187256\n",
            "step 9844: loss = 2.0160303115844727\n",
            "step 9845: loss = 2.088352918624878\n",
            "step 9846: loss = 1.8402093648910522\n",
            "step 9847: loss = 1.8394267559051514\n",
            "step 9848: loss = 1.8502826690673828\n",
            "step 9849: loss = 1.9454869031906128\n",
            "step 9850: loss = 2.087120532989502\n",
            "step 9851: loss = 2.0042099952697754\n",
            "step 9852: loss = 2.1154978275299072\n",
            "step 9853: loss = 1.844722867012024\n",
            "step 9854: loss = 2.0879318714141846\n",
            "step 9855: loss = 2.174290418624878\n",
            "step 9856: loss = 1.8094953298568726\n",
            "step 9857: loss = 2.135542392730713\n",
            "step 9858: loss = 2.048790454864502\n",
            "step 9859: loss = 2.2201719284057617\n",
            "step 9860: loss = 2.1980960369110107\n",
            "step 9861: loss = 2.3035213947296143\n",
            "step 9862: loss = 2.096261739730835\n",
            "step 9863: loss = 2.0924160480499268\n",
            "step 9864: loss = 1.957213044166565\n",
            "step 9865: loss = 2.291778326034546\n",
            "step 9866: loss = 2.1501355171203613\n",
            "step 9867: loss = 2.1701159477233887\n",
            "step 9868: loss = 1.8900576829910278\n",
            "step 9869: loss = 2.1789493560791016\n",
            "step 9870: loss = 2.1666338443756104\n",
            "step 9871: loss = 2.095945358276367\n",
            "step 9872: loss = 2.1034646034240723\n",
            "step 9873: loss = 2.155531406402588\n",
            "step 9874: loss = 2.2095746994018555\n",
            "step 9875: loss = 2.2486612796783447\n",
            "step 9876: loss = 1.9798212051391602\n",
            "step 9877: loss = 2.0191009044647217\n",
            "step 9878: loss = 2.073443651199341\n",
            "step 9879: loss = 2.0830819606781006\n",
            "step 9880: loss = 1.9383858442306519\n",
            "step 9881: loss = 2.059126615524292\n",
            "step 9882: loss = 2.1555964946746826\n",
            "step 9883: loss = 2.405587911605835\n",
            "step 9884: loss = 2.1422719955444336\n",
            "step 9885: loss = 2.2896416187286377\n",
            "step 9886: loss = 2.2880918979644775\n",
            "step 9887: loss = 2.1182005405426025\n",
            "step 9888: loss = 2.1714229583740234\n",
            "step 9889: loss = 1.8847260475158691\n",
            "step 9890: loss = 2.221187114715576\n",
            "step 9891: loss = 2.2676918506622314\n",
            "step 9892: loss = 2.1212637424468994\n",
            "step 9893: loss = 2.1455607414245605\n",
            "step 9894: loss = 2.304765224456787\n",
            "step 9895: loss = 2.217413902282715\n",
            "step 9896: loss = 2.1795973777770996\n",
            "step 9897: loss = 2.2599940299987793\n",
            "step 9898: loss = 2.3568646907806396\n",
            "step 9899: loss = 2.1234447956085205\n",
            "step 9900: loss = 2.2211434841156006\n",
            "step 9901: loss = 2.213503122329712\n",
            "step 9902: loss = 2.2534759044647217\n",
            "step 9903: loss = 1.8668673038482666\n",
            "step 9904: loss = 2.0365312099456787\n",
            "step 9905: loss = 1.962999939918518\n",
            "step 9906: loss = 1.8191555738449097\n",
            "step 9907: loss = 2.1860320568084717\n",
            "step 9908: loss = 1.9181182384490967\n",
            "step 9909: loss = 1.8861602544784546\n",
            "step 9910: loss = 1.9962141513824463\n",
            "step 9911: loss = 2.133345127105713\n",
            "step 9912: loss = 2.3095340728759766\n",
            "step 9913: loss = 1.9167758226394653\n",
            "step 9914: loss = 2.1453421115875244\n",
            "step 9915: loss = 2.2106099128723145\n",
            "step 9916: loss = 2.12650203704834\n",
            "step 9917: loss = 2.112204074859619\n",
            "step 9918: loss = 2.3319475650787354\n",
            "step 9919: loss = 2.095024824142456\n",
            "step 9920: loss = 2.0717122554779053\n",
            "step 9921: loss = 1.9842989444732666\n",
            "step 9922: loss = 2.271548271179199\n",
            "step 9923: loss = 2.099642276763916\n",
            "step 9924: loss = 1.9619395732879639\n",
            "step 9925: loss = 2.0628902912139893\n",
            "step 9926: loss = 2.126105308532715\n",
            "step 9927: loss = 2.011558771133423\n",
            "step 9928: loss = 2.1657180786132812\n",
            "step 9929: loss = 1.9074736833572388\n",
            "step 9930: loss = 2.016026496887207\n",
            "step 9931: loss = 2.415705680847168\n",
            "step 9932: loss = 1.9926129579544067\n",
            "step 9933: loss = 1.7998888492584229\n",
            "step 9934: loss = 2.0610315799713135\n",
            "step 9935: loss = 2.161515712738037\n",
            "step 9936: loss = 2.0773775577545166\n",
            "step 9937: loss = 2.0064139366149902\n",
            "step 9938: loss = 1.8435091972351074\n",
            "step 9939: loss = 2.039095640182495\n",
            "step 9940: loss = 2.3123908042907715\n",
            "step 9941: loss = 1.8862251043319702\n",
            "step 9942: loss = 2.0610761642456055\n",
            "step 9943: loss = 2.230193853378296\n",
            "step 9944: loss = 2.3597404956817627\n",
            "step 9945: loss = 2.281595468521118\n",
            "step 9946: loss = 2.4008827209472656\n",
            "step 9947: loss = 2.18780517578125\n",
            "step 9948: loss = 2.047778844833374\n",
            "step 9949: loss = 2.226987600326538\n",
            "step 9950: loss = 2.0730373859405518\n",
            "step 9951: loss = 2.041262626647949\n",
            "step 9952: loss = 2.113898992538452\n",
            "step 9953: loss = 2.0257160663604736\n",
            "step 9954: loss = 2.362436532974243\n",
            "step 9955: loss = 2.14406156539917\n",
            "step 9956: loss = 2.2111222743988037\n",
            "step 9957: loss = 2.1362650394439697\n",
            "step 9958: loss = 2.0582356452941895\n",
            "step 9959: loss = 2.2077624797821045\n",
            "step 9960: loss = 2.2113113403320312\n",
            "step 9961: loss = 2.0867655277252197\n",
            "step 9962: loss = 2.1283600330352783\n",
            "step 9963: loss = 1.928006887435913\n",
            "step 9964: loss = 1.824738621711731\n",
            "step 9965: loss = 2.270002841949463\n",
            "step 9966: loss = 2.282904624938965\n",
            "step 9967: loss = 2.1329197883605957\n",
            "step 9968: loss = 2.2342190742492676\n",
            "step 9969: loss = 1.9019376039505005\n",
            "step 9970: loss = 2.2716827392578125\n",
            "step 9971: loss = 2.0442590713500977\n",
            "step 9972: loss = 1.979662537574768\n",
            "step 9973: loss = 1.8362973928451538\n",
            "step 9974: loss = 2.1337597370147705\n",
            "step 9975: loss = 2.3471548557281494\n",
            "step 9976: loss = 2.219183921813965\n",
            "step 9977: loss = 2.3619799613952637\n",
            "step 9978: loss = 1.9651031494140625\n",
            "step 9979: loss = 2.3559646606445312\n",
            "step 9980: loss = 2.0754776000976562\n",
            "step 9981: loss = 2.32895565032959\n",
            "step 9982: loss = 2.129127264022827\n",
            "step 9983: loss = 2.2351503372192383\n",
            "step 9984: loss = 2.2559831142425537\n",
            "step 9985: loss = 2.0030202865600586\n",
            "step 9986: loss = 2.105729341506958\n",
            "step 9987: loss = 2.2773020267486572\n",
            "step 9988: loss = 2.2232911586761475\n",
            "step 9989: loss = 2.1638917922973633\n",
            "step 9990: loss = 2.218921184539795\n",
            "step 9991: loss = 2.2412571907043457\n",
            "step 9992: loss = 2.03674054145813\n",
            "step 9993: loss = 1.997183084487915\n",
            "step 9994: loss = 1.9593884944915771\n",
            "step 9995: loss = 1.9042340517044067\n",
            "step 9996: loss = 2.1983134746551514\n",
            "step 9997: loss = 1.8925198316574097\n",
            "step 9998: loss = 2.032769203186035\n",
            "step 9999: loss = 2.2539379596710205\n",
            "step 10000: loss = 2.1365628242492676\n",
            "step 10001: loss = 2.3968088626861572\n",
            "step 10002: loss = 2.1468772888183594\n",
            "step 10003: loss = 2.4630002975463867\n",
            "step 10004: loss = 2.17852783203125\n",
            "step 10005: loss = 2.0966203212738037\n",
            "step 10006: loss = 2.1794071197509766\n",
            "step 10007: loss = 2.0278403759002686\n",
            "step 10008: loss = 2.221815586090088\n",
            "step 10009: loss = 2.4069461822509766\n",
            "step 10010: loss = 2.191540479660034\n",
            "step 10011: loss = 2.1439337730407715\n",
            "step 10012: loss = 2.3235888481140137\n",
            "step 10013: loss = 2.36784291267395\n",
            "step 10014: loss = 2.2393951416015625\n",
            "step 10015: loss = 2.06748104095459\n",
            "step 10016: loss = 2.1130452156066895\n",
            "step 10017: loss = 2.1888554096221924\n",
            "step 10018: loss = 2.264219284057617\n",
            "step 10019: loss = 2.160583972930908\n",
            "step 10020: loss = 2.254744291305542\n",
            "step 10021: loss = 2.2224318981170654\n",
            "step 10022: loss = 2.168426752090454\n",
            "step 10023: loss = 2.11305832862854\n",
            "step 10024: loss = 2.366035223007202\n",
            "step 10025: loss = 2.008155345916748\n",
            "step 10026: loss = 2.330244541168213\n",
            "step 10027: loss = 1.9968537092208862\n",
            "step 10028: loss = 2.113949775695801\n",
            "step 10029: loss = 1.9846365451812744\n",
            "step 10030: loss = 2.151381254196167\n",
            "step 10031: loss = 2.031263828277588\n",
            "step 10032: loss = 2.040649652481079\n",
            "step 10033: loss = 1.9910647869110107\n",
            "step 10034: loss = 2.276627779006958\n",
            "step 10035: loss = 2.1233251094818115\n",
            "step 10036: loss = 2.166271686553955\n",
            "step 10037: loss = 2.1176974773406982\n",
            "step 10038: loss = 2.135202169418335\n",
            "step 10039: loss = 1.954618215560913\n",
            "step 10040: loss = 2.3655383586883545\n",
            "step 10041: loss = 2.244072437286377\n",
            "step 10042: loss = 1.9174154996871948\n",
            "step 10043: loss = 2.1407530307769775\n",
            "step 10044: loss = 2.0471742153167725\n",
            "step 10045: loss = 2.1659767627716064\n",
            "step 10046: loss = 2.121936559677124\n",
            "step 10047: loss = 2.295591115951538\n",
            "step 10048: loss = 2.020374059677124\n",
            "step 10049: loss = 2.2857353687286377\n",
            "step 10050: loss = 2.1756818294525146\n",
            "step 10051: loss = 1.913261890411377\n",
            "step 10052: loss = 2.289477825164795\n",
            "step 10053: loss = 2.0570812225341797\n",
            "step 10054: loss = 2.1040046215057373\n",
            "step 10055: loss = 2.174785852432251\n",
            "step 10056: loss = 2.1297099590301514\n",
            "step 10057: loss = 2.045257568359375\n",
            "step 10058: loss = 2.1234049797058105\n",
            "step 10059: loss = 1.9784482717514038\n",
            "step 10060: loss = 2.132859706878662\n",
            "step 10061: loss = 1.9806560277938843\n",
            "step 10062: loss = 2.2477364540100098\n",
            "step 10063: loss = 2.185732364654541\n",
            "step 10064: loss = 2.2933402061462402\n",
            "step 10065: loss = 2.5025157928466797\n",
            "step 10066: loss = 2.273679256439209\n",
            "step 10067: loss = 2.144648551940918\n",
            "step 10068: loss = 2.2072465419769287\n",
            "step 10069: loss = 1.905340552330017\n",
            "step 10070: loss = 2.274778127670288\n",
            "step 10071: loss = 2.1641175746917725\n",
            "step 10072: loss = 2.177543878555298\n",
            "step 10073: loss = 1.966951847076416\n",
            "step 10074: loss = 2.06878662109375\n",
            "step 10075: loss = 2.0450265407562256\n",
            "step 10076: loss = 2.274805784225464\n",
            "step 10077: loss = 2.480003595352173\n",
            "step 10078: loss = 2.1499691009521484\n",
            "step 10079: loss = 2.0674283504486084\n",
            "step 10080: loss = 2.1832668781280518\n",
            "step 10081: loss = 2.1782655715942383\n",
            "step 10082: loss = 2.219672679901123\n",
            "step 10083: loss = 2.1224589347839355\n",
            "step 10084: loss = 2.0677342414855957\n",
            "step 10085: loss = 2.4303534030914307\n",
            "step 10086: loss = 1.960988163948059\n",
            "step 10087: loss = 2.1833813190460205\n",
            "step 10088: loss = 2.225034713745117\n",
            "step 10089: loss = 2.0783028602600098\n",
            "step 10090: loss = 2.0406296253204346\n",
            "step 10091: loss = 2.1403050422668457\n",
            "step 10092: loss = 1.982987642288208\n",
            "step 10093: loss = 2.1051948070526123\n",
            "step 10094: loss = 2.1868858337402344\n",
            "step 10095: loss = 1.9963475465774536\n",
            "step 10096: loss = 1.8755842447280884\n",
            "step 10097: loss = 2.33170747756958\n",
            "step 10098: loss = 2.2640180587768555\n",
            "step 10099: loss = 1.921250820159912\n",
            "step 10100: loss = 2.1719858646392822\n",
            "step 10101: loss = 1.8621577024459839\n",
            "step 10102: loss = 2.069737195968628\n",
            "step 10103: loss = 2.0339341163635254\n",
            "step 10104: loss = 2.630056381225586\n",
            "step 10105: loss = 2.1605894565582275\n",
            "step 10106: loss = 2.2323014736175537\n",
            "step 10107: loss = 2.015324354171753\n",
            "step 10108: loss = 2.0341567993164062\n",
            "step 10109: loss = 1.9897806644439697\n",
            "step 10110: loss = 2.103895664215088\n",
            "step 10111: loss = 2.0131053924560547\n",
            "step 10112: loss = 2.078925132751465\n",
            "step 10113: loss = 2.2475123405456543\n",
            "step 10114: loss = 2.2951033115386963\n",
            "step 10115: loss = 2.077500104904175\n",
            "step 10116: loss = 2.3679096698760986\n",
            "step 10117: loss = 2.3568978309631348\n",
            "step 10118: loss = 2.019627571105957\n",
            "step 10119: loss = 2.0972561836242676\n",
            "step 10120: loss = 2.1445651054382324\n",
            "step 10121: loss = 2.209376335144043\n",
            "step 10122: loss = 2.1145312786102295\n",
            "step 10123: loss = 2.103344440460205\n",
            "step 10124: loss = 2.194148063659668\n",
            "step 10125: loss = 2.130605936050415\n",
            "step 10126: loss = 2.1716370582580566\n",
            "step 10127: loss = 2.069554090499878\n",
            "step 10128: loss = 2.1760401725769043\n",
            "step 10129: loss = 2.1084158420562744\n",
            "step 10130: loss = 2.0055882930755615\n",
            "step 10131: loss = 2.236159086227417\n",
            "step 10132: loss = 2.0655853748321533\n",
            "step 10133: loss = 2.0477454662323\n",
            "step 10134: loss = 2.038782835006714\n",
            "step 10135: loss = 2.435626983642578\n",
            "step 10136: loss = 2.0632481575012207\n",
            "step 10137: loss = 1.964506983757019\n",
            "step 10138: loss = 2.134239673614502\n",
            "step 10139: loss = 2.1326308250427246\n",
            "step 10140: loss = 2.110809087753296\n",
            "step 10141: loss = 2.2608137130737305\n",
            "step 10142: loss = 2.109578847885132\n",
            "step 10143: loss = 2.074827194213867\n",
            "step 10144: loss = 2.2081634998321533\n",
            "step 10145: loss = 2.1311144828796387\n",
            "step 10146: loss = 2.1762678623199463\n",
            "step 10147: loss = 1.9082531929016113\n",
            "step 10148: loss = 2.2142014503479004\n",
            "step 10149: loss = 2.0191876888275146\n",
            "step 10150: loss = 2.241811752319336\n",
            "step 10151: loss = 2.350433349609375\n",
            "step 10152: loss = 2.023747682571411\n",
            "step 10153: loss = 2.4688985347747803\n",
            "step 10154: loss = 2.081007480621338\n",
            "step 10155: loss = 2.0271189212799072\n",
            "step 10156: loss = 2.0488460063934326\n",
            "step 10157: loss = 2.178875684738159\n",
            "step 10158: loss = 2.320960760116577\n",
            "step 10159: loss = 2.3150126934051514\n",
            "step 10160: loss = 2.137883186340332\n",
            "step 10161: loss = 2.03047776222229\n",
            "step 10162: loss = 2.0643482208251953\n",
            "step 10163: loss = 1.9292829036712646\n",
            "step 10164: loss = 2.214433193206787\n",
            "step 10165: loss = 2.1785390377044678\n",
            "step 10166: loss = 2.167034387588501\n",
            "step 10167: loss = 2.3450653553009033\n",
            "step 10168: loss = 2.491363525390625\n",
            "step 10169: loss = 2.2206223011016846\n",
            "step 10170: loss = 2.1232731342315674\n",
            "step 10171: loss = 2.3045639991760254\n",
            "step 10172: loss = 1.855704426765442\n",
            "step 10173: loss = 1.9941990375518799\n",
            "step 10174: loss = 2.0211400985717773\n",
            "step 10175: loss = 2.3306167125701904\n",
            "step 10176: loss = 2.096991777420044\n",
            "step 10177: loss = 2.0644145011901855\n",
            "step 10178: loss = 2.0978355407714844\n",
            "step 10179: loss = 1.9773149490356445\n",
            "step 10180: loss = 2.3693976402282715\n",
            "step 10181: loss = 2.0482640266418457\n",
            "step 10182: loss = 2.2353081703186035\n",
            "step 10183: loss = 2.0221822261810303\n",
            "step 10184: loss = 2.215404987335205\n",
            "step 10185: loss = 2.0690739154815674\n",
            "step 10186: loss = 2.1106786727905273\n",
            "step 10187: loss = 2.180150270462036\n",
            "step 10188: loss = 2.082948923110962\n",
            "step 10189: loss = 2.235628604888916\n",
            "step 10190: loss = 2.063889741897583\n",
            "step 10191: loss = 1.9635354280471802\n",
            "step 10192: loss = 2.024548292160034\n",
            "step 10193: loss = 2.096820831298828\n",
            "step 10194: loss = 2.136219024658203\n",
            "step 10195: loss = 2.0551323890686035\n",
            "step 10196: loss = 1.9718736410140991\n",
            "step 10197: loss = 2.50797438621521\n",
            "step 10198: loss = 2.3564000129699707\n",
            "step 10199: loss = 1.9850921630859375\n",
            "step 10200: loss = 2.0533249378204346\n",
            "step 10201: loss = 2.220566749572754\n",
            "step 10202: loss = 2.191762685775757\n",
            "step 10203: loss = 2.226856231689453\n",
            "step 10204: loss = 1.8768621683120728\n",
            "step 10205: loss = 2.3164775371551514\n",
            "step 10206: loss = 2.163684606552124\n",
            "step 10207: loss = 2.194688081741333\n",
            "step 10208: loss = 2.2978034019470215\n",
            "step 10209: loss = 2.223289728164673\n",
            "step 10210: loss = 2.109652519226074\n",
            "step 10211: loss = 2.1106116771698\n",
            "step 10212: loss = 2.250823736190796\n",
            "step 10213: loss = 2.2272789478302\n",
            "step 10214: loss = 2.0823662281036377\n",
            "step 10215: loss = 1.954357385635376\n",
            "step 10216: loss = 2.0313827991485596\n",
            "step 10217: loss = 2.0140955448150635\n",
            "step 10218: loss = 2.062736749649048\n",
            "step 10219: loss = 2.047250747680664\n",
            "step 10220: loss = 2.2053494453430176\n",
            "step 10221: loss = 2.1106607913970947\n",
            "step 10222: loss = 2.2146966457366943\n",
            "step 10223: loss = 1.9706873893737793\n",
            "step 10224: loss = 2.0352094173431396\n",
            "step 10225: loss = 2.1066672801971436\n",
            "step 10226: loss = 2.1768434047698975\n",
            "step 10227: loss = 2.3790876865386963\n",
            "step 10228: loss = 1.8260186910629272\n",
            "step 10229: loss = 2.242166757583618\n",
            "step 10230: loss = 2.3611109256744385\n",
            "step 10231: loss = 2.4364233016967773\n",
            "step 10232: loss = 2.3077731132507324\n",
            "step 10233: loss = 2.3555877208709717\n",
            "step 10234: loss = 2.1564245223999023\n",
            "step 10235: loss = 1.974045991897583\n",
            "step 10236: loss = 2.3103246688842773\n",
            "step 10237: loss = 2.299607753753662\n",
            "step 10238: loss = 2.028766393661499\n",
            "step 10239: loss = 2.2549376487731934\n",
            "step 10240: loss = 2.417268753051758\n",
            "step 10241: loss = 2.076535940170288\n",
            "step 10242: loss = 2.2491519451141357\n",
            "step 10243: loss = 2.006284713745117\n",
            "step 10244: loss = 2.169557809829712\n",
            "step 10245: loss = 2.221827745437622\n",
            "step 10246: loss = 2.2485105991363525\n",
            "step 10247: loss = 2.2135558128356934\n",
            "step 10248: loss = 1.9904166460037231\n",
            "step 10249: loss = 2.2777087688446045\n",
            "step 10250: loss = 1.9047566652297974\n",
            "step 10251: loss = 2.058058500289917\n",
            "step 10252: loss = 2.2232072353363037\n",
            "step 10253: loss = 2.02964186668396\n",
            "step 10254: loss = 2.1570651531219482\n",
            "step 10255: loss = 2.3185360431671143\n",
            "step 10256: loss = 2.1014564037323\n",
            "step 10257: loss = 2.079272508621216\n",
            "step 10258: loss = 2.1521224975585938\n",
            "step 10259: loss = 2.4092319011688232\n",
            "step 10260: loss = 1.9294734001159668\n",
            "step 10261: loss = 2.268106698989868\n",
            "step 10262: loss = 2.4864602088928223\n",
            "step 10263: loss = 2.041300058364868\n",
            "step 10264: loss = 2.3478519916534424\n",
            "step 10265: loss = 2.215675115585327\n",
            "step 10266: loss = 2.2293283939361572\n",
            "step 10267: loss = 1.9867565631866455\n",
            "step 10268: loss = 1.9091205596923828\n",
            "step 10269: loss = 2.289524793624878\n",
            "step 10270: loss = 1.9405733346939087\n",
            "step 10271: loss = 2.1842496395111084\n",
            "step 10272: loss = 2.0113987922668457\n",
            "step 10273: loss = 2.0338573455810547\n",
            "step 10274: loss = 2.2502501010894775\n",
            "step 10275: loss = 1.9900938272476196\n",
            "step 10276: loss = 2.3913369178771973\n",
            "step 10277: loss = 2.0470633506774902\n",
            "step 10278: loss = 2.2773914337158203\n",
            "step 10279: loss = 2.113661050796509\n",
            "step 10280: loss = 2.3515217304229736\n",
            "step 10281: loss = 2.2479169368743896\n",
            "step 10282: loss = 2.2276129722595215\n",
            "step 10283: loss = 2.080754518508911\n",
            "step 10284: loss = 2.2192647457122803\n",
            "step 10285: loss = 2.2043557167053223\n",
            "step 10286: loss = 2.2708284854888916\n",
            "step 10287: loss = 2.2737152576446533\n",
            "step 10288: loss = 2.199859857559204\n",
            "step 10289: loss = 2.2140610218048096\n",
            "step 10290: loss = 2.1042640209198\n",
            "step 10291: loss = 2.1378207206726074\n",
            "step 10292: loss = 2.0641062259674072\n",
            "step 10293: loss = 2.1226072311401367\n",
            "step 10294: loss = 1.9727983474731445\n",
            "step 10295: loss = 2.1684319972991943\n",
            "step 10296: loss = 1.918941617012024\n",
            "step 10297: loss = 2.058617115020752\n",
            "step 10298: loss = 1.9497324228286743\n",
            "step 10299: loss = 2.2632553577423096\n",
            "step 10300: loss = 2.2186150550842285\n",
            "step 10301: loss = 2.1173362731933594\n",
            "step 10302: loss = 2.18772029876709\n",
            "step 10303: loss = 2.0730700492858887\n",
            "step 10304: loss = 2.100177764892578\n",
            "step 10305: loss = 2.1570119857788086\n",
            "step 10306: loss = 2.205397605895996\n",
            "step 10307: loss = 2.3216099739074707\n",
            "step 10308: loss = 2.1469030380249023\n",
            "step 10309: loss = 2.1527962684631348\n",
            "step 10310: loss = 2.178356170654297\n",
            "step 10311: loss = 2.33038067817688\n",
            "step 10312: loss = 2.1437602043151855\n",
            "step 10313: loss = 2.1437952518463135\n",
            "step 10314: loss = 2.1375410556793213\n",
            "step 10315: loss = 2.038922071456909\n",
            "step 10316: loss = 2.315777063369751\n",
            "step 10317: loss = 2.2287769317626953\n",
            "step 10318: loss = 2.0402987003326416\n",
            "step 10319: loss = 2.2777278423309326\n",
            "step 10320: loss = 2.0243711471557617\n",
            "step 10321: loss = 2.10664439201355\n",
            "step 10322: loss = 2.2458558082580566\n",
            "step 10323: loss = 2.3847105503082275\n",
            "step 10324: loss = 2.1302947998046875\n",
            "step 10325: loss = 2.367993116378784\n",
            "step 10326: loss = 2.1971077919006348\n",
            "step 10327: loss = 2.434032678604126\n",
            "step 10328: loss = 2.196274518966675\n",
            "step 10329: loss = 2.515705108642578\n",
            "step 10330: loss = 2.0798494815826416\n",
            "step 10331: loss = 2.121793746948242\n",
            "step 10332: loss = 2.2685160636901855\n",
            "step 10333: loss = 2.0301599502563477\n",
            "step 10334: loss = 2.0874991416931152\n",
            "step 10335: loss = 2.2861969470977783\n",
            "step 10336: loss = 2.0596415996551514\n",
            "step 10337: loss = 2.379577398300171\n",
            "step 10338: loss = 1.9503355026245117\n",
            "step 10339: loss = 2.169630289077759\n",
            "step 10340: loss = 1.8472222089767456\n",
            "step 10341: loss = 2.10697340965271\n",
            "step 10342: loss = 2.055163621902466\n",
            "step 10343: loss = 2.227776050567627\n",
            "step 10344: loss = 2.3090262413024902\n",
            "step 10345: loss = 2.484506130218506\n",
            "step 10346: loss = 2.3818037509918213\n",
            "step 10347: loss = 2.402982711791992\n",
            "step 10348: loss = 2.157496690750122\n",
            "step 10349: loss = 1.978536605834961\n",
            "step 10350: loss = 2.2502388954162598\n",
            "step 10351: loss = 2.2419581413269043\n",
            "step 10352: loss = 1.9266326427459717\n",
            "step 10353: loss = 2.05500864982605\n",
            "step 10354: loss = 2.4356131553649902\n",
            "step 10355: loss = 2.2348315715789795\n",
            "step 10356: loss = 2.2697315216064453\n",
            "step 10357: loss = 2.315911054611206\n",
            "step 10358: loss = 2.385925769805908\n",
            "step 10359: loss = 2.1740221977233887\n",
            "step 10360: loss = 2.2673490047454834\n",
            "step 10361: loss = 2.371541738510132\n",
            "step 10362: loss = 2.239582061767578\n",
            "step 10363: loss = 2.0896644592285156\n",
            "step 10364: loss = 2.432859182357788\n",
            "step 10365: loss = 2.166167974472046\n",
            "step 10366: loss = 2.355665445327759\n",
            "step 10367: loss = 2.0480875968933105\n",
            "step 10368: loss = 2.028393030166626\n",
            "step 10369: loss = 1.9399058818817139\n",
            "step 10370: loss = 2.265883445739746\n",
            "step 10371: loss = 2.1064255237579346\n",
            "step 10372: loss = 2.205974817276001\n",
            "step 10373: loss = 2.016481876373291\n",
            "step 10374: loss = 2.3459742069244385\n",
            "step 10375: loss = 1.9489998817443848\n",
            "step 10376: loss = 2.2165818214416504\n",
            "step 10377: loss = 2.312537431716919\n",
            "step 10378: loss = 2.2763657569885254\n",
            "step 10379: loss = 2.153903007507324\n",
            "step 10380: loss = 2.0788869857788086\n",
            "step 10381: loss = 2.108043670654297\n",
            "step 10382: loss = 2.1093087196350098\n",
            "step 10383: loss = 2.29768705368042\n",
            "step 10384: loss = 2.2551326751708984\n",
            "step 10385: loss = 2.232841968536377\n",
            "step 10386: loss = 2.1479668617248535\n",
            "step 10387: loss = 2.3106462955474854\n",
            "step 10388: loss = 2.097790241241455\n",
            "step 10389: loss = 2.21915864944458\n",
            "step 10390: loss = 2.257896900177002\n",
            "step 10391: loss = 2.298750877380371\n",
            "step 10392: loss = 2.1452951431274414\n",
            "step 10393: loss = 2.2708678245544434\n",
            "step 10394: loss = 2.4440956115722656\n",
            "step 10395: loss = 2.2101070880889893\n",
            "step 10396: loss = 2.2996041774749756\n",
            "step 10397: loss = 2.234914541244507\n",
            "step 10398: loss = 2.121428966522217\n",
            "step 10399: loss = 2.4566941261291504\n",
            "step 10400: loss = 2.2837419509887695\n",
            "step 10401: loss = 2.374502182006836\n",
            "step 10402: loss = 2.2068793773651123\n",
            "step 10403: loss = 2.468345880508423\n",
            "step 10404: loss = 2.1358823776245117\n",
            "step 10405: loss = 2.2579095363616943\n",
            "step 10406: loss = 2.1752116680145264\n",
            "step 10407: loss = 2.2363851070404053\n",
            "step 10408: loss = 2.0323967933654785\n",
            "step 10409: loss = 2.323496103286743\n",
            "step 10410: loss = 2.0080535411834717\n",
            "step 10411: loss = 2.04738187789917\n",
            "step 10412: loss = 2.239744186401367\n",
            "step 10413: loss = 2.0025453567504883\n",
            "step 10414: loss = 2.3336822986602783\n",
            "step 10415: loss = 2.1058311462402344\n",
            "step 10416: loss = 2.2991206645965576\n",
            "step 10417: loss = 2.049379348754883\n",
            "step 10418: loss = 2.081557512283325\n",
            "step 10419: loss = 1.9802273511886597\n",
            "step 10420: loss = 2.3431787490844727\n",
            "step 10421: loss = 1.9974784851074219\n",
            "step 10422: loss = 1.98897123336792\n",
            "step 10423: loss = 1.8997527360916138\n",
            "step 10424: loss = 2.466362237930298\n",
            "step 10425: loss = 2.1036617755889893\n",
            "step 10426: loss = 2.1181554794311523\n",
            "step 10427: loss = 2.029703140258789\n",
            "step 10428: loss = 2.1524031162261963\n",
            "step 10429: loss = 2.07641339302063\n",
            "step 10430: loss = 2.1445674896240234\n",
            "step 10431: loss = 2.255553960800171\n",
            "step 10432: loss = 2.3561224937438965\n",
            "step 10433: loss = 2.147829055786133\n",
            "step 10434: loss = 2.176377773284912\n",
            "step 10435: loss = 2.1464731693267822\n",
            "step 10436: loss = 2.090789318084717\n",
            "step 10437: loss = 2.33021879196167\n",
            "step 10438: loss = 2.0954604148864746\n",
            "step 10439: loss = 2.1001172065734863\n",
            "step 10440: loss = 2.359285831451416\n",
            "step 10441: loss = 2.2629547119140625\n",
            "step 10442: loss = 2.312913417816162\n",
            "step 10443: loss = 2.128249406814575\n",
            "step 10444: loss = 2.436790943145752\n",
            "step 10445: loss = 2.5080273151397705\n",
            "step 10446: loss = 2.0680415630340576\n",
            "step 10447: loss = 2.1771421432495117\n",
            "step 10448: loss = 2.366567611694336\n",
            "step 10449: loss = 2.1728053092956543\n",
            "step 10450: loss = 2.1862194538116455\n",
            "step 10451: loss = 2.1786561012268066\n",
            "step 10452: loss = 2.0359902381896973\n",
            "step 10453: loss = 2.4013569355010986\n",
            "step 10454: loss = 2.114351749420166\n",
            "step 10455: loss = 2.052896022796631\n",
            "step 10456: loss = 2.3145408630371094\n",
            "step 10457: loss = 2.0341224670410156\n",
            "step 10458: loss = 2.15624737739563\n",
            "step 10459: loss = 2.153249740600586\n",
            "step 10460: loss = 1.9634383916854858\n",
            "step 10461: loss = 2.142298936843872\n",
            "step 10462: loss = 2.2708253860473633\n",
            "step 10463: loss = 2.1797165870666504\n",
            "step 10464: loss = 2.234187126159668\n",
            "step 10465: loss = 2.1390697956085205\n",
            "step 10466: loss = 2.0730092525482178\n",
            "step 10467: loss = 2.3943233489990234\n",
            "step 10468: loss = 2.076498508453369\n",
            "step 10469: loss = 2.012655258178711\n",
            "step 10470: loss = 2.156513214111328\n",
            "step 10471: loss = 2.2051711082458496\n",
            "step 10472: loss = 2.1432063579559326\n",
            "step 10473: loss = 2.33396315574646\n",
            "step 10474: loss = 2.1055116653442383\n",
            "step 10475: loss = 2.3680191040039062\n",
            "step 10476: loss = 1.8461570739746094\n",
            "step 10477: loss = 2.229235887527466\n",
            "step 10478: loss = 2.057246208190918\n",
            "step 10479: loss = 2.0308783054351807\n",
            "step 10480: loss = 2.0846798419952393\n",
            "step 10481: loss = 2.1879305839538574\n",
            "step 10482: loss = 2.2890782356262207\n",
            "step 10483: loss = 1.9445017576217651\n",
            "step 10484: loss = 2.2765469551086426\n",
            "step 10485: loss = 2.007398843765259\n",
            "step 10486: loss = 2.1628756523132324\n",
            "step 10487: loss = 2.017883777618408\n",
            "step 10488: loss = 2.293283224105835\n",
            "step 10489: loss = 2.211250066757202\n",
            "step 10490: loss = 2.039011001586914\n",
            "step 10491: loss = 2.1661667823791504\n",
            "step 10492: loss = 2.1936726570129395\n",
            "step 10493: loss = 1.9902548789978027\n",
            "step 10494: loss = 2.246790647506714\n",
            "step 10495: loss = 2.1604228019714355\n",
            "step 10496: loss = 2.0983424186706543\n",
            "step 10497: loss = 2.2871923446655273\n",
            "step 10498: loss = 2.330326795578003\n",
            "step 10499: loss = 2.12221622467041\n",
            "step 10500: loss = 2.444650173187256\n",
            "step 10501: loss = 2.173985004425049\n",
            "step 10502: loss = 2.0767602920532227\n",
            "step 10503: loss = 2.249654769897461\n",
            "step 10504: loss = 1.9928569793701172\n",
            "step 10505: loss = 2.1493136882781982\n",
            "step 10506: loss = 2.1758081912994385\n",
            "step 10507: loss = 2.2024855613708496\n",
            "step 10508: loss = 2.15240740776062\n",
            "step 10509: loss = 2.1960504055023193\n",
            "step 10510: loss = 2.1784441471099854\n",
            "step 10511: loss = 2.0426440238952637\n",
            "step 10512: loss = 2.2377936840057373\n",
            "step 10513: loss = 2.069012403488159\n",
            "step 10514: loss = 2.100959300994873\n",
            "step 10515: loss = 2.286231517791748\n",
            "step 10516: loss = 2.274744987487793\n",
            "step 10517: loss = 2.1919195652008057\n",
            "step 10518: loss = 2.3128557205200195\n",
            "step 10519: loss = 2.225558042526245\n",
            "step 10520: loss = 2.570004940032959\n",
            "step 10521: loss = 2.136746883392334\n",
            "step 10522: loss = 2.442347288131714\n",
            "step 10523: loss = 2.2865400314331055\n",
            "step 10524: loss = 2.141697883605957\n",
            "step 10525: loss = 2.4279186725616455\n",
            "step 10526: loss = 2.2870407104492188\n",
            "step 10527: loss = 2.001182794570923\n",
            "step 10528: loss = 2.4224321842193604\n",
            "step 10529: loss = 2.062495231628418\n",
            "step 10530: loss = 2.045156240463257\n",
            "step 10531: loss = 2.149031400680542\n",
            "step 10532: loss = 1.9724706411361694\n",
            "step 10533: loss = 2.1621792316436768\n",
            "step 10534: loss = 1.9360288381576538\n",
            "step 10535: loss = 2.2232890129089355\n",
            "step 10536: loss = 2.169532537460327\n",
            "step 10537: loss = 2.057133436203003\n",
            "step 10538: loss = 2.169605016708374\n",
            "step 10539: loss = 2.3102362155914307\n",
            "step 10540: loss = 2.336026668548584\n",
            "step 10541: loss = 2.4247629642486572\n",
            "step 10542: loss = 2.1540169715881348\n",
            "step 10543: loss = 2.0423669815063477\n",
            "step 10544: loss = 2.1180102825164795\n",
            "step 10545: loss = 2.202901601791382\n",
            "step 10546: loss = 2.1592915058135986\n",
            "step 10547: loss = 2.117328405380249\n",
            "step 10548: loss = 2.1727259159088135\n",
            "step 10549: loss = 2.017717123031616\n",
            "step 10550: loss = 2.0903120040893555\n",
            "step 10551: loss = 2.2759366035461426\n",
            "step 10552: loss = 2.2165603637695312\n",
            "step 10553: loss = 2.2709169387817383\n",
            "step 10554: loss = 2.1218435764312744\n",
            "step 10555: loss = 2.406672239303589\n",
            "step 10556: loss = 1.9649790525436401\n",
            "step 10557: loss = 2.0876946449279785\n",
            "step 10558: loss = 2.224381446838379\n",
            "step 10559: loss = 2.115936756134033\n",
            "step 10560: loss = 2.1884043216705322\n",
            "step 10561: loss = 2.2876765727996826\n",
            "step 10562: loss = 2.1073496341705322\n",
            "step 10563: loss = 2.1926944255828857\n",
            "step 10564: loss = 2.1646718978881836\n",
            "step 10565: loss = 2.5773086547851562\n",
            "step 10566: loss = 2.212899923324585\n",
            "step 10567: loss = 2.3939476013183594\n",
            "step 10568: loss = 2.1145284175872803\n",
            "step 10569: loss = 2.3553266525268555\n",
            "step 10570: loss = 2.07128643989563\n",
            "step 10571: loss = 2.119737386703491\n",
            "step 10572: loss = 2.282296657562256\n",
            "step 10573: loss = 2.265026569366455\n",
            "step 10574: loss = 2.3204760551452637\n",
            "step 10575: loss = 2.114651918411255\n",
            "step 10576: loss = 1.9229199886322021\n",
            "step 10577: loss = 2.2695202827453613\n",
            "step 10578: loss = 2.270726442337036\n",
            "step 10579: loss = 2.3492062091827393\n",
            "step 10580: loss = 2.177952527999878\n",
            "step 10581: loss = 2.254082202911377\n",
            "step 10582: loss = 2.3799655437469482\n",
            "step 10583: loss = 1.9575601816177368\n",
            "step 10584: loss = 2.1864264011383057\n",
            "step 10585: loss = 2.5145978927612305\n",
            "step 10586: loss = 2.196755886077881\n",
            "step 10587: loss = 2.203578472137451\n",
            "step 10588: loss = 2.066046714782715\n",
            "step 10589: loss = 2.3725523948669434\n",
            "step 10590: loss = 2.2819013595581055\n",
            "step 10591: loss = 2.3180859088897705\n",
            "step 10592: loss = 2.31609845161438\n",
            "step 10593: loss = 2.228147268295288\n",
            "step 10594: loss = 2.2640504837036133\n",
            "step 10595: loss = 2.325582504272461\n",
            "step 10596: loss = 2.1932644844055176\n",
            "step 10597: loss = 2.215841293334961\n",
            "step 10598: loss = 2.141101598739624\n",
            "step 10599: loss = 2.1856765747070312\n",
            "step 10600: loss = 2.4862420558929443\n",
            "step 10601: loss = 2.272742986679077\n",
            "step 10602: loss = 2.216841220855713\n",
            "step 10603: loss = 2.272571563720703\n",
            "step 10604: loss = 2.103546380996704\n",
            "step 10605: loss = 2.160079002380371\n",
            "step 10606: loss = 2.1758155822753906\n",
            "step 10607: loss = 2.584709405899048\n",
            "step 10608: loss = 2.2563233375549316\n",
            "step 10609: loss = 2.380103349685669\n",
            "step 10610: loss = 2.069692850112915\n",
            "step 10611: loss = 2.2119932174682617\n",
            "step 10612: loss = 2.1420836448669434\n",
            "step 10613: loss = 2.073598861694336\n",
            "step 10614: loss = 2.1119372844696045\n",
            "step 10615: loss = 2.259486436843872\n",
            "step 10616: loss = 2.306018590927124\n",
            "step 10617: loss = 2.2264511585235596\n",
            "step 10618: loss = 2.2805798053741455\n",
            "step 10619: loss = 1.9762479066848755\n",
            "step 10620: loss = 2.2284536361694336\n",
            "step 10621: loss = 2.294997453689575\n",
            "step 10622: loss = 2.024911642074585\n",
            "step 10623: loss = 2.1143691539764404\n",
            "step 10624: loss = 2.2625367641448975\n",
            "step 10625: loss = 2.5493409633636475\n",
            "step 10626: loss = 2.4241175651550293\n",
            "step 10627: loss = 2.112279176712036\n",
            "step 10628: loss = 2.1003761291503906\n",
            "step 10629: loss = 2.308562755584717\n",
            "step 10630: loss = 2.2456793785095215\n",
            "step 10631: loss = 2.417139768600464\n",
            "step 10632: loss = 2.3412158489227295\n",
            "step 10633: loss = 2.197127342224121\n",
            "step 10634: loss = 2.206645965576172\n",
            "step 10635: loss = 2.221161365509033\n",
            "step 10636: loss = 2.3195767402648926\n",
            "step 10637: loss = 2.177814483642578\n",
            "step 10638: loss = 2.004748582839966\n",
            "step 10639: loss = 2.1346306800842285\n",
            "step 10640: loss = 2.286546230316162\n",
            "step 10641: loss = 2.3971951007843018\n",
            "step 10642: loss = 2.2869579792022705\n",
            "step 10643: loss = 2.2712879180908203\n",
            "step 10644: loss = 2.096658945083618\n",
            "step 10645: loss = 2.2136435508728027\n",
            "step 10646: loss = 2.272639751434326\n",
            "step 10647: loss = 2.2369771003723145\n",
            "step 10648: loss = 1.9143171310424805\n",
            "step 10649: loss = 2.182372570037842\n",
            "step 10650: loss = 2.365010976791382\n",
            "step 10651: loss = 2.395101547241211\n",
            "step 10652: loss = 2.4335291385650635\n",
            "step 10653: loss = 2.3260040283203125\n",
            "step 10654: loss = 1.826391577720642\n",
            "step 10655: loss = 2.3743247985839844\n",
            "step 10656: loss = 2.186647653579712\n",
            "step 10657: loss = 2.196479082107544\n",
            "step 10658: loss = 2.206442356109619\n",
            "step 10659: loss = 2.1840367317199707\n",
            "step 10660: loss = 2.2244176864624023\n",
            "step 10661: loss = 2.3167078495025635\n",
            "step 10662: loss = 2.033907413482666\n",
            "step 10663: loss = 2.08005428314209\n",
            "step 10664: loss = 2.3047046661376953\n",
            "step 10665: loss = 2.104513168334961\n",
            "step 10666: loss = 2.2900314331054688\n",
            "step 10667: loss = 2.168224811553955\n",
            "step 10668: loss = 2.2893776893615723\n",
            "step 10669: loss = 2.0390305519104004\n",
            "step 10670: loss = 2.218958854675293\n",
            "step 10671: loss = 1.9824291467666626\n",
            "step 10672: loss = 2.2673072814941406\n",
            "step 10673: loss = 2.079523801803589\n",
            "step 10674: loss = 2.3940610885620117\n",
            "step 10675: loss = 2.3841474056243896\n",
            "step 10676: loss = 2.1285762786865234\n",
            "step 10677: loss = 2.269697427749634\n",
            "step 10678: loss = 2.2298696041107178\n",
            "step 10679: loss = 2.211940050125122\n",
            "step 10680: loss = 2.431023359298706\n",
            "step 10681: loss = 2.037458658218384\n",
            "step 10682: loss = 2.3420286178588867\n",
            "step 10683: loss = 1.9210529327392578\n",
            "step 10684: loss = 2.3015122413635254\n",
            "step 10685: loss = 2.44393253326416\n",
            "step 10686: loss = 2.4445416927337646\n",
            "step 10687: loss = 2.3152871131896973\n",
            "step 10688: loss = 2.3563594818115234\n",
            "step 10689: loss = 2.3512682914733887\n",
            "step 10690: loss = 2.3586862087249756\n",
            "step 10691: loss = 2.2219486236572266\n",
            "step 10692: loss = 2.193519115447998\n",
            "step 10693: loss = 2.3302905559539795\n",
            "step 10694: loss = 2.2471201419830322\n",
            "step 10695: loss = 2.2431869506835938\n",
            "step 10696: loss = 2.0536017417907715\n",
            "step 10697: loss = 2.023426055908203\n",
            "step 10698: loss = 2.328619956970215\n",
            "step 10699: loss = 2.236755132675171\n",
            "step 10700: loss = 2.1330184936523438\n",
            "step 10701: loss = 2.046588659286499\n",
            "step 10702: loss = 2.2272849082946777\n",
            "step 10703: loss = 2.2316932678222656\n",
            "step 10704: loss = 2.2596309185028076\n",
            "step 10705: loss = 1.8446576595306396\n",
            "step 10706: loss = 2.485480308532715\n",
            "step 10707: loss = 1.9627224206924438\n",
            "step 10708: loss = 2.0272462368011475\n",
            "step 10709: loss = 2.252251386642456\n",
            "step 10710: loss = 1.93158757686615\n",
            "step 10711: loss = 1.943466067314148\n",
            "step 10712: loss = 1.9569340944290161\n",
            "step 10713: loss = 2.393799066543579\n",
            "step 10714: loss = 2.2929797172546387\n",
            "step 10715: loss = 2.1784348487854004\n",
            "step 10716: loss = 2.174437999725342\n",
            "step 10717: loss = 2.1450600624084473\n",
            "step 10718: loss = 2.4969775676727295\n",
            "step 10719: loss = 2.3140883445739746\n",
            "step 10720: loss = 2.1470863819122314\n",
            "step 10721: loss = 2.2895567417144775\n",
            "step 10722: loss = 2.0554680824279785\n",
            "step 10723: loss = 2.3915903568267822\n",
            "step 10724: loss = 2.136392593383789\n",
            "step 10725: loss = 2.1755452156066895\n",
            "step 10726: loss = 2.2794017791748047\n",
            "step 10727: loss = 2.4249277114868164\n",
            "step 10728: loss = 2.2303783893585205\n",
            "step 10729: loss = 2.4292094707489014\n",
            "step 10730: loss = 2.096445322036743\n",
            "step 10731: loss = 2.1570606231689453\n",
            "step 10732: loss = 2.1087093353271484\n",
            "step 10733: loss = 2.182204246520996\n",
            "step 10734: loss = 2.7323882579803467\n",
            "step 10735: loss = 2.208099842071533\n",
            "step 10736: loss = 2.301107406616211\n",
            "step 10737: loss = 2.0678467750549316\n",
            "step 10738: loss = 2.2277047634124756\n",
            "step 10739: loss = 2.1200685501098633\n",
            "step 10740: loss = 2.293865203857422\n",
            "step 10741: loss = 2.2584340572357178\n",
            "step 10742: loss = 2.025634527206421\n",
            "step 10743: loss = 1.9564017057418823\n",
            "step 10744: loss = 2.1814675331115723\n",
            "step 10745: loss = 1.9999349117279053\n",
            "step 10746: loss = 2.194342851638794\n",
            "step 10747: loss = 2.1174654960632324\n",
            "step 10748: loss = 1.9515140056610107\n",
            "step 10749: loss = 2.0457186698913574\n",
            "step 10750: loss = 2.2514686584472656\n",
            "step 10751: loss = 2.2755391597747803\n",
            "step 10752: loss = 2.222210645675659\n",
            "step 10753: loss = 2.522639513015747\n",
            "step 10754: loss = 2.350492238998413\n",
            "step 10755: loss = 2.2630298137664795\n",
            "step 10756: loss = 2.4468746185302734\n",
            "step 10757: loss = 2.116999864578247\n",
            "step 10758: loss = 2.1261253356933594\n",
            "step 10759: loss = 1.8613159656524658\n",
            "step 10760: loss = 2.4048564434051514\n",
            "step 10761: loss = 2.1915056705474854\n",
            "step 10762: loss = 2.145095109939575\n",
            "step 10763: loss = 2.353578567504883\n",
            "step 10764: loss = 2.21909236907959\n",
            "step 10765: loss = 2.184399366378784\n",
            "step 10766: loss = 2.3457460403442383\n",
            "step 10767: loss = 2.240450143814087\n",
            "step 10768: loss = 2.2906599044799805\n",
            "step 10769: loss = 2.594834089279175\n",
            "step 10770: loss = 2.0677506923675537\n",
            "step 10771: loss = 2.309297561645508\n",
            "step 10772: loss = 2.2736756801605225\n",
            "step 10773: loss = 2.0015602111816406\n",
            "step 10774: loss = 2.1673834323883057\n",
            "step 10775: loss = 2.0092813968658447\n",
            "step 10776: loss = 2.2474563121795654\n",
            "step 10777: loss = 2.241109609603882\n",
            "step 10778: loss = 2.376567840576172\n",
            "step 10779: loss = 2.104703664779663\n",
            "step 10780: loss = 2.5817384719848633\n",
            "step 10781: loss = 2.1601574420928955\n",
            "step 10782: loss = 2.4059674739837646\n",
            "step 10783: loss = 2.4508588314056396\n",
            "step 10784: loss = 2.1912970542907715\n",
            "step 10785: loss = 2.2061238288879395\n",
            "step 10786: loss = 2.3769922256469727\n",
            "step 10787: loss = 2.0804150104522705\n",
            "step 10788: loss = 2.2039976119995117\n",
            "step 10789: loss = 2.263338565826416\n",
            "step 10790: loss = 1.9737521409988403\n",
            "step 10791: loss = 2.131866216659546\n",
            "step 10792: loss = 2.5380396842956543\n",
            "step 10793: loss = 2.373141050338745\n",
            "step 10794: loss = 2.05067777633667\n",
            "step 10795: loss = 2.3292417526245117\n",
            "step 10796: loss = 2.3882012367248535\n",
            "step 10797: loss = 2.2964484691619873\n",
            "step 10798: loss = 2.4103024005889893\n",
            "step 10799: loss = 2.2276206016540527\n",
            "step 10800: loss = 2.6024444103240967\n",
            "step 10801: loss = 2.1837069988250732\n",
            "step 10802: loss = 2.335211992263794\n",
            "step 10803: loss = 1.9795315265655518\n",
            "step 10804: loss = 2.3236844539642334\n",
            "step 10805: loss = 2.5410571098327637\n",
            "step 10806: loss = 2.0091097354888916\n",
            "step 10807: loss = 2.435197353363037\n",
            "step 10808: loss = 2.0478434562683105\n",
            "step 10809: loss = 2.2687435150146484\n",
            "step 10810: loss = 2.3133738040924072\n",
            "step 10811: loss = 2.421874761581421\n",
            "step 10812: loss = 2.2803268432617188\n",
            "step 10813: loss = 2.118847131729126\n",
            "step 10814: loss = 2.1529221534729004\n",
            "step 10815: loss = 2.2081589698791504\n",
            "step 10816: loss = 2.0575308799743652\n",
            "step 10817: loss = 2.303223133087158\n",
            "step 10818: loss = 2.136892795562744\n",
            "step 10819: loss = 2.3444247245788574\n",
            "step 10820: loss = 2.182635545730591\n",
            "step 10821: loss = 2.274174213409424\n",
            "step 10822: loss = 2.153759479522705\n",
            "step 10823: loss = 2.232869863510132\n",
            "step 10824: loss = 2.11820387840271\n",
            "step 10825: loss = 2.2659664154052734\n",
            "step 10826: loss = 2.1380884647369385\n",
            "step 10827: loss = 2.3529231548309326\n",
            "step 10828: loss = 2.3721563816070557\n",
            "step 10829: loss = 2.347813129425049\n",
            "step 10830: loss = 2.305476188659668\n",
            "step 10831: loss = 2.326092004776001\n",
            "step 10832: loss = 2.1698055267333984\n",
            "step 10833: loss = 2.2419281005859375\n",
            "step 10834: loss = 2.3080668449401855\n",
            "step 10835: loss = 2.430415391921997\n",
            "step 10836: loss = 2.289889097213745\n",
            "step 10837: loss = 2.5898447036743164\n",
            "step 10838: loss = 2.261687994003296\n",
            "step 10839: loss = 2.2703654766082764\n",
            "step 10840: loss = 2.3954885005950928\n",
            "step 10841: loss = 2.383434534072876\n",
            "step 10842: loss = 2.141099691390991\n",
            "step 10843: loss = 2.188497304916382\n",
            "step 10844: loss = 2.5653185844421387\n",
            "step 10845: loss = 2.0382227897644043\n",
            "step 10846: loss = 2.534287452697754\n",
            "step 10847: loss = 2.3584845066070557\n",
            "step 10848: loss = 2.48214054107666\n",
            "step 10849: loss = 2.4090261459350586\n",
            "step 10850: loss = 2.278836488723755\n",
            "step 10851: loss = 2.347827434539795\n",
            "step 10852: loss = 2.268282413482666\n",
            "step 10853: loss = 2.2625951766967773\n",
            "step 10854: loss = 2.175429344177246\n",
            "step 10855: loss = 2.394819498062134\n",
            "step 10856: loss = 2.1865451335906982\n",
            "step 10857: loss = 2.46675181388855\n",
            "step 10858: loss = 2.187833786010742\n",
            "step 10859: loss = 2.3343260288238525\n",
            "step 10860: loss = 2.196640729904175\n",
            "step 10861: loss = 2.308588743209839\n",
            "step 10862: loss = 2.392751455307007\n",
            "step 10863: loss = 2.217890739440918\n",
            "step 10864: loss = 2.398325204849243\n",
            "step 10865: loss = 2.2551472187042236\n",
            "step 10866: loss = 2.1331467628479004\n",
            "step 10867: loss = 2.332714796066284\n",
            "step 10868: loss = 2.397007465362549\n",
            "step 10869: loss = 2.306234359741211\n",
            "step 10870: loss = 2.2516019344329834\n",
            "step 10871: loss = 2.270876169204712\n",
            "step 10872: loss = 2.2455313205718994\n",
            "step 10873: loss = 2.432248592376709\n",
            "step 10874: loss = 2.152006149291992\n",
            "step 10875: loss = 2.192948818206787\n",
            "step 10876: loss = 2.332195997238159\n",
            "step 10877: loss = 2.1358802318573\n",
            "step 10878: loss = 2.3935248851776123\n",
            "step 10879: loss = 2.2467434406280518\n",
            "step 10880: loss = 2.333137035369873\n",
            "step 10881: loss = 2.310767650604248\n",
            "step 10882: loss = 2.292353630065918\n",
            "step 10883: loss = 2.1166608333587646\n",
            "step 10884: loss = 2.280815601348877\n",
            "step 10885: loss = 2.1211600303649902\n",
            "step 10886: loss = 2.0533483028411865\n",
            "step 10887: loss = 2.4145984649658203\n",
            "step 10888: loss = 2.196409225463867\n",
            "step 10889: loss = 2.4970061779022217\n",
            "step 10890: loss = 2.3345842361450195\n",
            "step 10891: loss = 2.4054203033447266\n",
            "step 10892: loss = 2.2103724479675293\n",
            "step 10893: loss = 2.2596023082733154\n",
            "step 10894: loss = 2.3954877853393555\n",
            "step 10895: loss = 2.253255605697632\n",
            "step 10896: loss = 2.407837390899658\n",
            "step 10897: loss = 2.227062225341797\n",
            "step 10898: loss = 2.3380908966064453\n",
            "step 10899: loss = 2.3979921340942383\n",
            "step 10900: loss = 2.1682217121124268\n",
            "step 10901: loss = 2.3160295486450195\n",
            "step 10902: loss = 2.3901631832122803\n",
            "step 10903: loss = 2.1510531902313232\n",
            "step 10904: loss = 2.3271799087524414\n",
            "step 10905: loss = 2.242793083190918\n",
            "step 10906: loss = 1.9758840799331665\n",
            "step 10907: loss = 2.2603631019592285\n",
            "step 10908: loss = 2.249713659286499\n",
            "step 10909: loss = 2.2080790996551514\n",
            "step 10910: loss = 2.228564977645874\n",
            "step 10911: loss = 2.0741689205169678\n",
            "step 10912: loss = 2.181051254272461\n",
            "step 10913: loss = 2.3591134548187256\n",
            "step 10914: loss = 1.995239496231079\n",
            "step 10915: loss = 2.213005781173706\n",
            "step 10916: loss = 1.9760003089904785\n",
            "step 10917: loss = 2.256706714630127\n",
            "step 10918: loss = 2.1383731365203857\n",
            "step 10919: loss = 2.2208497524261475\n",
            "step 10920: loss = 2.2929794788360596\n",
            "step 10921: loss = 2.2284445762634277\n",
            "step 10922: loss = 2.3391804695129395\n",
            "step 10923: loss = 2.3078677654266357\n",
            "step 10924: loss = 2.3497889041900635\n",
            "step 10925: loss = 1.9233521223068237\n",
            "step 10926: loss = 2.3645594120025635\n",
            "step 10927: loss = 2.001303195953369\n",
            "step 10928: loss = 2.3252785205841064\n",
            "step 10929: loss = 2.231015920639038\n",
            "step 10930: loss = 2.10660719871521\n",
            "step 10931: loss = 1.9575344324111938\n",
            "step 10932: loss = 2.3359436988830566\n",
            "step 10933: loss = 2.4649288654327393\n",
            "step 10934: loss = 2.4094398021698\n",
            "Finish epoch 7\n",
            "New model saved, minimum loss: 2.1410149374783574 \n",
            "\n",
            "step 10935: loss = 1.7674918174743652\n",
            "step 10936: loss = 1.786314606666565\n",
            "step 10937: loss = 1.9777908325195312\n",
            "step 10938: loss = 1.7815417051315308\n",
            "step 10939: loss = 1.9115103483200073\n",
            "step 10940: loss = 1.9868491888046265\n",
            "step 10941: loss = 1.7851133346557617\n",
            "step 10942: loss = 1.6545600891113281\n",
            "step 10943: loss = 1.875800371170044\n",
            "step 10944: loss = 1.8637745380401611\n",
            "step 10945: loss = 1.8310236930847168\n",
            "step 10946: loss = 1.5993223190307617\n",
            "step 10947: loss = 1.8089197874069214\n",
            "step 10948: loss = 1.758055329322815\n",
            "step 10949: loss = 1.5914958715438843\n",
            "step 10950: loss = 1.8247231245040894\n",
            "step 10951: loss = 1.5842312574386597\n",
            "step 10952: loss = 1.665963888168335\n",
            "step 10953: loss = 1.9663026332855225\n",
            "step 10954: loss = 1.5870968103408813\n",
            "step 10955: loss = 1.887389898300171\n",
            "step 10956: loss = 1.8946757316589355\n",
            "step 10957: loss = 1.7013239860534668\n",
            "step 10958: loss = 1.9405782222747803\n",
            "step 10959: loss = 2.0545380115509033\n",
            "step 10960: loss = 1.5914455652236938\n",
            "step 10961: loss = 1.6934953927993774\n",
            "step 10962: loss = 1.7597246170043945\n",
            "step 10963: loss = 1.8826574087142944\n",
            "step 10964: loss = 1.8307024240493774\n",
            "step 10965: loss = 1.8590720891952515\n",
            "step 10966: loss = 1.8756425380706787\n",
            "step 10967: loss = 1.7236454486846924\n",
            "step 10968: loss = 1.8024635314941406\n",
            "step 10969: loss = 1.9726430177688599\n",
            "step 10970: loss = 1.9411617517471313\n",
            "step 10971: loss = 1.585639238357544\n",
            "step 10972: loss = 1.9698292016983032\n",
            "step 10973: loss = 1.5648860931396484\n",
            "step 10974: loss = 1.8353568315505981\n",
            "step 10975: loss = 2.0677847862243652\n",
            "step 10976: loss = 1.9330912828445435\n",
            "step 10977: loss = 1.7533012628555298\n",
            "step 10978: loss = 2.194277286529541\n",
            "step 10979: loss = 1.7371171712875366\n",
            "step 10980: loss = 1.8688453435897827\n",
            "step 10981: loss = 1.84286367893219\n",
            "step 10982: loss = 1.7775644063949585\n",
            "step 10983: loss = 1.6420358419418335\n",
            "step 10984: loss = 1.7678636312484741\n",
            "step 10985: loss = 1.5720211267471313\n",
            "step 10986: loss = 1.712420105934143\n",
            "step 10987: loss = 1.8463906049728394\n",
            "step 10988: loss = 1.784371018409729\n",
            "step 10989: loss = 1.6958438158035278\n",
            "step 10990: loss = 1.7936029434204102\n",
            "step 10991: loss = 1.5961612462997437\n",
            "step 10992: loss = 1.8691134452819824\n",
            "step 10993: loss = 1.6352254152297974\n",
            "step 10994: loss = 1.7910752296447754\n",
            "step 10995: loss = 1.9935768842697144\n",
            "step 10996: loss = 1.703492522239685\n",
            "step 10997: loss = 1.7095893621444702\n",
            "step 10998: loss = 1.707816481590271\n",
            "step 10999: loss = 1.8321136236190796\n",
            "step 11000: loss = 1.8857516050338745\n",
            "step 11001: loss = 1.6461948156356812\n",
            "step 11002: loss = 1.8380564451217651\n",
            "step 11003: loss = 1.828094720840454\n",
            "step 11004: loss = 1.7129786014556885\n",
            "step 11005: loss = 1.721147060394287\n",
            "step 11006: loss = 1.7878823280334473\n",
            "step 11007: loss = 1.736504316329956\n",
            "step 11008: loss = 1.8275940418243408\n",
            "step 11009: loss = 1.735368013381958\n",
            "step 11010: loss = 1.9721883535385132\n",
            "step 11011: loss = 1.4864119291305542\n",
            "step 11012: loss = 1.960517406463623\n",
            "step 11013: loss = 1.7079482078552246\n",
            "step 11014: loss = 2.0167183876037598\n",
            "step 11015: loss = 1.8940702676773071\n",
            "step 11016: loss = 1.8158693313598633\n",
            "step 11017: loss = 1.9725556373596191\n",
            "step 11018: loss = 1.5553805828094482\n",
            "step 11019: loss = 1.843636155128479\n",
            "step 11020: loss = 1.7186256647109985\n",
            "step 11021: loss = 1.7458984851837158\n",
            "step 11022: loss = 1.5851857662200928\n",
            "step 11023: loss = 1.6468961238861084\n",
            "step 11024: loss = 1.764695405960083\n",
            "step 11025: loss = 1.6895413398742676\n",
            "step 11026: loss = 1.6181871891021729\n",
            "step 11027: loss = 1.8130104541778564\n",
            "step 11028: loss = 1.968384861946106\n",
            "step 11029: loss = 1.773232102394104\n",
            "step 11030: loss = 1.736618995666504\n",
            "step 11031: loss = 1.780503749847412\n",
            "step 11032: loss = 1.9568309783935547\n",
            "step 11033: loss = 1.839078664779663\n",
            "step 11034: loss = 1.780801773071289\n",
            "step 11035: loss = 1.8320941925048828\n",
            "step 11036: loss = 1.8714849948883057\n",
            "step 11037: loss = 1.797749400138855\n",
            "step 11038: loss = 1.8747122287750244\n",
            "step 11039: loss = 1.7664411067962646\n",
            "step 11040: loss = 1.7034969329833984\n",
            "step 11041: loss = 1.8913546800613403\n",
            "step 11042: loss = 1.77738356590271\n",
            "step 11043: loss = 1.9825395345687866\n",
            "step 11044: loss = 1.9679964780807495\n",
            "step 11045: loss = 1.6716350317001343\n",
            "step 11046: loss = 1.9443691968917847\n",
            "step 11047: loss = 1.7079143524169922\n",
            "step 11048: loss = 1.888805866241455\n",
            "step 11049: loss = 1.7927653789520264\n",
            "step 11050: loss = 1.870349407196045\n",
            "step 11051: loss = 1.9684830904006958\n",
            "step 11052: loss = 1.6840215921401978\n",
            "step 11053: loss = 1.8226258754730225\n",
            "step 11054: loss = 1.6970161199569702\n",
            "step 11055: loss = 1.7426698207855225\n",
            "step 11056: loss = 1.9171810150146484\n",
            "step 11057: loss = 1.9131596088409424\n",
            "step 11058: loss = 1.6536146402359009\n",
            "step 11059: loss = 2.0714778900146484\n",
            "step 11060: loss = 1.6187278032302856\n",
            "step 11061: loss = 1.7166144847869873\n",
            "step 11062: loss = 1.8478902578353882\n",
            "step 11063: loss = 1.6552762985229492\n",
            "step 11064: loss = 1.85805344581604\n",
            "step 11065: loss = 2.178941249847412\n",
            "step 11066: loss = 1.706252932548523\n",
            "step 11067: loss = 1.7160199880599976\n",
            "step 11068: loss = 1.613540530204773\n",
            "step 11069: loss = 1.9237874746322632\n",
            "step 11070: loss = 1.978026270866394\n",
            "step 11071: loss = 1.7482326030731201\n",
            "step 11072: loss = 1.950656771659851\n",
            "step 11073: loss = 2.004711627960205\n",
            "step 11074: loss = 1.970659852027893\n",
            "step 11075: loss = 1.8508148193359375\n",
            "step 11076: loss = 1.9492957592010498\n",
            "step 11077: loss = 1.7588698863983154\n",
            "step 11078: loss = 2.17391300201416\n",
            "step 11079: loss = 1.8176815509796143\n",
            "step 11080: loss = 1.9390432834625244\n",
            "step 11081: loss = 1.8497440814971924\n",
            "step 11082: loss = 2.0201480388641357\n",
            "step 11083: loss = 1.8159739971160889\n",
            "step 11084: loss = 1.9361096620559692\n",
            "step 11085: loss = 1.8765517473220825\n",
            "step 11086: loss = 1.8513017892837524\n",
            "step 11087: loss = 1.9897452592849731\n",
            "step 11088: loss = 1.9403588771820068\n",
            "step 11089: loss = 2.0189919471740723\n",
            "step 11090: loss = 1.9888954162597656\n",
            "step 11091: loss = 1.9162976741790771\n",
            "step 11092: loss = 1.664159893989563\n",
            "step 11093: loss = 1.6665369272232056\n",
            "step 11094: loss = 1.6929352283477783\n",
            "step 11095: loss = 1.9115455150604248\n",
            "step 11096: loss = 2.0466065406799316\n",
            "step 11097: loss = 1.5092934370040894\n",
            "step 11098: loss = 1.9043406248092651\n",
            "step 11099: loss = 1.8004851341247559\n",
            "step 11100: loss = 1.8402502536773682\n",
            "step 11101: loss = 1.861140251159668\n",
            "step 11102: loss = 1.9352234601974487\n",
            "step 11103: loss = 1.8278142213821411\n",
            "step 11104: loss = 1.6513569355010986\n",
            "step 11105: loss = 1.7036454677581787\n",
            "step 11106: loss = 1.5402413606643677\n",
            "step 11107: loss = 1.9635941982269287\n",
            "step 11108: loss = 2.1241769790649414\n",
            "step 11109: loss = 1.6281381845474243\n",
            "step 11110: loss = 1.6055166721343994\n",
            "step 11111: loss = 1.9291729927062988\n",
            "step 11112: loss = 1.7538137435913086\n",
            "step 11113: loss = 1.7012746334075928\n",
            "step 11114: loss = 1.8936243057250977\n",
            "step 11115: loss = 1.860020399093628\n",
            "step 11116: loss = 2.126629590988159\n",
            "step 11117: loss = 1.845851182937622\n",
            "step 11118: loss = 1.6634619235992432\n",
            "step 11119: loss = 1.6814954280853271\n",
            "step 11120: loss = 1.6172618865966797\n",
            "step 11121: loss = 2.0698986053466797\n",
            "step 11122: loss = 1.645391821861267\n",
            "step 11123: loss = 2.0710253715515137\n",
            "step 11124: loss = 1.956020474433899\n",
            "step 11125: loss = 1.5746513605117798\n",
            "step 11126: loss = 2.0249624252319336\n",
            "step 11127: loss = 2.038012742996216\n",
            "step 11128: loss = 1.915501356124878\n",
            "step 11129: loss = 1.8326643705368042\n",
            "step 11130: loss = 1.8003778457641602\n",
            "step 11131: loss = 1.9900270700454712\n",
            "step 11132: loss = 1.919231653213501\n",
            "step 11133: loss = 1.8099751472473145\n",
            "step 11134: loss = 1.7238712310791016\n",
            "step 11135: loss = 1.5378130674362183\n",
            "step 11136: loss = 1.6702760457992554\n",
            "step 11137: loss = 1.8907016515731812\n",
            "step 11138: loss = 2.0104568004608154\n",
            "step 11139: loss = 1.9113372564315796\n",
            "step 11140: loss = 1.7867408990859985\n",
            "step 11141: loss = 1.660226821899414\n",
            "step 11142: loss = 1.6592938899993896\n",
            "step 11143: loss = 1.909867286682129\n",
            "step 11144: loss = 1.589257001876831\n",
            "step 11145: loss = 1.7256299257278442\n",
            "step 11146: loss = 1.7687917947769165\n",
            "step 11147: loss = 1.5863723754882812\n",
            "step 11148: loss = 1.724998116493225\n",
            "step 11149: loss = 1.9702519178390503\n",
            "step 11150: loss = 1.833304762840271\n",
            "step 11151: loss = 1.7347856760025024\n",
            "step 11152: loss = 1.83680260181427\n",
            "step 11153: loss = 1.993781566619873\n",
            "step 11154: loss = 1.9679428339004517\n",
            "step 11155: loss = 1.8938995599746704\n",
            "step 11156: loss = 1.811629056930542\n",
            "step 11157: loss = 1.665165901184082\n",
            "step 11158: loss = 1.9402871131896973\n",
            "step 11159: loss = 1.7598826885223389\n",
            "step 11160: loss = 1.6172032356262207\n",
            "step 11161: loss = 1.948452115058899\n",
            "step 11162: loss = 1.8790369033813477\n",
            "step 11163: loss = 1.8928402662277222\n",
            "step 11164: loss = 1.9269545078277588\n",
            "step 11165: loss = 1.8631399869918823\n",
            "step 11166: loss = 1.8657091856002808\n",
            "step 11167: loss = 1.994231939315796\n",
            "step 11168: loss = 1.8009281158447266\n",
            "step 11169: loss = 2.0054054260253906\n",
            "step 11170: loss = 1.840281367301941\n",
            "step 11171: loss = 1.735243320465088\n",
            "step 11172: loss = 1.7907686233520508\n",
            "step 11173: loss = 1.7754547595977783\n",
            "step 11174: loss = 1.611782193183899\n",
            "step 11175: loss = 1.7779033184051514\n",
            "step 11176: loss = 1.9339871406555176\n",
            "step 11177: loss = 1.7829854488372803\n",
            "step 11178: loss = 1.8096870183944702\n",
            "step 11179: loss = 1.8833781480789185\n",
            "step 11180: loss = 1.9924285411834717\n",
            "step 11181: loss = 1.9973877668380737\n",
            "step 11182: loss = 1.553849697113037\n",
            "step 11183: loss = 1.8601603507995605\n",
            "step 11184: loss = 1.9829479455947876\n",
            "step 11185: loss = 1.806599736213684\n",
            "step 11186: loss = 1.8267121315002441\n",
            "step 11187: loss = 1.9595133066177368\n",
            "step 11188: loss = 1.855434775352478\n",
            "step 11189: loss = 1.8609871864318848\n",
            "step 11190: loss = 1.5962687730789185\n",
            "step 11191: loss = 1.9140952825546265\n",
            "step 11192: loss = 1.867908239364624\n",
            "step 11193: loss = 1.809195637702942\n",
            "step 11194: loss = 2.0752432346343994\n",
            "step 11195: loss = 2.1552481651306152\n",
            "step 11196: loss = 1.8719652891159058\n",
            "step 11197: loss = 1.923373818397522\n",
            "step 11198: loss = 2.1066370010375977\n",
            "step 11199: loss = 1.8946822881698608\n",
            "step 11200: loss = 1.9594380855560303\n",
            "step 11201: loss = 1.6004328727722168\n",
            "step 11202: loss = 1.8478600978851318\n",
            "step 11203: loss = 1.830038070678711\n",
            "step 11204: loss = 2.0755016803741455\n",
            "step 11205: loss = 1.9005017280578613\n",
            "step 11206: loss = 1.847315788269043\n",
            "step 11207: loss = 1.9002995491027832\n",
            "step 11208: loss = 1.9958394765853882\n",
            "step 11209: loss = 1.664543867111206\n",
            "step 11210: loss = 2.0152475833892822\n",
            "step 11211: loss = 1.76610267162323\n",
            "step 11212: loss = 1.7921867370605469\n",
            "step 11213: loss = 1.7998802661895752\n",
            "step 11214: loss = 1.775341510772705\n",
            "step 11215: loss = 1.8166860342025757\n",
            "step 11216: loss = 1.9647772312164307\n",
            "step 11217: loss = 1.8184846639633179\n",
            "step 11218: loss = 2.1981794834136963\n",
            "step 11219: loss = 2.0422487258911133\n",
            "step 11220: loss = 1.77262282371521\n",
            "step 11221: loss = 2.149513006210327\n",
            "step 11222: loss = 1.8177512884140015\n",
            "step 11223: loss = 1.7566308975219727\n",
            "step 11224: loss = 1.743558406829834\n",
            "step 11225: loss = 1.9202661514282227\n",
            "step 11226: loss = 2.035555362701416\n",
            "step 11227: loss = 1.7432799339294434\n",
            "step 11228: loss = 1.8896592855453491\n",
            "step 11229: loss = 2.0724706649780273\n",
            "step 11230: loss = 1.8927638530731201\n",
            "step 11231: loss = 1.779613971710205\n",
            "step 11232: loss = 1.7988983392715454\n",
            "step 11233: loss = 1.9561392068862915\n",
            "step 11234: loss = 1.99191415309906\n",
            "step 11235: loss = 2.0330941677093506\n",
            "step 11236: loss = 1.8162704706192017\n",
            "step 11237: loss = 1.8737351894378662\n",
            "step 11238: loss = 1.794780969619751\n",
            "step 11239: loss = 1.8948262929916382\n",
            "step 11240: loss = 1.9555140733718872\n",
            "step 11241: loss = 1.6835846900939941\n",
            "step 11242: loss = 1.8296834230422974\n",
            "step 11243: loss = 1.9485323429107666\n",
            "step 11244: loss = 1.8507829904556274\n",
            "step 11245: loss = 1.7699369192123413\n",
            "step 11246: loss = 1.5771872997283936\n",
            "step 11247: loss = 1.879621148109436\n",
            "step 11248: loss = 1.7847203016281128\n",
            "step 11249: loss = 2.0603525638580322\n",
            "step 11250: loss = 1.6962788105010986\n",
            "step 11251: loss = 1.6554391384124756\n",
            "step 11252: loss = 1.7424030303955078\n",
            "step 11253: loss = 1.9026985168457031\n",
            "step 11254: loss = 1.866822600364685\n",
            "step 11255: loss = 1.9003525972366333\n",
            "step 11256: loss = 1.9151647090911865\n",
            "step 11257: loss = 1.8342647552490234\n",
            "step 11258: loss = 1.693036437034607\n",
            "step 11259: loss = 1.8940333127975464\n",
            "step 11260: loss = 1.628023624420166\n",
            "step 11261: loss = 1.9742799997329712\n",
            "step 11262: loss = 1.7753491401672363\n",
            "step 11263: loss = 1.9135955572128296\n",
            "step 11264: loss = 1.8686069250106812\n",
            "step 11265: loss = 1.8214291334152222\n",
            "step 11266: loss = 1.6461796760559082\n",
            "step 11267: loss = 1.731521487236023\n",
            "step 11268: loss = 1.993901014328003\n",
            "step 11269: loss = 1.9272953271865845\n",
            "step 11270: loss = 1.9422696828842163\n",
            "step 11271: loss = 2.0968387126922607\n",
            "step 11272: loss = 1.8988850116729736\n",
            "step 11273: loss = 1.9774038791656494\n",
            "step 11274: loss = 1.7549030780792236\n",
            "step 11275: loss = 1.7339239120483398\n",
            "step 11276: loss = 1.74256432056427\n",
            "step 11277: loss = 1.8025094270706177\n",
            "step 11278: loss = 1.7745375633239746\n",
            "step 11279: loss = 2.081068515777588\n",
            "step 11280: loss = 2.2331860065460205\n",
            "step 11281: loss = 1.9215584993362427\n",
            "step 11282: loss = 2.0183768272399902\n",
            "step 11283: loss = 1.901692509651184\n",
            "step 11284: loss = 1.9476226568222046\n",
            "step 11285: loss = 1.898327350616455\n",
            "step 11286: loss = 2.0363473892211914\n",
            "step 11287: loss = 1.8173094987869263\n",
            "step 11288: loss = 1.9361270666122437\n",
            "step 11289: loss = 2.019134759902954\n",
            "step 11290: loss = 1.9412364959716797\n",
            "step 11291: loss = 2.063941478729248\n",
            "step 11292: loss = 2.0237789154052734\n",
            "step 11293: loss = 1.7244172096252441\n",
            "step 11294: loss = 2.086439371109009\n",
            "step 11295: loss = 1.6125050783157349\n",
            "step 11296: loss = 1.7717777490615845\n",
            "step 11297: loss = 2.143880605697632\n",
            "step 11298: loss = 2.152684211730957\n",
            "step 11299: loss = 1.900683879852295\n",
            "step 11300: loss = 2.1426706314086914\n",
            "step 11301: loss = 2.1170780658721924\n",
            "step 11302: loss = 1.945007085800171\n",
            "step 11303: loss = 2.0752785205841064\n",
            "step 11304: loss = 2.1333444118499756\n",
            "step 11305: loss = 1.776309609413147\n",
            "step 11306: loss = 1.9842478036880493\n",
            "step 11307: loss = 2.0083765983581543\n",
            "step 11308: loss = 1.989827275276184\n",
            "step 11309: loss = 1.7668535709381104\n",
            "step 11310: loss = 2.0940864086151123\n",
            "step 11311: loss = 1.8340681791305542\n",
            "step 11312: loss = 1.7103185653686523\n",
            "step 11313: loss = 1.840113639831543\n",
            "step 11314: loss = 1.7828608751296997\n",
            "step 11315: loss = 1.8025459051132202\n",
            "step 11316: loss = 1.8005448579788208\n",
            "step 11317: loss = 1.9555188417434692\n",
            "step 11318: loss = 1.8517661094665527\n",
            "step 11319: loss = 1.7403665781021118\n",
            "step 11320: loss = 1.8733057975769043\n",
            "step 11321: loss = 1.8228709697723389\n",
            "step 11322: loss = 1.7362416982650757\n",
            "step 11323: loss = 2.0893070697784424\n",
            "step 11324: loss = 1.884688138961792\n",
            "step 11325: loss = 1.9867862462997437\n",
            "step 11326: loss = 1.889360785484314\n",
            "step 11327: loss = 1.7780945301055908\n",
            "step 11328: loss = 1.8237615823745728\n",
            "step 11329: loss = 1.8828768730163574\n",
            "step 11330: loss = 1.942678689956665\n",
            "step 11331: loss = 1.8170745372772217\n",
            "step 11332: loss = 1.935534954071045\n",
            "step 11333: loss = 1.9175735712051392\n",
            "step 11334: loss = 1.8743075132369995\n",
            "step 11335: loss = 1.8677555322647095\n",
            "step 11336: loss = 2.2591631412506104\n",
            "step 11337: loss = 2.0606155395507812\n",
            "step 11338: loss = 1.8901443481445312\n",
            "step 11339: loss = 2.007267951965332\n",
            "step 11340: loss = 1.9503302574157715\n",
            "step 11341: loss = 1.8594026565551758\n",
            "step 11342: loss = 1.8343570232391357\n",
            "step 11343: loss = 2.0399749279022217\n",
            "step 11344: loss = 2.0147743225097656\n",
            "step 11345: loss = 2.0070643424987793\n",
            "step 11346: loss = 2.2457783222198486\n",
            "step 11347: loss = 2.1640613079071045\n",
            "step 11348: loss = 2.107125759124756\n",
            "step 11349: loss = 1.7344406843185425\n",
            "step 11350: loss = 1.8619670867919922\n",
            "step 11351: loss = 1.9755233526229858\n",
            "step 11352: loss = 1.884419322013855\n",
            "step 11353: loss = 1.7877047061920166\n",
            "step 11354: loss = 2.0281434059143066\n",
            "step 11355: loss = 1.822139859199524\n",
            "step 11356: loss = 1.8498841524124146\n",
            "step 11357: loss = 1.7589319944381714\n",
            "step 11358: loss = 2.016508102416992\n",
            "step 11359: loss = 1.689012050628662\n",
            "step 11360: loss = 1.9071744680404663\n",
            "step 11361: loss = 2.1271157264709473\n",
            "step 11362: loss = 1.8692597150802612\n",
            "step 11363: loss = 1.7904289960861206\n",
            "step 11364: loss = 1.8136861324310303\n",
            "step 11365: loss = 1.86564302444458\n",
            "step 11366: loss = 1.7554833889007568\n",
            "step 11367: loss = 2.0943379402160645\n",
            "step 11368: loss = 2.1769139766693115\n",
            "step 11369: loss = 1.896138072013855\n",
            "step 11370: loss = 1.930701732635498\n",
            "step 11371: loss = 1.9132583141326904\n",
            "step 11372: loss = 1.9329043626785278\n",
            "step 11373: loss = 2.108711004257202\n",
            "step 11374: loss = 2.0298709869384766\n",
            "step 11375: loss = 1.885561466217041\n",
            "step 11376: loss = 2.073166847229004\n",
            "step 11377: loss = 1.8767375946044922\n",
            "step 11378: loss = 2.1992573738098145\n",
            "step 11379: loss = 1.9807208776474\n",
            "step 11380: loss = 1.8559824228286743\n",
            "step 11381: loss = 1.7687568664550781\n",
            "step 11382: loss = 2.179499387741089\n",
            "step 11383: loss = 1.8785394430160522\n",
            "step 11384: loss = 1.9060348272323608\n",
            "step 11385: loss = 1.9476178884506226\n",
            "step 11386: loss = 2.040712594985962\n",
            "step 11387: loss = 1.680303931236267\n",
            "step 11388: loss = 1.8471330404281616\n",
            "step 11389: loss = 1.9409137964248657\n",
            "step 11390: loss = 1.8411914110183716\n",
            "step 11391: loss = 1.9565997123718262\n",
            "step 11392: loss = 1.7338374853134155\n",
            "step 11393: loss = 1.78928542137146\n",
            "step 11394: loss = 1.7921524047851562\n",
            "step 11395: loss = 1.8087369203567505\n",
            "step 11396: loss = 1.8768850564956665\n",
            "step 11397: loss = 1.9968727827072144\n",
            "step 11398: loss = 2.1061408519744873\n",
            "step 11399: loss = 1.9621150493621826\n",
            "step 11400: loss = 2.0042614936828613\n",
            "step 11401: loss = 1.66472327709198\n",
            "step 11402: loss = 1.8246296644210815\n",
            "step 11403: loss = 1.718555212020874\n",
            "step 11404: loss = 2.0276355743408203\n",
            "step 11405: loss = 1.837391972541809\n",
            "step 11406: loss = 1.8426012992858887\n",
            "step 11407: loss = 1.8999654054641724\n",
            "step 11408: loss = 1.9340144395828247\n",
            "step 11409: loss = 1.7787227630615234\n",
            "step 11410: loss = 2.075322389602661\n",
            "step 11411: loss = 1.7887322902679443\n",
            "step 11412: loss = 1.8312666416168213\n",
            "step 11413: loss = 1.9312264919281006\n",
            "step 11414: loss = 2.012680768966675\n",
            "step 11415: loss = 1.9795445203781128\n",
            "step 11416: loss = 1.9091289043426514\n",
            "step 11417: loss = 2.0069477558135986\n",
            "step 11418: loss = 2.060227632522583\n",
            "step 11419: loss = 1.7440474033355713\n",
            "step 11420: loss = 1.9952478408813477\n",
            "step 11421: loss = 1.7731670141220093\n",
            "step 11422: loss = 2.1651387214660645\n",
            "step 11423: loss = 1.9938870668411255\n",
            "step 11424: loss = 2.067317247390747\n",
            "step 11425: loss = 2.0176808834075928\n",
            "step 11426: loss = 1.835838794708252\n",
            "step 11427: loss = 1.6117535829544067\n",
            "step 11428: loss = 1.8475884199142456\n",
            "step 11429: loss = 1.9093303680419922\n",
            "step 11430: loss = 1.8057007789611816\n",
            "step 11431: loss = 1.8813318014144897\n",
            "step 11432: loss = 1.7362890243530273\n",
            "step 11433: loss = 2.0685112476348877\n",
            "step 11434: loss = 1.888969898223877\n",
            "step 11435: loss = 2.100747585296631\n",
            "step 11436: loss = 1.8050971031188965\n",
            "step 11437: loss = 2.0853357315063477\n",
            "step 11438: loss = 2.026618480682373\n",
            "step 11439: loss = 1.849511742591858\n",
            "step 11440: loss = 2.0721585750579834\n",
            "step 11441: loss = 1.8330776691436768\n",
            "step 11442: loss = 1.8692991733551025\n",
            "step 11443: loss = 2.013185739517212\n",
            "step 11444: loss = 1.974305510520935\n",
            "step 11445: loss = 2.0764095783233643\n",
            "step 11446: loss = 1.90851891040802\n",
            "step 11447: loss = 2.002667188644409\n",
            "step 11448: loss = 1.9087222814559937\n",
            "step 11449: loss = 1.670532464981079\n",
            "step 11450: loss = 1.8881827592849731\n",
            "step 11451: loss = 1.8630815744400024\n",
            "step 11452: loss = 1.923603892326355\n",
            "step 11453: loss = 1.8530514240264893\n",
            "step 11454: loss = 1.7355220317840576\n",
            "step 11455: loss = 1.8485560417175293\n",
            "step 11456: loss = 2.00046706199646\n",
            "step 11457: loss = 2.0345773696899414\n",
            "step 11458: loss = 1.9378328323364258\n",
            "step 11459: loss = 1.8377764225006104\n",
            "step 11460: loss = 1.8313565254211426\n",
            "step 11461: loss = 1.7725261449813843\n",
            "step 11462: loss = 1.7407841682434082\n",
            "step 11463: loss = 1.9507956504821777\n",
            "step 11464: loss = 1.714758276939392\n",
            "step 11465: loss = 2.0181987285614014\n",
            "step 11466: loss = 1.8419276475906372\n",
            "step 11467: loss = 1.995816946029663\n",
            "step 11468: loss = 2.0254414081573486\n",
            "step 11469: loss = 2.0908501148223877\n",
            "step 11470: loss = 2.0407626628875732\n",
            "step 11471: loss = 1.6326035261154175\n",
            "step 11472: loss = 1.8470618724822998\n",
            "step 11473: loss = 1.726027250289917\n",
            "step 11474: loss = 2.0615503787994385\n",
            "step 11475: loss = 1.881120204925537\n",
            "step 11476: loss = 2.144737482070923\n",
            "step 11477: loss = 2.119054079055786\n",
            "step 11478: loss = 1.7734380960464478\n",
            "step 11479: loss = 1.783074140548706\n",
            "step 11480: loss = 1.766373634338379\n",
            "step 11481: loss = 2.0769193172454834\n",
            "step 11482: loss = 2.0841152667999268\n",
            "step 11483: loss = 2.0045199394226074\n",
            "step 11484: loss = 1.9097398519515991\n",
            "step 11485: loss = 1.7086403369903564\n",
            "step 11486: loss = 1.8027023077011108\n",
            "step 11487: loss = 2.0664267539978027\n",
            "step 11488: loss = 1.9312114715576172\n",
            "step 11489: loss = 2.0391156673431396\n",
            "step 11490: loss = 2.1544578075408936\n",
            "step 11491: loss = 1.9945207834243774\n",
            "step 11492: loss = 1.7691702842712402\n",
            "step 11493: loss = 2.0399961471557617\n",
            "step 11494: loss = 2.0326144695281982\n",
            "step 11495: loss = 2.0671091079711914\n",
            "step 11496: loss = 1.896985650062561\n",
            "step 11497: loss = 1.7146239280700684\n",
            "step 11498: loss = 2.312429904937744\n",
            "step 11499: loss = 2.047564744949341\n",
            "step 11500: loss = 2.010082244873047\n",
            "step 11501: loss = 1.948707938194275\n",
            "step 11502: loss = 2.014963388442993\n",
            "step 11503: loss = 2.083594560623169\n",
            "step 11504: loss = 1.779879093170166\n",
            "step 11505: loss = 2.070512056350708\n",
            "step 11506: loss = 1.9720757007598877\n",
            "step 11507: loss = 1.8565900325775146\n",
            "step 11508: loss = 1.7835873365402222\n",
            "step 11509: loss = 1.9662587642669678\n",
            "step 11510: loss = 1.7890833616256714\n",
            "step 11511: loss = 1.9679030179977417\n",
            "step 11512: loss = 1.8448569774627686\n",
            "step 11513: loss = 1.9794833660125732\n",
            "step 11514: loss = 2.082688808441162\n",
            "step 11515: loss = 2.019946336746216\n",
            "step 11516: loss = 1.9702048301696777\n",
            "step 11517: loss = 1.9454962015151978\n",
            "step 11518: loss = 2.054630994796753\n",
            "step 11519: loss = 2.1811063289642334\n",
            "step 11520: loss = 1.8227248191833496\n",
            "step 11521: loss = 1.9415115118026733\n",
            "step 11522: loss = 2.21559476852417\n",
            "step 11523: loss = 1.9302374124526978\n",
            "step 11524: loss = 2.1162302494049072\n",
            "step 11525: loss = 2.039682149887085\n",
            "step 11526: loss = 2.0522422790527344\n",
            "step 11527: loss = 2.0636777877807617\n",
            "step 11528: loss = 1.946840763092041\n",
            "step 11529: loss = 1.8637664318084717\n",
            "step 11530: loss = 2.308314800262451\n",
            "step 11531: loss = 1.9744844436645508\n",
            "step 11532: loss = 2.2347512245178223\n",
            "step 11533: loss = 2.0759549140930176\n",
            "step 11534: loss = 1.7083479166030884\n",
            "step 11535: loss = 2.077228307723999\n",
            "step 11536: loss = 1.804645299911499\n",
            "step 11537: loss = 1.8706780672073364\n",
            "step 11538: loss = 2.0132553577423096\n",
            "step 11539: loss = 2.151630163192749\n",
            "step 11540: loss = 1.8012620210647583\n",
            "step 11541: loss = 2.1115939617156982\n",
            "step 11542: loss = 1.7729065418243408\n",
            "step 11543: loss = 2.076188325881958\n",
            "step 11544: loss = 1.7954572439193726\n",
            "step 11545: loss = 1.808978796005249\n",
            "step 11546: loss = 2.0301015377044678\n",
            "step 11547: loss = 1.9067271947860718\n",
            "step 11548: loss = 1.916728138923645\n",
            "step 11549: loss = 1.8414080142974854\n",
            "step 11550: loss = 2.038674831390381\n",
            "step 11551: loss = 1.960019588470459\n",
            "step 11552: loss = 2.011293888092041\n",
            "step 11553: loss = 2.187739133834839\n",
            "step 11554: loss = 1.8051718473434448\n",
            "step 11555: loss = 1.8706786632537842\n",
            "step 11556: loss = 1.7076669931411743\n",
            "step 11557: loss = 1.9341272115707397\n",
            "step 11558: loss = 1.8370126485824585\n",
            "step 11559: loss = 1.8467038869857788\n",
            "step 11560: loss = 2.1136693954467773\n",
            "step 11561: loss = 1.8421231508255005\n",
            "step 11562: loss = 1.7421741485595703\n",
            "step 11563: loss = 2.000352621078491\n",
            "step 11564: loss = 1.9877194166183472\n",
            "step 11565: loss = 1.8748416900634766\n",
            "step 11566: loss = 1.9864293336868286\n",
            "step 11567: loss = 1.7230522632598877\n",
            "step 11568: loss = 1.952807903289795\n",
            "step 11569: loss = 2.1916983127593994\n",
            "step 11570: loss = 1.7631661891937256\n",
            "step 11571: loss = 1.9582418203353882\n",
            "step 11572: loss = 2.1012110710144043\n",
            "step 11573: loss = 2.0971128940582275\n",
            "step 11574: loss = 2.014348030090332\n",
            "step 11575: loss = 1.862612009048462\n",
            "step 11576: loss = 1.8875893354415894\n",
            "step 11577: loss = 1.8648966550827026\n",
            "step 11578: loss = 1.9647598266601562\n",
            "step 11579: loss = 1.745326280593872\n",
            "step 11580: loss = 2.119271755218506\n",
            "step 11581: loss = 1.9542815685272217\n",
            "step 11582: loss = 1.9235082864761353\n",
            "step 11583: loss = 1.8527942895889282\n",
            "step 11584: loss = 2.082160472869873\n",
            "step 11585: loss = 1.8053734302520752\n",
            "step 11586: loss = 1.7450599670410156\n",
            "step 11587: loss = 1.796477198600769\n",
            "step 11588: loss = 1.959181785583496\n",
            "step 11589: loss = 2.1560847759246826\n",
            "step 11590: loss = 1.9997807741165161\n",
            "step 11591: loss = 1.9104342460632324\n",
            "step 11592: loss = 1.8991848230361938\n",
            "step 11593: loss = 1.8281255960464478\n",
            "step 11594: loss = 1.8338475227355957\n",
            "step 11595: loss = 2.1093671321868896\n",
            "step 11596: loss = 1.9940992593765259\n",
            "step 11597: loss = 1.855323076248169\n",
            "step 11598: loss = 2.1574432849884033\n",
            "step 11599: loss = 1.7824116945266724\n",
            "step 11600: loss = 1.8912255764007568\n",
            "step 11601: loss = 2.0782768726348877\n",
            "step 11602: loss = 2.0628700256347656\n",
            "step 11603: loss = 2.0722200870513916\n",
            "step 11604: loss = 1.8866462707519531\n",
            "step 11605: loss = 2.010462760925293\n",
            "step 11606: loss = 1.8419550657272339\n",
            "step 11607: loss = 1.7381037473678589\n",
            "step 11608: loss = 1.9522801637649536\n",
            "step 11609: loss = 1.8121082782745361\n",
            "step 11610: loss = 1.95131254196167\n",
            "step 11611: loss = 1.8985166549682617\n",
            "step 11612: loss = 2.0763070583343506\n",
            "step 11613: loss = 1.9621864557266235\n",
            "step 11614: loss = 1.984856128692627\n",
            "step 11615: loss = 2.268296003341675\n",
            "step 11616: loss = 1.929848074913025\n",
            "step 11617: loss = 1.8612091541290283\n",
            "step 11618: loss = 1.7767727375030518\n",
            "step 11619: loss = 1.8620713949203491\n",
            "step 11620: loss = 2.060342311859131\n",
            "step 11621: loss = 1.9461250305175781\n",
            "step 11622: loss = 2.14811372756958\n",
            "step 11623: loss = 1.7759058475494385\n",
            "step 11624: loss = 1.7818620204925537\n",
            "step 11625: loss = 2.0852465629577637\n",
            "step 11626: loss = 1.9805302619934082\n",
            "step 11627: loss = 2.0251572132110596\n",
            "step 11628: loss = 2.25189208984375\n",
            "step 11629: loss = 2.0161428451538086\n",
            "step 11630: loss = 1.83628249168396\n",
            "step 11631: loss = 1.9532095193862915\n",
            "step 11632: loss = 2.0133047103881836\n",
            "step 11633: loss = 2.0558409690856934\n",
            "step 11634: loss = 1.925818681716919\n",
            "step 11635: loss = 2.044191837310791\n",
            "step 11636: loss = 2.000398874282837\n",
            "step 11637: loss = 1.9933149814605713\n",
            "step 11638: loss = 1.8823541402816772\n",
            "step 11639: loss = 2.00571346282959\n",
            "step 11640: loss = 1.8908528089523315\n",
            "step 11641: loss = 1.961560606956482\n",
            "step 11642: loss = 1.9526689052581787\n",
            "step 11643: loss = 1.9500685930252075\n",
            "step 11644: loss = 1.9478312730789185\n",
            "step 11645: loss = 1.822603464126587\n",
            "step 11646: loss = 1.9575412273406982\n",
            "step 11647: loss = 1.935336947441101\n",
            "step 11648: loss = 2.0087578296661377\n",
            "step 11649: loss = 1.7740353345870972\n",
            "step 11650: loss = 1.8820549249649048\n",
            "step 11651: loss = 1.6989648342132568\n",
            "step 11652: loss = 2.1052732467651367\n",
            "step 11653: loss = 1.9237946271896362\n",
            "step 11654: loss = 1.8795298337936401\n",
            "step 11655: loss = 1.992951512336731\n",
            "step 11656: loss = 1.8437894582748413\n",
            "step 11657: loss = 1.8597545623779297\n",
            "step 11658: loss = 1.9148060083389282\n",
            "step 11659: loss = 2.0200695991516113\n",
            "step 11660: loss = 1.9288500547409058\n",
            "step 11661: loss = 1.9554193019866943\n",
            "step 11662: loss = 1.9145749807357788\n",
            "step 11663: loss = 1.8957598209381104\n",
            "step 11664: loss = 1.952162265777588\n",
            "step 11665: loss = 1.8093780279159546\n",
            "step 11666: loss = 1.8491451740264893\n",
            "step 11667: loss = 1.7849233150482178\n",
            "step 11668: loss = 2.3186964988708496\n",
            "step 11669: loss = 1.924230694770813\n",
            "step 11670: loss = 1.8209450244903564\n",
            "step 11671: loss = 2.2482736110687256\n",
            "step 11672: loss = 1.911224126815796\n",
            "step 11673: loss = 1.870160698890686\n",
            "step 11674: loss = 2.155230760574341\n",
            "step 11675: loss = 1.9437061548233032\n",
            "step 11676: loss = 1.8959132432937622\n",
            "step 11677: loss = 2.202124834060669\n",
            "step 11678: loss = 2.2145488262176514\n",
            "step 11679: loss = 1.9650589227676392\n",
            "step 11680: loss = 1.9967061281204224\n",
            "step 11681: loss = 1.9188388586044312\n",
            "step 11682: loss = 1.9739400148391724\n",
            "step 11683: loss = 2.152623414993286\n",
            "step 11684: loss = 1.6325385570526123\n",
            "step 11685: loss = 1.8887076377868652\n",
            "step 11686: loss = 2.01737642288208\n",
            "step 11687: loss = 1.9587687253952026\n",
            "step 11688: loss = 1.9102460145950317\n",
            "step 11689: loss = 1.871233344078064\n",
            "step 11690: loss = 2.1174848079681396\n",
            "step 11691: loss = 1.7792344093322754\n",
            "step 11692: loss = 2.189687728881836\n",
            "step 11693: loss = 1.8442816734313965\n",
            "step 11694: loss = 2.1792068481445312\n",
            "step 11695: loss = 1.8401999473571777\n",
            "step 11696: loss = 1.9793964624404907\n",
            "step 11697: loss = 1.990301251411438\n",
            "step 11698: loss = 1.7528313398361206\n",
            "step 11699: loss = 2.187291145324707\n",
            "step 11700: loss = 2.0363996028900146\n",
            "step 11701: loss = 2.205894708633423\n",
            "step 11702: loss = 1.9904836416244507\n",
            "step 11703: loss = 2.0248005390167236\n",
            "step 11704: loss = 1.943185567855835\n",
            "step 11705: loss = 2.102576732635498\n",
            "step 11706: loss = 1.9894357919692993\n",
            "step 11707: loss = 2.09645414352417\n",
            "step 11708: loss = 1.937395453453064\n",
            "step 11709: loss = 1.7607604265213013\n",
            "step 11710: loss = 1.8017578125\n",
            "step 11711: loss = 2.0494651794433594\n",
            "step 11712: loss = 2.0972232818603516\n",
            "step 11713: loss = 2.264033555984497\n",
            "step 11714: loss = 2.1627326011657715\n",
            "step 11715: loss = 1.9884809255599976\n",
            "step 11716: loss = 2.1043405532836914\n",
            "step 11717: loss = 1.978079080581665\n",
            "step 11718: loss = 2.1384642124176025\n",
            "step 11719: loss = 2.168820858001709\n",
            "step 11720: loss = 1.890017032623291\n",
            "step 11721: loss = 2.157574415206909\n",
            "step 11722: loss = 2.129028797149658\n",
            "step 11723: loss = 1.9011021852493286\n",
            "step 11724: loss = 2.078340530395508\n",
            "step 11725: loss = 1.6973459720611572\n",
            "step 11726: loss = 1.980063796043396\n",
            "step 11727: loss = 2.136202812194824\n",
            "step 11728: loss = 2.0496299266815186\n",
            "step 11729: loss = 1.9626976251602173\n",
            "step 11730: loss = 1.7700319290161133\n",
            "step 11731: loss = 1.7413023710250854\n",
            "step 11732: loss = 1.8362635374069214\n",
            "step 11733: loss = 1.9337846040725708\n",
            "step 11734: loss = 2.2319698333740234\n",
            "step 11735: loss = 2.0668540000915527\n",
            "step 11736: loss = 1.8323214054107666\n",
            "step 11737: loss = 2.2131636142730713\n",
            "step 11738: loss = 2.0427639484405518\n",
            "step 11739: loss = 2.0056090354919434\n",
            "step 11740: loss = 2.0171241760253906\n",
            "step 11741: loss = 2.1206812858581543\n",
            "step 11742: loss = 1.8749959468841553\n",
            "step 11743: loss = 1.8191243410110474\n",
            "step 11744: loss = 1.7206332683563232\n",
            "step 11745: loss = 1.8678990602493286\n",
            "step 11746: loss = 2.1818087100982666\n",
            "step 11747: loss = 2.1445257663726807\n",
            "step 11748: loss = 1.8739105463027954\n",
            "step 11749: loss = 2.033052921295166\n",
            "step 11750: loss = 2.1024534702301025\n",
            "step 11751: loss = 1.922882318496704\n",
            "step 11752: loss = 1.970960021018982\n",
            "step 11753: loss = 2.0932936668395996\n",
            "step 11754: loss = 1.9399100542068481\n",
            "step 11755: loss = 2.0489234924316406\n",
            "step 11756: loss = 1.8371015787124634\n",
            "step 11757: loss = 1.8383910655975342\n",
            "step 11758: loss = 2.186490774154663\n",
            "step 11759: loss = 1.9684975147247314\n",
            "step 11760: loss = 2.179734468460083\n",
            "step 11761: loss = 2.0091497898101807\n",
            "step 11762: loss = 1.8700679540634155\n",
            "step 11763: loss = 2.0896477699279785\n",
            "step 11764: loss = 2.106029510498047\n",
            "step 11765: loss = 1.9778023958206177\n",
            "step 11766: loss = 2.1968934535980225\n",
            "step 11767: loss = 1.8816566467285156\n",
            "step 11768: loss = 1.8704369068145752\n",
            "step 11769: loss = 2.010375499725342\n",
            "step 11770: loss = 2.0121216773986816\n",
            "step 11771: loss = 1.9167611598968506\n",
            "step 11772: loss = 2.01029634475708\n",
            "step 11773: loss = 1.7809239625930786\n",
            "step 11774: loss = 2.0977120399475098\n",
            "step 11775: loss = 1.8485416173934937\n",
            "step 11776: loss = 1.9104163646697998\n",
            "step 11777: loss = 2.3131442070007324\n",
            "step 11778: loss = 2.220668315887451\n",
            "step 11779: loss = 2.1142072677612305\n",
            "step 11780: loss = 1.9983083009719849\n",
            "step 11781: loss = 2.1376843452453613\n",
            "step 11782: loss = 1.8554867506027222\n",
            "step 11783: loss = 1.8964214324951172\n",
            "step 11784: loss = 2.0380859375\n",
            "step 11785: loss = 2.1509130001068115\n",
            "step 11786: loss = 2.0861735343933105\n",
            "step 11787: loss = 2.0579562187194824\n",
            "step 11788: loss = 2.2264657020568848\n",
            "step 11789: loss = 1.998136043548584\n",
            "step 11790: loss = 2.312626600265503\n",
            "step 11791: loss = 1.9415063858032227\n",
            "step 11792: loss = 1.8437180519104004\n",
            "step 11793: loss = 1.926672101020813\n",
            "step 11794: loss = 1.9812617301940918\n",
            "step 11795: loss = 1.9028652906417847\n",
            "step 11796: loss = 1.9081153869628906\n",
            "step 11797: loss = 2.2016799449920654\n",
            "step 11798: loss = 2.1042633056640625\n",
            "step 11799: loss = 2.0454297065734863\n",
            "step 11800: loss = 1.9471086263656616\n",
            "step 11801: loss = 1.7891511917114258\n",
            "step 11802: loss = 1.9115475416183472\n",
            "step 11803: loss = 1.9996421337127686\n",
            "step 11804: loss = 2.105050802230835\n",
            "step 11805: loss = 2.0030059814453125\n",
            "step 11806: loss = 2.020425319671631\n",
            "step 11807: loss = 2.0708422660827637\n",
            "step 11808: loss = 1.9625988006591797\n",
            "step 11809: loss = 1.8260935544967651\n",
            "step 11810: loss = 2.0033693313598633\n",
            "step 11811: loss = 1.9399086236953735\n",
            "step 11812: loss = 1.913465976715088\n",
            "step 11813: loss = 2.034355401992798\n",
            "step 11814: loss = 2.0823960304260254\n",
            "step 11815: loss = 1.710978388786316\n",
            "step 11816: loss = 1.9843519926071167\n",
            "step 11817: loss = 1.8433911800384521\n",
            "step 11818: loss = 2.184373378753662\n",
            "step 11819: loss = 2.0936050415039062\n",
            "step 11820: loss = 2.1928350925445557\n",
            "step 11821: loss = 2.0250067710876465\n",
            "step 11822: loss = 2.18381404876709\n",
            "step 11823: loss = 2.0512001514434814\n",
            "step 11824: loss = 2.0732085704803467\n",
            "step 11825: loss = 1.8927741050720215\n",
            "step 11826: loss = 1.811818242073059\n",
            "step 11827: loss = 2.0255777835845947\n",
            "step 11828: loss = 1.770154595375061\n",
            "step 11829: loss = 2.082064151763916\n",
            "step 11830: loss = 2.1383230686187744\n",
            "step 11831: loss = 1.8053014278411865\n",
            "step 11832: loss = 1.9136359691619873\n",
            "step 11833: loss = 1.758533000946045\n",
            "step 11834: loss = 2.008068323135376\n",
            "step 11835: loss = 2.0470666885375977\n",
            "step 11836: loss = 2.2532777786254883\n",
            "step 11837: loss = 1.9429203271865845\n",
            "step 11838: loss = 1.9261457920074463\n",
            "step 11839: loss = 2.058657646179199\n",
            "step 11840: loss = 1.9450522661209106\n",
            "step 11841: loss = 2.0335891246795654\n",
            "step 11842: loss = 2.141556739807129\n",
            "step 11843: loss = 2.143308162689209\n",
            "step 11844: loss = 1.9552571773529053\n",
            "step 11845: loss = 2.40401554107666\n",
            "step 11846: loss = 2.076993703842163\n",
            "step 11847: loss = 1.9769117832183838\n",
            "step 11848: loss = 1.963979721069336\n",
            "step 11849: loss = 1.8977631330490112\n",
            "step 11850: loss = 2.1431808471679688\n",
            "step 11851: loss = 2.038893699645996\n",
            "step 11852: loss = 1.9548853635787964\n",
            "step 11853: loss = 1.9309098720550537\n",
            "step 11854: loss = 2.0578768253326416\n",
            "step 11855: loss = 2.0463743209838867\n",
            "step 11856: loss = 2.216515302658081\n",
            "step 11857: loss = 1.9741204977035522\n",
            "step 11858: loss = 1.9096181392669678\n",
            "step 11859: loss = 1.9422670602798462\n",
            "step 11860: loss = 2.224675178527832\n",
            "step 11861: loss = 2.0067873001098633\n",
            "step 11862: loss = 1.9245857000350952\n",
            "step 11863: loss = 1.9735969305038452\n",
            "step 11864: loss = 2.059805154800415\n",
            "step 11865: loss = 2.120715379714966\n",
            "step 11866: loss = 2.0491628646850586\n",
            "step 11867: loss = 2.035649538040161\n",
            "step 11868: loss = 1.8465608358383179\n",
            "step 11869: loss = 1.930944561958313\n",
            "step 11870: loss = 1.9072175025939941\n",
            "step 11871: loss = 2.344836711883545\n",
            "step 11872: loss = 1.8581491708755493\n",
            "step 11873: loss = 1.9650307893753052\n",
            "step 11874: loss = 2.0914952754974365\n",
            "step 11875: loss = 2.016862630844116\n",
            "step 11876: loss = 1.9445643424987793\n",
            "step 11877: loss = 2.3363208770751953\n",
            "step 11878: loss = 2.084585428237915\n",
            "step 11879: loss = 1.691046953201294\n",
            "step 11880: loss = 2.0241146087646484\n",
            "step 11881: loss = 2.045539379119873\n",
            "step 11882: loss = 2.076432943344116\n",
            "step 11883: loss = 1.9297451972961426\n",
            "step 11884: loss = 2.212620973587036\n",
            "step 11885: loss = 2.0998058319091797\n",
            "step 11886: loss = 2.177615165710449\n",
            "step 11887: loss = 2.018935203552246\n",
            "step 11888: loss = 1.8687682151794434\n",
            "step 11889: loss = 2.0251834392547607\n",
            "step 11890: loss = 1.8242543935775757\n",
            "step 11891: loss = 2.0501186847686768\n",
            "step 11892: loss = 2.021463632583618\n",
            "step 11893: loss = 1.9399681091308594\n",
            "step 11894: loss = 1.8718746900558472\n",
            "step 11895: loss = 2.1324496269226074\n",
            "step 11896: loss = 1.8442333936691284\n",
            "step 11897: loss = 2.067936658859253\n",
            "step 11898: loss = 2.1544251441955566\n",
            "step 11899: loss = 2.1540989875793457\n",
            "step 11900: loss = 2.2400052547454834\n",
            "step 11901: loss = 2.078076124191284\n",
            "step 11902: loss = 1.9274486303329468\n",
            "step 11903: loss = 1.9945186376571655\n",
            "step 11904: loss = 2.079038143157959\n",
            "step 11905: loss = 1.9835401773452759\n",
            "step 11906: loss = 2.0989060401916504\n",
            "step 11907: loss = 2.213534116744995\n",
            "step 11908: loss = 2.031054735183716\n",
            "step 11909: loss = 2.184537887573242\n",
            "step 11910: loss = 2.2278695106506348\n",
            "step 11911: loss = 2.1602530479431152\n",
            "step 11912: loss = 2.0314486026763916\n",
            "step 11913: loss = 1.9841294288635254\n",
            "step 11914: loss = 2.165715456008911\n",
            "step 11915: loss = 2.0138769149780273\n",
            "step 11916: loss = 1.915911316871643\n",
            "step 11917: loss = 2.119629383087158\n",
            "step 11918: loss = 2.131788492202759\n",
            "step 11919: loss = 1.7677279710769653\n",
            "step 11920: loss = 2.096747636795044\n",
            "step 11921: loss = 2.1452510356903076\n",
            "step 11922: loss = 1.7161753177642822\n",
            "step 11923: loss = 2.135450839996338\n",
            "step 11924: loss = 2.2363650798797607\n",
            "step 11925: loss = 2.2866697311401367\n",
            "step 11926: loss = 2.06272292137146\n",
            "step 11927: loss = 1.9664785861968994\n",
            "step 11928: loss = 2.2163803577423096\n",
            "step 11929: loss = 1.9829975366592407\n",
            "step 11930: loss = 2.0689823627471924\n",
            "step 11931: loss = 1.9329367876052856\n",
            "step 11932: loss = 2.1408326625823975\n",
            "step 11933: loss = 1.746440052986145\n",
            "step 11934: loss = 2.079389810562134\n",
            "step 11935: loss = 2.1480560302734375\n",
            "step 11936: loss = 2.033162832260132\n",
            "step 11937: loss = 2.032611608505249\n",
            "step 11938: loss = 2.0550754070281982\n",
            "step 11939: loss = 2.0034806728363037\n",
            "step 11940: loss = 2.1560280323028564\n",
            "step 11941: loss = 2.053194999694824\n",
            "step 11942: loss = 2.144988775253296\n",
            "step 11943: loss = 2.10884690284729\n",
            "step 11944: loss = 2.043369770050049\n",
            "step 11945: loss = 2.249807357788086\n",
            "step 11946: loss = 2.250153064727783\n",
            "step 11947: loss = 1.960758090019226\n",
            "step 11948: loss = 2.2937400341033936\n",
            "step 11949: loss = 1.934361219406128\n",
            "step 11950: loss = 2.16751766204834\n",
            "step 11951: loss = 1.90357506275177\n",
            "step 11952: loss = 1.6881657838821411\n",
            "step 11953: loss = 1.9203243255615234\n",
            "step 11954: loss = 2.169851064682007\n",
            "step 11955: loss = 2.1228809356689453\n",
            "step 11956: loss = 2.1498124599456787\n",
            "step 11957: loss = 1.9223136901855469\n",
            "step 11958: loss = 2.0801541805267334\n",
            "step 11959: loss = 2.071749687194824\n",
            "step 11960: loss = 2.265134334564209\n",
            "step 11961: loss = 2.109612226486206\n",
            "step 11962: loss = 2.1763827800750732\n",
            "step 11963: loss = 2.048008441925049\n",
            "step 11964: loss = 1.8043947219848633\n",
            "step 11965: loss = 2.0157291889190674\n",
            "step 11966: loss = 2.252495765686035\n",
            "step 11967: loss = 1.935902714729309\n",
            "step 11968: loss = 2.1008574962615967\n",
            "step 11969: loss = 1.8937779664993286\n",
            "step 11970: loss = 2.1428685188293457\n",
            "step 11971: loss = 2.0615859031677246\n",
            "step 11972: loss = 1.9840384721755981\n",
            "step 11973: loss = 1.9930155277252197\n",
            "step 11974: loss = 2.010162830352783\n",
            "step 11975: loss = 2.0302352905273438\n",
            "step 11976: loss = 1.9813776016235352\n",
            "step 11977: loss = 2.139618158340454\n",
            "step 11978: loss = 1.973976969718933\n",
            "step 11979: loss = 1.8392261266708374\n",
            "step 11980: loss = 2.0192344188690186\n",
            "step 11981: loss = 1.9740924835205078\n",
            "step 11982: loss = 1.9652349948883057\n",
            "step 11983: loss = 1.9409401416778564\n",
            "step 11984: loss = 1.8952741622924805\n",
            "step 11985: loss = 2.02351975440979\n",
            "step 11986: loss = 2.1083080768585205\n",
            "step 11987: loss = 1.928999662399292\n",
            "step 11988: loss = 2.268714427947998\n",
            "step 11989: loss = 2.0871524810791016\n",
            "step 11990: loss = 1.7679332494735718\n",
            "step 11991: loss = 1.9169750213623047\n",
            "step 11992: loss = 1.9384229183197021\n",
            "step 11993: loss = 2.014693260192871\n",
            "step 11994: loss = 2.132045269012451\n",
            "step 11995: loss = 1.8862026929855347\n",
            "step 11996: loss = 2.0424818992614746\n",
            "step 11997: loss = 2.0108180046081543\n",
            "step 11998: loss = 2.1493711471557617\n",
            "step 11999: loss = 2.157741069793701\n",
            "step 12000: loss = 1.8233915567398071\n",
            "step 12001: loss = 1.9618501663208008\n",
            "step 12002: loss = 2.0585110187530518\n",
            "step 12003: loss = 2.085219383239746\n",
            "step 12004: loss = 2.0009522438049316\n",
            "step 12005: loss = 2.037762403488159\n",
            "step 12006: loss = 2.0623769760131836\n",
            "step 12007: loss = 1.9647231101989746\n",
            "step 12008: loss = 2.0912868976593018\n",
            "step 12009: loss = 2.2593538761138916\n",
            "step 12010: loss = 2.2160749435424805\n",
            "step 12011: loss = 2.267982244491577\n",
            "step 12012: loss = 2.148113965988159\n",
            "step 12013: loss = 2.082939624786377\n",
            "step 12014: loss = 1.965275764465332\n",
            "step 12015: loss = 1.8498852252960205\n",
            "step 12016: loss = 2.3658640384674072\n",
            "step 12017: loss = 1.9633173942565918\n",
            "step 12018: loss = 1.8445944786071777\n",
            "step 12019: loss = 1.916943073272705\n",
            "step 12020: loss = 1.9911367893218994\n",
            "step 12021: loss = 2.172302484512329\n",
            "step 12022: loss = 1.8911854028701782\n",
            "step 12023: loss = 2.118903398513794\n",
            "step 12024: loss = 1.8108617067337036\n",
            "step 12025: loss = 2.1138906478881836\n",
            "step 12026: loss = 2.0443739891052246\n",
            "step 12027: loss = 2.0463452339172363\n",
            "step 12028: loss = 2.107346296310425\n",
            "step 12029: loss = 1.8961745500564575\n",
            "step 12030: loss = 1.9982279539108276\n",
            "step 12031: loss = 2.1072587966918945\n",
            "step 12032: loss = 1.7765101194381714\n",
            "step 12033: loss = 1.9607088565826416\n",
            "step 12034: loss = 1.9016786813735962\n",
            "step 12035: loss = 2.00515079498291\n",
            "step 12036: loss = 2.2058846950531006\n",
            "step 12037: loss = 2.0915348529815674\n",
            "step 12038: loss = 1.8031762838363647\n",
            "step 12039: loss = 1.900645136833191\n",
            "step 12040: loss = 1.918556809425354\n",
            "step 12041: loss = 2.1713664531707764\n",
            "step 12042: loss = 2.1153275966644287\n",
            "step 12043: loss = 1.972408652305603\n",
            "step 12044: loss = 1.7716816663742065\n",
            "step 12045: loss = 2.082585573196411\n",
            "step 12046: loss = 2.0616872310638428\n",
            "step 12047: loss = 2.2886104583740234\n",
            "step 12048: loss = 2.0742247104644775\n",
            "step 12049: loss = 1.9913619756698608\n",
            "step 12050: loss = 2.095463752746582\n",
            "step 12051: loss = 2.4061524868011475\n",
            "step 12052: loss = 1.9078844785690308\n",
            "step 12053: loss = 1.9790995121002197\n",
            "step 12054: loss = 2.0556986331939697\n",
            "step 12055: loss = 2.047409772872925\n",
            "step 12056: loss = 1.884833574295044\n",
            "step 12057: loss = 1.7540994882583618\n",
            "step 12058: loss = 1.9089288711547852\n",
            "step 12059: loss = 2.2058403491973877\n",
            "step 12060: loss = 2.1127898693084717\n",
            "step 12061: loss = 1.8595807552337646\n",
            "step 12062: loss = 2.2237634658813477\n",
            "step 12063: loss = 2.2297799587249756\n",
            "step 12064: loss = 2.0502028465270996\n",
            "step 12065: loss = 2.073821783065796\n",
            "step 12066: loss = 2.2126965522766113\n",
            "step 12067: loss = 2.0674381256103516\n",
            "step 12068: loss = 2.0208210945129395\n",
            "step 12069: loss = 1.8982033729553223\n",
            "step 12070: loss = 1.9462970495224\n",
            "step 12071: loss = 2.251887798309326\n",
            "step 12072: loss = 2.0390124320983887\n",
            "step 12073: loss = 2.0904862880706787\n",
            "step 12074: loss = 2.0259552001953125\n",
            "step 12075: loss = 1.9538010358810425\n",
            "step 12076: loss = 2.112781286239624\n",
            "step 12077: loss = 1.9345130920410156\n",
            "step 12078: loss = 1.9713876247406006\n",
            "step 12079: loss = 1.93536376953125\n",
            "step 12080: loss = 2.006298780441284\n",
            "step 12081: loss = 2.0903053283691406\n",
            "step 12082: loss = 2.0318455696105957\n",
            "step 12083: loss = 1.7599152326583862\n",
            "step 12084: loss = 1.834956407546997\n",
            "step 12085: loss = 2.0766866207122803\n",
            "step 12086: loss = 2.080955982208252\n",
            "step 12087: loss = 1.9956374168395996\n",
            "step 12088: loss = 2.291524887084961\n",
            "step 12089: loss = 1.973122239112854\n",
            "step 12090: loss = 2.1776070594787598\n",
            "step 12091: loss = 2.1525630950927734\n",
            "step 12092: loss = 1.8836276531219482\n",
            "step 12093: loss = 2.0002927780151367\n",
            "step 12094: loss = 2.067316770553589\n",
            "step 12095: loss = 1.6927083730697632\n",
            "step 12096: loss = 2.021806001663208\n",
            "step 12097: loss = 1.8100221157073975\n",
            "step 12098: loss = 2.00004506111145\n",
            "step 12099: loss = 1.8585121631622314\n",
            "step 12100: loss = 2.0968527793884277\n",
            "step 12101: loss = 2.1968250274658203\n",
            "step 12102: loss = 2.0466859340667725\n",
            "step 12103: loss = 2.0662119388580322\n",
            "step 12104: loss = 2.056426763534546\n",
            "step 12105: loss = 2.174588680267334\n",
            "step 12106: loss = 2.11676287651062\n",
            "step 12107: loss = 2.052438259124756\n",
            "step 12108: loss = 1.8081377744674683\n",
            "step 12109: loss = 2.2058300971984863\n",
            "step 12110: loss = 1.733730673789978\n",
            "step 12111: loss = 2.1898250579833984\n",
            "step 12112: loss = 1.9657678604125977\n",
            "step 12113: loss = 2.3605167865753174\n",
            "step 12114: loss = 1.9012651443481445\n",
            "step 12115: loss = 2.0429303646087646\n",
            "step 12116: loss = 1.8438509702682495\n",
            "step 12117: loss = 2.0418436527252197\n",
            "step 12118: loss = 1.8925511837005615\n",
            "step 12119: loss = 2.2026336193084717\n",
            "step 12120: loss = 1.8564459085464478\n",
            "step 12121: loss = 1.8351547718048096\n",
            "step 12122: loss = 2.0965805053710938\n",
            "step 12123: loss = 2.235363483428955\n",
            "step 12124: loss = 2.1436405181884766\n",
            "step 12125: loss = 1.9321913719177246\n",
            "step 12126: loss = 2.0624144077301025\n",
            "step 12127: loss = 2.0480010509490967\n",
            "step 12128: loss = 1.9588552713394165\n",
            "step 12129: loss = 2.445025682449341\n",
            "step 12130: loss = 2.0117907524108887\n",
            "step 12131: loss = 1.9889334440231323\n",
            "step 12132: loss = 1.9509327411651611\n",
            "step 12133: loss = 2.0805952548980713\n",
            "step 12134: loss = 1.9687774181365967\n",
            "step 12135: loss = 2.192586898803711\n",
            "step 12136: loss = 2.1182711124420166\n",
            "step 12137: loss = 2.027107000350952\n",
            "step 12138: loss = 2.265493869781494\n",
            "step 12139: loss = 2.142345905303955\n",
            "step 12140: loss = 1.963650107383728\n",
            "step 12141: loss = 2.0148417949676514\n",
            "step 12142: loss = 2.0881097316741943\n",
            "step 12143: loss = 2.2198095321655273\n",
            "step 12144: loss = 1.9677623510360718\n",
            "step 12145: loss = 1.9540014266967773\n",
            "step 12146: loss = 2.018562078475952\n",
            "step 12147: loss = 2.1809961795806885\n",
            "step 12148: loss = 2.1046135425567627\n",
            "step 12149: loss = 1.9280824661254883\n",
            "step 12150: loss = 2.0058367252349854\n",
            "step 12151: loss = 1.8757320642471313\n",
            "step 12152: loss = 2.1213464736938477\n",
            "step 12153: loss = 1.9540777206420898\n",
            "step 12154: loss = 1.9596575498580933\n",
            "step 12155: loss = 2.152123212814331\n",
            "step 12156: loss = 1.9216668605804443\n",
            "step 12157: loss = 1.8671395778656006\n",
            "step 12158: loss = 2.02834153175354\n",
            "step 12159: loss = 1.9845048189163208\n",
            "step 12160: loss = 1.9228057861328125\n",
            "step 12161: loss = 1.908837080001831\n",
            "step 12162: loss = 2.007575511932373\n",
            "step 12163: loss = 2.0139312744140625\n",
            "step 12164: loss = 1.9369133710861206\n",
            "step 12165: loss = 1.9036325216293335\n",
            "step 12166: loss = 1.9624074697494507\n",
            "step 12167: loss = 1.824705958366394\n",
            "step 12168: loss = 2.0704915523529053\n",
            "step 12169: loss = 1.9816749095916748\n",
            "step 12170: loss = 2.093871593475342\n",
            "step 12171: loss = 2.1661362648010254\n",
            "step 12172: loss = 1.7702176570892334\n",
            "step 12173: loss = 2.118211269378662\n",
            "step 12174: loss = 2.1378066539764404\n",
            "step 12175: loss = 2.0344738960266113\n",
            "step 12176: loss = 2.1280970573425293\n",
            "step 12177: loss = 1.947784185409546\n",
            "step 12178: loss = 2.070310115814209\n",
            "step 12179: loss = 2.106832265853882\n",
            "step 12180: loss = 1.9769212007522583\n",
            "step 12181: loss = 2.013340473175049\n",
            "step 12182: loss = 1.97810697555542\n",
            "step 12183: loss = 2.2284915447235107\n",
            "step 12184: loss = 2.3138959407806396\n",
            "step 12185: loss = 2.087749719619751\n",
            "step 12186: loss = 1.9507472515106201\n",
            "step 12187: loss = 1.9565863609313965\n",
            "step 12188: loss = 2.305499315261841\n",
            "step 12189: loss = 2.065790891647339\n",
            "step 12190: loss = 1.9437509775161743\n",
            "step 12191: loss = 2.2461297512054443\n",
            "step 12192: loss = 2.1788737773895264\n",
            "step 12193: loss = 1.9456665515899658\n",
            "step 12194: loss = 2.008490562438965\n",
            "step 12195: loss = 1.94156813621521\n",
            "step 12196: loss = 2.3419108390808105\n",
            "step 12197: loss = 2.060835123062134\n",
            "step 12198: loss = 1.8605186939239502\n",
            "step 12199: loss = 1.8609780073165894\n",
            "step 12200: loss = 2.09458589553833\n",
            "step 12201: loss = 1.9522114992141724\n",
            "step 12202: loss = 1.9535576105117798\n",
            "step 12203: loss = 2.117249011993408\n",
            "step 12204: loss = 2.0374817848205566\n",
            "step 12205: loss = 2.0424344539642334\n",
            "step 12206: loss = 2.0056262016296387\n",
            "step 12207: loss = 2.1270108222961426\n",
            "step 12208: loss = 2.0865478515625\n",
            "step 12209: loss = 2.031447649002075\n",
            "step 12210: loss = 2.022423028945923\n",
            "step 12211: loss = 2.0824334621429443\n",
            "step 12212: loss = 1.8933359384536743\n",
            "step 12213: loss = 1.8482173681259155\n",
            "step 12214: loss = 2.3091979026794434\n",
            "step 12215: loss = 2.101611375808716\n",
            "step 12216: loss = 2.010152578353882\n",
            "step 12217: loss = 1.909090518951416\n",
            "step 12218: loss = 2.2219130992889404\n",
            "step 12219: loss = 2.057926893234253\n",
            "step 12220: loss = 2.150329351425171\n",
            "step 12221: loss = 2.0667567253112793\n",
            "step 12222: loss = 1.6878130435943604\n",
            "step 12223: loss = 1.8331564664840698\n",
            "step 12224: loss = 1.9498646259307861\n",
            "step 12225: loss = 2.0352208614349365\n",
            "step 12226: loss = 2.024812698364258\n",
            "step 12227: loss = 2.2976057529449463\n",
            "step 12228: loss = 2.3381502628326416\n",
            "step 12229: loss = 1.977002501487732\n",
            "step 12230: loss = 1.9200993776321411\n",
            "step 12231: loss = 2.2433478832244873\n",
            "step 12232: loss = 2.1495680809020996\n",
            "step 12233: loss = 1.9286049604415894\n",
            "step 12234: loss = 2.241466522216797\n",
            "step 12235: loss = 2.000960350036621\n",
            "step 12236: loss = 2.044588565826416\n",
            "step 12237: loss = 2.5006744861602783\n",
            "step 12238: loss = 2.204667091369629\n",
            "step 12239: loss = 2.189631223678589\n",
            "step 12240: loss = 1.9959964752197266\n",
            "step 12241: loss = 2.068718194961548\n",
            "step 12242: loss = 1.9139572381973267\n",
            "step 12243: loss = 2.027265787124634\n",
            "step 12244: loss = 2.239626407623291\n",
            "step 12245: loss = 2.1653335094451904\n",
            "step 12246: loss = 2.0741961002349854\n",
            "step 12247: loss = 2.187389373779297\n",
            "step 12248: loss = 2.1250102519989014\n",
            "step 12249: loss = 1.931486964225769\n",
            "step 12250: loss = 1.980185866355896\n",
            "step 12251: loss = 2.011216163635254\n",
            "step 12252: loss = 2.13051438331604\n",
            "step 12253: loss = 2.496685266494751\n",
            "step 12254: loss = 2.025827407836914\n",
            "step 12255: loss = 2.2226476669311523\n",
            "step 12256: loss = 2.1744117736816406\n",
            "step 12257: loss = 2.186763286590576\n",
            "step 12258: loss = 2.041748046875\n",
            "step 12259: loss = 2.2893998622894287\n",
            "step 12260: loss = 1.9796223640441895\n",
            "step 12261: loss = 2.00406551361084\n",
            "step 12262: loss = 2.0223827362060547\n",
            "step 12263: loss = 2.0180976390838623\n",
            "step 12264: loss = 1.7616671323776245\n",
            "step 12265: loss = 2.214048385620117\n",
            "step 12266: loss = 2.1368072032928467\n",
            "step 12267: loss = 1.9162533283233643\n",
            "step 12268: loss = 2.0955536365509033\n",
            "step 12269: loss = 2.1192567348480225\n",
            "step 12270: loss = 2.109983444213867\n",
            "step 12271: loss = 2.0595600605010986\n",
            "step 12272: loss = 2.4223239421844482\n",
            "step 12273: loss = 1.8443862199783325\n",
            "step 12274: loss = 2.152920961380005\n",
            "step 12275: loss = 2.0881247520446777\n",
            "step 12276: loss = 2.090608835220337\n",
            "step 12277: loss = 1.8164787292480469\n",
            "step 12278: loss = 2.242609739303589\n",
            "step 12279: loss = 2.1365458965301514\n",
            "step 12280: loss = 2.201331853866577\n",
            "step 12281: loss = 2.244326114654541\n",
            "step 12282: loss = 1.6921747922897339\n",
            "step 12283: loss = 2.214841604232788\n",
            "step 12284: loss = 1.9722363948822021\n",
            "step 12285: loss = 2.0585834980010986\n",
            "step 12286: loss = 1.8818565607070923\n",
            "step 12287: loss = 2.2562150955200195\n",
            "step 12288: loss = 1.855717420578003\n",
            "step 12289: loss = 2.008040428161621\n",
            "step 12290: loss = 1.8779853582382202\n",
            "step 12291: loss = 1.7728561162948608\n",
            "step 12292: loss = 2.0690157413482666\n",
            "step 12293: loss = 1.9991496801376343\n",
            "step 12294: loss = 2.1175808906555176\n",
            "step 12295: loss = 1.8406869173049927\n",
            "step 12296: loss = 2.1362268924713135\n",
            "step 12297: loss = 1.8916174173355103\n",
            "step 12298: loss = 2.211904287338257\n",
            "step 12299: loss = 1.979276180267334\n",
            "step 12300: loss = 1.9189012050628662\n",
            "step 12301: loss = 2.008349657058716\n",
            "step 12302: loss = 2.269320249557495\n",
            "step 12303: loss = 2.1014888286590576\n",
            "step 12304: loss = 2.116039991378784\n",
            "step 12305: loss = 2.3261077404022217\n",
            "step 12306: loss = 2.062953472137451\n",
            "step 12307: loss = 1.8383936882019043\n",
            "step 12308: loss = 2.1563034057617188\n",
            "step 12309: loss = 1.8561365604400635\n",
            "step 12310: loss = 2.0916554927825928\n",
            "step 12311: loss = 2.22682523727417\n",
            "step 12312: loss = 2.2389256954193115\n",
            "step 12313: loss = 2.0190563201904297\n",
            "step 12314: loss = 1.7980189323425293\n",
            "step 12315: loss = 2.3354721069335938\n",
            "step 12316: loss = 2.3190438747406006\n",
            "step 12317: loss = 1.9218989610671997\n",
            "step 12318: loss = 2.074960708618164\n",
            "step 12319: loss = 1.9580484628677368\n",
            "step 12320: loss = 2.046041965484619\n",
            "step 12321: loss = 2.0512120723724365\n",
            "step 12322: loss = 2.0276105403900146\n",
            "step 12323: loss = 2.0912184715270996\n",
            "step 12324: loss = 1.7925852537155151\n",
            "step 12325: loss = 2.046459674835205\n",
            "step 12326: loss = 2.0960676670074463\n",
            "step 12327: loss = 2.3018076419830322\n",
            "step 12328: loss = 2.009788990020752\n",
            "step 12329: loss = 1.9948680400848389\n",
            "step 12330: loss = 1.9619224071502686\n",
            "step 12331: loss = 2.0429000854492188\n",
            "step 12332: loss = 2.160470724105835\n",
            "step 12333: loss = 2.1160244941711426\n",
            "step 12334: loss = 2.294984817504883\n",
            "step 12335: loss = 1.8410968780517578\n",
            "step 12336: loss = 1.9503716230392456\n",
            "step 12337: loss = 2.448582410812378\n",
            "step 12338: loss = 2.0322232246398926\n",
            "step 12339: loss = 1.942734956741333\n",
            "step 12340: loss = 1.8893077373504639\n",
            "step 12341: loss = 1.9559396505355835\n",
            "step 12342: loss = 1.9009199142456055\n",
            "step 12343: loss = 1.8808344602584839\n",
            "step 12344: loss = 1.963101863861084\n",
            "step 12345: loss = 2.172586441040039\n",
            "step 12346: loss = 2.2057156562805176\n",
            "step 12347: loss = 2.226860523223877\n",
            "step 12348: loss = 2.1244728565216064\n",
            "step 12349: loss = 2.0201754570007324\n",
            "step 12350: loss = 2.0237107276916504\n",
            "step 12351: loss = 2.1381499767303467\n",
            "step 12352: loss = 1.9917864799499512\n",
            "step 12353: loss = 2.1080987453460693\n",
            "step 12354: loss = 1.739615559577942\n",
            "step 12355: loss = 2.1807680130004883\n",
            "step 12356: loss = 1.998048186302185\n",
            "step 12357: loss = 2.091219663619995\n",
            "step 12358: loss = 1.8813714981079102\n",
            "step 12359: loss = 2.038978338241577\n",
            "step 12360: loss = 1.7949352264404297\n",
            "step 12361: loss = 2.0496041774749756\n",
            "step 12362: loss = 2.0657238960266113\n",
            "step 12363: loss = 2.0232701301574707\n",
            "step 12364: loss = 2.176754951477051\n",
            "step 12365: loss = 2.3002631664276123\n",
            "step 12366: loss = 2.0425314903259277\n",
            "step 12367: loss = 2.0328309535980225\n",
            "step 12368: loss = 2.2704739570617676\n",
            "step 12369: loss = 1.9405667781829834\n",
            "step 12370: loss = 2.0139999389648438\n",
            "step 12371: loss = 2.0784199237823486\n",
            "step 12372: loss = 1.9655277729034424\n",
            "step 12373: loss = 2.239084005355835\n",
            "step 12374: loss = 2.2088069915771484\n",
            "step 12375: loss = 2.0275981426239014\n",
            "step 12376: loss = 2.115799903869629\n",
            "step 12377: loss = 2.232459545135498\n",
            "step 12378: loss = 2.215776205062866\n",
            "step 12379: loss = 2.265554189682007\n",
            "step 12380: loss = 2.2469825744628906\n",
            "step 12381: loss = 2.0129737854003906\n",
            "step 12382: loss = 2.05173921585083\n",
            "step 12383: loss = 2.0709426403045654\n",
            "step 12384: loss = 1.9327054023742676\n",
            "step 12385: loss = 2.359055995941162\n",
            "step 12386: loss = 2.000584602355957\n",
            "step 12387: loss = 2.076019048690796\n",
            "step 12388: loss = 1.9622046947479248\n",
            "step 12389: loss = 1.9232406616210938\n",
            "step 12390: loss = 2.1126582622528076\n",
            "step 12391: loss = 1.9939532279968262\n",
            "step 12392: loss = 1.824217438697815\n",
            "step 12393: loss = 2.0471320152282715\n",
            "step 12394: loss = 1.9739649295806885\n",
            "step 12395: loss = 2.0387778282165527\n",
            "step 12396: loss = 2.1788218021392822\n",
            "step 12397: loss = 1.868119478225708\n",
            "step 12398: loss = 1.7804670333862305\n",
            "step 12399: loss = 2.0938990116119385\n",
            "step 12400: loss = 2.1874189376831055\n",
            "step 12401: loss = 2.115748882293701\n",
            "step 12402: loss = 2.0094363689422607\n",
            "step 12403: loss = 2.141404867172241\n",
            "step 12404: loss = 2.031198501586914\n",
            "step 12405: loss = 2.1070234775543213\n",
            "step 12406: loss = 2.1138882637023926\n",
            "step 12407: loss = 2.1162991523742676\n",
            "step 12408: loss = 1.7163771390914917\n",
            "step 12409: loss = 2.0033671855926514\n",
            "step 12410: loss = 2.0729804039001465\n",
            "step 12411: loss = 2.0061213970184326\n",
            "step 12412: loss = 1.9231986999511719\n",
            "step 12413: loss = 1.9646027088165283\n",
            "step 12414: loss = 1.863110899925232\n",
            "step 12415: loss = 2.0483999252319336\n",
            "step 12416: loss = 2.310377359390259\n",
            "step 12417: loss = 1.9127997159957886\n",
            "step 12418: loss = 2.0668463706970215\n",
            "step 12419: loss = 1.8806818723678589\n",
            "step 12420: loss = 1.9717516899108887\n",
            "step 12421: loss = 1.9990153312683105\n",
            "step 12422: loss = 2.1882972717285156\n",
            "step 12423: loss = 2.1710851192474365\n",
            "step 12424: loss = 2.1044273376464844\n",
            "step 12425: loss = 2.2576894760131836\n",
            "step 12426: loss = 2.2055139541625977\n",
            "step 12427: loss = 2.1340837478637695\n",
            "step 12428: loss = 2.1029467582702637\n",
            "step 12429: loss = 2.1065266132354736\n",
            "step 12430: loss = 2.309938430786133\n",
            "step 12431: loss = 2.1843526363372803\n",
            "step 12432: loss = 2.0384953022003174\n",
            "step 12433: loss = 2.011474609375\n",
            "step 12434: loss = 2.0773203372955322\n",
            "step 12435: loss = 1.8489960432052612\n",
            "step 12436: loss = 2.0418143272399902\n",
            "step 12437: loss = 2.230271816253662\n",
            "step 12438: loss = 2.0747082233428955\n",
            "step 12439: loss = 1.7479974031448364\n",
            "step 12440: loss = 2.0599968433380127\n",
            "step 12441: loss = 1.9217463731765747\n",
            "step 12442: loss = 2.119405746459961\n",
            "step 12443: loss = 1.9144407510757446\n",
            "step 12444: loss = 2.230766773223877\n",
            "step 12445: loss = 2.1568076610565186\n",
            "step 12446: loss = 1.9537742137908936\n",
            "step 12447: loss = 2.148467779159546\n",
            "step 12448: loss = 1.944745659828186\n",
            "step 12449: loss = 2.0042104721069336\n",
            "step 12450: loss = 1.9418824911117554\n",
            "step 12451: loss = 1.6646654605865479\n",
            "step 12452: loss = 2.083477258682251\n",
            "step 12453: loss = 1.8634788990020752\n",
            "step 12454: loss = 1.9418270587921143\n",
            "step 12455: loss = 1.88544499874115\n",
            "step 12456: loss = 2.065979480743408\n",
            "step 12457: loss = 2.2231690883636475\n",
            "step 12458: loss = 2.0287160873413086\n",
            "step 12459: loss = 2.005859136581421\n",
            "step 12460: loss = 2.1728157997131348\n",
            "step 12461: loss = 2.194251775741577\n",
            "step 12462: loss = 2.076613187789917\n",
            "step 12463: loss = 1.8750349283218384\n",
            "step 12464: loss = 1.8447139263153076\n",
            "step 12465: loss = 2.191594362258911\n",
            "step 12466: loss = 2.1842105388641357\n",
            "step 12467: loss = 2.1495447158813477\n",
            "step 12468: loss = 2.036078453063965\n",
            "step 12469: loss = 2.1359059810638428\n",
            "step 12470: loss = 1.9908517599105835\n",
            "step 12471: loss = 1.874457597732544\n",
            "step 12472: loss = 2.3126063346862793\n",
            "step 12473: loss = 1.8631893396377563\n",
            "step 12474: loss = 2.031660556793213\n",
            "step 12475: loss = 2.3719797134399414\n",
            "step 12476: loss = 2.0371553897857666\n",
            "step 12477: loss = 1.811340570449829\n",
            "step 12478: loss = 2.036036968231201\n",
            "step 12479: loss = 2.132047176361084\n",
            "step 12480: loss = 2.1654820442199707\n",
            "step 12481: loss = 2.179685115814209\n",
            "step 12482: loss = 2.2010016441345215\n",
            "step 12483: loss = 2.1208760738372803\n",
            "step 12484: loss = 2.1258950233459473\n",
            "step 12485: loss = 2.0932538509368896\n",
            "step 12486: loss = 2.3048739433288574\n",
            "step 12487: loss = 2.079407215118408\n",
            "step 12488: loss = 2.1541950702667236\n",
            "step 12489: loss = 2.0115318298339844\n",
            "step 12490: loss = 2.144911050796509\n",
            "step 12491: loss = 2.1125946044921875\n",
            "step 12492: loss = 1.965760350227356\n",
            "step 12493: loss = 2.2330970764160156\n",
            "step 12494: loss = 2.111360788345337\n",
            "step 12495: loss = 1.9954814910888672\n",
            "step 12496: loss = 1.7474371194839478\n",
            "Finish epoch 8\n",
            "New model saved, minimum loss: 1.9667309322674662 \n",
            "\n",
            "step 12497: loss = 1.7204735279083252\n",
            "step 12498: loss = 1.719740867614746\n",
            "step 12499: loss = 1.6531798839569092\n",
            "step 12500: loss = 1.6708735227584839\n",
            "step 12501: loss = 1.803539752960205\n",
            "step 12502: loss = 1.6156386137008667\n",
            "step 12503: loss = 1.9034442901611328\n",
            "step 12504: loss = 1.438071370124817\n",
            "step 12505: loss = 1.678151249885559\n",
            "step 12506: loss = 1.771219253540039\n",
            "step 12507: loss = 1.7400627136230469\n",
            "step 12508: loss = 1.6811635494232178\n",
            "step 12509: loss = 1.7364410161972046\n",
            "step 12510: loss = 1.6992493867874146\n",
            "step 12511: loss = 1.400976300239563\n",
            "step 12512: loss = 1.726972222328186\n",
            "step 12513: loss = 1.562247395515442\n",
            "step 12514: loss = 1.5729702711105347\n",
            "step 12515: loss = 1.6109613180160522\n",
            "step 12516: loss = 1.8739811182022095\n",
            "step 12517: loss = 1.7002463340759277\n",
            "step 12518: loss = 1.7461026906967163\n",
            "step 12519: loss = 1.5466526746749878\n",
            "step 12520: loss = 1.4835970401763916\n",
            "step 12521: loss = 1.6581491231918335\n",
            "step 12522: loss = 1.9091365337371826\n",
            "step 12523: loss = 1.6416444778442383\n",
            "step 12524: loss = 1.5395456552505493\n",
            "step 12525: loss = 1.5258686542510986\n",
            "step 12526: loss = 1.5724700689315796\n",
            "step 12527: loss = 1.392383337020874\n",
            "step 12528: loss = 1.5397155284881592\n",
            "step 12529: loss = 1.8431991338729858\n",
            "step 12530: loss = 1.634356141090393\n",
            "step 12531: loss = 1.6161521673202515\n",
            "step 12532: loss = 1.8134076595306396\n",
            "step 12533: loss = 1.7121117115020752\n",
            "step 12534: loss = 1.6301919221878052\n",
            "step 12535: loss = 1.6502342224121094\n",
            "step 12536: loss = 1.711094617843628\n",
            "step 12537: loss = 1.824193000793457\n",
            "step 12538: loss = 1.8614981174468994\n",
            "step 12539: loss = 1.6955846548080444\n",
            "step 12540: loss = 1.7648035287857056\n",
            "step 12541: loss = 1.6742695569992065\n",
            "step 12542: loss = 1.4621596336364746\n",
            "step 12543: loss = 1.6177550554275513\n",
            "step 12544: loss = 1.6283560991287231\n",
            "step 12545: loss = 1.7425389289855957\n",
            "step 12546: loss = 1.5973217487335205\n",
            "step 12547: loss = 1.7333773374557495\n",
            "step 12548: loss = 1.673801064491272\n",
            "step 12549: loss = 1.6540398597717285\n",
            "step 12550: loss = 1.4550907611846924\n",
            "step 12551: loss = 1.642812967300415\n",
            "step 12552: loss = 1.6066900491714478\n",
            "step 12553: loss = 1.8324007987976074\n",
            "step 12554: loss = 1.4916613101959229\n",
            "step 12555: loss = 1.7102079391479492\n",
            "step 12556: loss = 1.6665759086608887\n",
            "step 12557: loss = 1.6835691928863525\n",
            "step 12558: loss = 1.5134397745132446\n",
            "step 12559: loss = 1.6314492225646973\n",
            "step 12560: loss = 1.5297785997390747\n",
            "step 12561: loss = 1.6994043588638306\n",
            "step 12562: loss = 1.5731760263442993\n",
            "step 12563: loss = 1.6552202701568604\n",
            "step 12564: loss = 1.4179271459579468\n",
            "step 12565: loss = 1.5354549884796143\n",
            "step 12566: loss = 1.6817917823791504\n",
            "step 12567: loss = 1.7029688358306885\n",
            "step 12568: loss = 1.5979540348052979\n",
            "step 12569: loss = 1.8768534660339355\n",
            "step 12570: loss = 1.5349154472351074\n",
            "step 12571: loss = 1.6734904050827026\n",
            "step 12572: loss = 1.4965202808380127\n",
            "step 12573: loss = 1.5640093088150024\n",
            "step 12574: loss = 1.5985736846923828\n",
            "step 12575: loss = 1.7055952548980713\n",
            "step 12576: loss = 1.7441405057907104\n",
            "step 12577: loss = 1.7762446403503418\n",
            "step 12578: loss = 1.5332050323486328\n",
            "step 12579: loss = 1.6234755516052246\n",
            "step 12580: loss = 1.5365926027297974\n",
            "step 12581: loss = 1.5776302814483643\n",
            "step 12582: loss = 1.6441450119018555\n",
            "step 12583: loss = 1.7292841672897339\n",
            "step 12584: loss = 1.4547978639602661\n",
            "step 12585: loss = 1.7102564573287964\n",
            "step 12586: loss = 1.7069907188415527\n",
            "step 12587: loss = 1.514943242073059\n",
            "step 12588: loss = 1.6965491771697998\n",
            "step 12589: loss = 1.5478911399841309\n",
            "step 12590: loss = 1.5438456535339355\n",
            "step 12591: loss = 1.641318440437317\n",
            "step 12592: loss = 1.6459777355194092\n",
            "step 12593: loss = 1.738371729850769\n",
            "step 12594: loss = 1.548516869544983\n",
            "step 12595: loss = 1.7111810445785522\n",
            "step 12596: loss = 1.8554658889770508\n",
            "step 12597: loss = 1.7846920490264893\n",
            "step 12598: loss = 1.6022963523864746\n",
            "step 12599: loss = 1.8261141777038574\n",
            "step 12600: loss = 1.8405969142913818\n",
            "step 12601: loss = 1.402410864830017\n",
            "step 12602: loss = 1.595353126525879\n",
            "step 12603: loss = 1.8878458738327026\n",
            "step 12604: loss = 1.510640025138855\n",
            "step 12605: loss = 1.5990110635757446\n",
            "step 12606: loss = 1.7662031650543213\n",
            "step 12607: loss = 1.8955284357070923\n",
            "step 12608: loss = 1.563672423362732\n",
            "step 12609: loss = 1.510741949081421\n",
            "step 12610: loss = 1.648625135421753\n",
            "step 12611: loss = 1.8419132232666016\n",
            "step 12612: loss = 1.6433675289154053\n",
            "step 12613: loss = 1.5552390813827515\n",
            "step 12614: loss = 1.531358242034912\n",
            "step 12615: loss = 1.7424296140670776\n",
            "step 12616: loss = 1.6816891431808472\n",
            "step 12617: loss = 1.876404881477356\n",
            "step 12618: loss = 1.4516754150390625\n",
            "step 12619: loss = 1.5702041387557983\n",
            "step 12620: loss = 1.6493778228759766\n",
            "step 12621: loss = 1.5567240715026855\n",
            "step 12622: loss = 1.4871565103530884\n",
            "step 12623: loss = 1.4398247003555298\n",
            "step 12624: loss = 1.5448933839797974\n",
            "step 12625: loss = 1.6783912181854248\n",
            "step 12626: loss = 1.4466179609298706\n",
            "step 12627: loss = 1.5605875253677368\n",
            "step 12628: loss = 1.6216822862625122\n",
            "step 12629: loss = 1.633647084236145\n",
            "step 12630: loss = 1.7102129459381104\n",
            "step 12631: loss = 1.6939008235931396\n",
            "step 12632: loss = 1.73556649684906\n",
            "step 12633: loss = 1.6935863494873047\n",
            "step 12634: loss = 1.5556551218032837\n",
            "step 12635: loss = 1.7144256830215454\n",
            "step 12636: loss = 1.5449106693267822\n",
            "step 12637: loss = 1.6413840055465698\n",
            "step 12638: loss = 1.6362955570220947\n",
            "step 12639: loss = 1.8618216514587402\n",
            "step 12640: loss = 1.6166987419128418\n",
            "step 12641: loss = 1.5196081399917603\n",
            "step 12642: loss = 1.5740031003952026\n",
            "step 12643: loss = 1.6916369199752808\n",
            "step 12644: loss = 1.8425155878067017\n",
            "step 12645: loss = 1.5124961137771606\n",
            "step 12646: loss = 1.6656500101089478\n",
            "step 12647: loss = 1.8341617584228516\n",
            "step 12648: loss = 1.815782904624939\n",
            "step 12649: loss = 1.6237642765045166\n",
            "step 12650: loss = 1.7941224575042725\n",
            "step 12651: loss = 1.560910940170288\n",
            "step 12652: loss = 1.6130937337875366\n",
            "step 12653: loss = 1.6694644689559937\n",
            "step 12654: loss = 1.6971591711044312\n",
            "step 12655: loss = 1.829262137413025\n",
            "step 12656: loss = 1.6552326679229736\n",
            "step 12657: loss = 1.5415633916854858\n",
            "step 12658: loss = 1.7004060745239258\n",
            "step 12659: loss = 1.6626853942871094\n",
            "step 12660: loss = 1.6700516939163208\n",
            "step 12661: loss = 1.5315122604370117\n",
            "step 12662: loss = 1.5827723741531372\n",
            "step 12663: loss = 1.7483078241348267\n",
            "step 12664: loss = 1.55019211769104\n",
            "step 12665: loss = 1.713078260421753\n",
            "step 12666: loss = 1.6287413835525513\n",
            "step 12667: loss = 1.760951280593872\n",
            "step 12668: loss = 1.556643009185791\n",
            "step 12669: loss = 1.6617763042449951\n",
            "step 12670: loss = 1.7529828548431396\n",
            "step 12671: loss = 1.8505853414535522\n",
            "step 12672: loss = 1.6443971395492554\n",
            "step 12673: loss = 1.8378620147705078\n",
            "step 12674: loss = 1.87855064868927\n",
            "step 12675: loss = 1.790446400642395\n",
            "step 12676: loss = 1.7352997064590454\n",
            "step 12677: loss = 1.5806729793548584\n",
            "step 12678: loss = 1.7881619930267334\n",
            "step 12679: loss = 1.5650728940963745\n",
            "step 12680: loss = 1.5401417016983032\n",
            "step 12681: loss = 1.7017227411270142\n",
            "step 12682: loss = 1.5467994213104248\n",
            "step 12683: loss = 1.6396143436431885\n",
            "step 12684: loss = 1.5303025245666504\n",
            "step 12685: loss = 1.881596326828003\n",
            "step 12686: loss = 1.568141222000122\n",
            "step 12687: loss = 1.627946376800537\n",
            "step 12688: loss = 1.5455741882324219\n",
            "step 12689: loss = 1.66892409324646\n",
            "step 12690: loss = 1.6068737506866455\n",
            "step 12691: loss = 1.693042278289795\n",
            "step 12692: loss = 1.6183631420135498\n",
            "step 12693: loss = 1.9233143329620361\n",
            "step 12694: loss = 1.6689486503601074\n",
            "step 12695: loss = 2.0181541442871094\n",
            "step 12696: loss = 1.843155860900879\n",
            "step 12697: loss = 1.6702951192855835\n",
            "step 12698: loss = 1.5889132022857666\n",
            "step 12699: loss = 1.6452620029449463\n",
            "step 12700: loss = 1.8369344472885132\n",
            "step 12701: loss = 1.5061378479003906\n",
            "step 12702: loss = 1.7291243076324463\n",
            "step 12703: loss = 1.6650350093841553\n",
            "step 12704: loss = 1.7136460542678833\n",
            "step 12705: loss = 2.005082845687866\n",
            "step 12706: loss = 1.8699102401733398\n",
            "step 12707: loss = 1.5890299081802368\n",
            "step 12708: loss = 1.7794833183288574\n",
            "step 12709: loss = 1.5507006645202637\n",
            "step 12710: loss = 1.8069041967391968\n",
            "step 12711: loss = 1.73131263256073\n",
            "step 12712: loss = 1.6935819387435913\n",
            "step 12713: loss = 1.4500023126602173\n",
            "step 12714: loss = 1.7172861099243164\n",
            "step 12715: loss = 1.8704776763916016\n",
            "step 12716: loss = 1.6073176860809326\n",
            "step 12717: loss = 1.5725067853927612\n",
            "step 12718: loss = 1.7751706838607788\n",
            "step 12719: loss = 1.6221379041671753\n",
            "step 12720: loss = 1.835854411125183\n",
            "step 12721: loss = 1.7268143892288208\n",
            "step 12722: loss = 1.9214792251586914\n",
            "step 12723: loss = 1.5033819675445557\n",
            "step 12724: loss = 1.831200361251831\n",
            "step 12725: loss = 1.7657909393310547\n",
            "step 12726: loss = 1.6572500467300415\n",
            "step 12727: loss = 1.7923874855041504\n",
            "step 12728: loss = 1.6012150049209595\n",
            "step 12729: loss = 1.6787735223770142\n",
            "step 12730: loss = 1.8972123861312866\n",
            "step 12731: loss = 1.6526533365249634\n",
            "step 12732: loss = 1.757003664970398\n",
            "step 12733: loss = 1.7368017435073853\n",
            "step 12734: loss = 1.767581820487976\n",
            "step 12735: loss = 1.7654768228530884\n",
            "step 12736: loss = 1.780332088470459\n",
            "step 12737: loss = 1.7493432760238647\n",
            "step 12738: loss = 1.6540052890777588\n",
            "step 12739: loss = 1.667508602142334\n",
            "step 12740: loss = 1.6768027544021606\n",
            "step 12741: loss = 1.823287010192871\n",
            "step 12742: loss = 1.4605858325958252\n",
            "step 12743: loss = 1.7124203443527222\n",
            "step 12744: loss = 1.8114961385726929\n",
            "step 12745: loss = 1.9396286010742188\n",
            "step 12746: loss = 1.8066414594650269\n",
            "step 12747: loss = 1.6677666902542114\n",
            "step 12748: loss = 1.8380584716796875\n",
            "step 12749: loss = 1.8206613063812256\n",
            "step 12750: loss = 1.6212407350540161\n",
            "step 12751: loss = 1.9081742763519287\n",
            "step 12752: loss = 1.5906572341918945\n",
            "step 12753: loss = 1.615530252456665\n",
            "step 12754: loss = 1.799048900604248\n",
            "step 12755: loss = 1.726123332977295\n",
            "step 12756: loss = 2.0359044075012207\n",
            "step 12757: loss = 2.0510306358337402\n",
            "step 12758: loss = 1.67487370967865\n",
            "step 12759: loss = 1.5485289096832275\n",
            "step 12760: loss = 1.7753554582595825\n",
            "step 12761: loss = 1.6440905332565308\n",
            "step 12762: loss = 1.7327682971954346\n",
            "step 12763: loss = 1.710930347442627\n",
            "step 12764: loss = 1.640430212020874\n",
            "step 12765: loss = 1.6250261068344116\n",
            "step 12766: loss = 1.6163197755813599\n",
            "step 12767: loss = 1.8899521827697754\n",
            "step 12768: loss = 1.742828130722046\n",
            "step 12769: loss = 1.7784690856933594\n",
            "step 12770: loss = 1.797120451927185\n",
            "step 12771: loss = 1.6669590473175049\n",
            "step 12772: loss = 1.7410616874694824\n",
            "step 12773: loss = 1.7062586545944214\n",
            "step 12774: loss = 1.7902179956436157\n",
            "step 12775: loss = 1.5652778148651123\n",
            "step 12776: loss = 1.5972126722335815\n",
            "step 12777: loss = 1.8989497423171997\n",
            "step 12778: loss = 1.8269680738449097\n",
            "step 12779: loss = 1.765809178352356\n",
            "step 12780: loss = 1.6010974645614624\n",
            "step 12781: loss = 1.7862522602081299\n",
            "step 12782: loss = 1.7361936569213867\n",
            "step 12783: loss = 1.7929909229278564\n",
            "step 12784: loss = 1.8706376552581787\n",
            "step 12785: loss = 1.6703835725784302\n",
            "step 12786: loss = 1.7981911897659302\n",
            "step 12787: loss = 1.6337716579437256\n",
            "step 12788: loss = 1.7451032400131226\n",
            "step 12789: loss = 1.7583250999450684\n",
            "step 12790: loss = 1.57175612449646\n",
            "step 12791: loss = 1.9257405996322632\n",
            "step 12792: loss = 1.852325201034546\n",
            "step 12793: loss = 1.6038542985916138\n",
            "step 12794: loss = 1.7319730520248413\n",
            "step 12795: loss = 1.5348153114318848\n",
            "step 12796: loss = 1.6796449422836304\n",
            "step 12797: loss = 1.7755439281463623\n",
            "step 12798: loss = 1.6612255573272705\n",
            "step 12799: loss = 1.582137942314148\n",
            "step 12800: loss = 1.5664029121398926\n",
            "step 12801: loss = 1.4459686279296875\n",
            "step 12802: loss = 1.8284612894058228\n",
            "step 12803: loss = 1.465576410293579\n",
            "step 12804: loss = 1.585914969444275\n",
            "step 12805: loss = 1.6445049047470093\n",
            "step 12806: loss = 1.8627469539642334\n",
            "step 12807: loss = 1.5443035364151\n",
            "step 12808: loss = 1.7374204397201538\n",
            "step 12809: loss = 1.6276004314422607\n",
            "step 12810: loss = 1.772459864616394\n",
            "step 12811: loss = 1.7741308212280273\n",
            "step 12812: loss = 1.9835413694381714\n",
            "step 12813: loss = 1.9057680368423462\n",
            "step 12814: loss = 1.6315845251083374\n",
            "step 12815: loss = 1.6877747774124146\n",
            "step 12816: loss = 1.6123909950256348\n",
            "step 12817: loss = 1.7119237184524536\n",
            "step 12818: loss = 1.6820685863494873\n",
            "step 12819: loss = 2.122028350830078\n",
            "step 12820: loss = 1.7388232946395874\n",
            "step 12821: loss = 1.5153136253356934\n",
            "step 12822: loss = 1.7583109140396118\n",
            "step 12823: loss = 1.9891031980514526\n",
            "step 12824: loss = 1.5679908990859985\n",
            "step 12825: loss = 1.6433405876159668\n",
            "step 12826: loss = 1.8129557371139526\n",
            "step 12827: loss = 1.9371405839920044\n",
            "step 12828: loss = 1.8351860046386719\n",
            "step 12829: loss = 1.8361198902130127\n",
            "step 12830: loss = 1.7179826498031616\n",
            "step 12831: loss = 1.8923848867416382\n",
            "step 12832: loss = 1.6490195989608765\n",
            "step 12833: loss = 1.8272825479507446\n",
            "step 12834: loss = 1.8677400350570679\n",
            "step 12835: loss = 1.7726044654846191\n",
            "step 12836: loss = 1.771193504333496\n",
            "step 12837: loss = 1.9558515548706055\n",
            "step 12838: loss = 1.6599293947219849\n",
            "step 12839: loss = 1.6308183670043945\n",
            "step 12840: loss = 1.6422126293182373\n",
            "step 12841: loss = 1.620959997177124\n",
            "step 12842: loss = 1.89858078956604\n",
            "step 12843: loss = 1.9513473510742188\n",
            "step 12844: loss = 1.6127238273620605\n",
            "step 12845: loss = 1.9344885349273682\n",
            "step 12846: loss = 1.665874719619751\n",
            "step 12847: loss = 1.7604445219039917\n",
            "step 12848: loss = 1.6994162797927856\n",
            "step 12849: loss = 1.7793742418289185\n",
            "step 12850: loss = 1.470344066619873\n",
            "step 12851: loss = 1.8485965728759766\n",
            "step 12852: loss = 1.8079781532287598\n",
            "step 12853: loss = 1.885485291481018\n",
            "step 12854: loss = 1.7703324556350708\n",
            "step 12855: loss = 1.8153148889541626\n",
            "step 12856: loss = 1.6015024185180664\n",
            "step 12857: loss = 1.56345534324646\n",
            "step 12858: loss = 1.6622768640518188\n",
            "step 12859: loss = 1.5138980150222778\n",
            "step 12860: loss = 1.614622712135315\n",
            "step 12861: loss = 1.7368531227111816\n",
            "step 12862: loss = 1.692622423171997\n",
            "step 12863: loss = 1.6669927835464478\n",
            "step 12864: loss = 1.907458782196045\n",
            "step 12865: loss = 1.7268078327178955\n",
            "step 12866: loss = 1.839094638824463\n",
            "step 12867: loss = 1.6888296604156494\n",
            "step 12868: loss = 1.6154431104660034\n",
            "step 12869: loss = 1.7686980962753296\n",
            "step 12870: loss = 1.6394603252410889\n",
            "step 12871: loss = 1.7099218368530273\n",
            "step 12872: loss = 2.0247387886047363\n",
            "step 12873: loss = 1.7368366718292236\n",
            "step 12874: loss = 1.7965060472488403\n",
            "step 12875: loss = 1.784961223602295\n",
            "step 12876: loss = 1.5443341732025146\n",
            "step 12877: loss = 1.5904608964920044\n",
            "step 12878: loss = 1.967313289642334\n",
            "step 12879: loss = 1.8565726280212402\n",
            "step 12880: loss = 1.6670974493026733\n",
            "step 12881: loss = 1.81158447265625\n",
            "step 12882: loss = 1.5504704713821411\n",
            "step 12883: loss = 1.824282169342041\n",
            "step 12884: loss = 1.6701267957687378\n",
            "step 12885: loss = 1.7617275714874268\n",
            "step 12886: loss = 1.6623538732528687\n",
            "step 12887: loss = 1.5509988069534302\n",
            "step 12888: loss = 1.7208470106124878\n",
            "step 12889: loss = 1.8393687009811401\n",
            "step 12890: loss = 1.6337014436721802\n",
            "step 12891: loss = 1.6786460876464844\n",
            "step 12892: loss = 1.7606770992279053\n",
            "step 12893: loss = 1.8828741312026978\n",
            "step 12894: loss = 1.7466273307800293\n",
            "step 12895: loss = 1.7238596677780151\n",
            "step 12896: loss = 1.8399564027786255\n",
            "step 12897: loss = 1.735891342163086\n",
            "step 12898: loss = 1.5906668901443481\n",
            "step 12899: loss = 1.548104166984558\n",
            "step 12900: loss = 1.583256721496582\n",
            "step 12901: loss = 1.852571964263916\n",
            "step 12902: loss = 1.939815878868103\n",
            "step 12903: loss = 1.6993260383605957\n",
            "step 12904: loss = 1.8180726766586304\n",
            "step 12905: loss = 1.9206117391586304\n",
            "step 12906: loss = 1.9372913837432861\n",
            "step 12907: loss = 1.8425788879394531\n",
            "step 12908: loss = 1.9109352827072144\n",
            "step 12909: loss = 1.8525452613830566\n",
            "step 12910: loss = 1.8792074918746948\n",
            "step 12911: loss = 1.777051329612732\n",
            "step 12912: loss = 1.830980896949768\n",
            "step 12913: loss = 1.5883309841156006\n",
            "step 12914: loss = 1.818077802658081\n",
            "step 12915: loss = 1.6949397325515747\n",
            "step 12916: loss = 1.6866782903671265\n",
            "step 12917: loss = 1.7661805152893066\n",
            "step 12918: loss = 2.063755750656128\n",
            "step 12919: loss = 2.137559413909912\n",
            "step 12920: loss = 1.7401940822601318\n",
            "step 12921: loss = 1.9335097074508667\n",
            "step 12922: loss = 1.9137881994247437\n",
            "step 12923: loss = 1.8718541860580444\n",
            "step 12924: loss = 1.7300994396209717\n",
            "step 12925: loss = 1.829087734222412\n",
            "step 12926: loss = 1.7078864574432373\n",
            "step 12927: loss = 1.6163878440856934\n",
            "step 12928: loss = 1.8477094173431396\n",
            "step 12929: loss = 1.7817926406860352\n",
            "step 12930: loss = 1.5310027599334717\n",
            "step 12931: loss = 1.6777034997940063\n",
            "step 12932: loss = 1.7197189331054688\n",
            "step 12933: loss = 1.76960027217865\n",
            "step 12934: loss = 1.7924401760101318\n",
            "step 12935: loss = 2.064852476119995\n",
            "step 12936: loss = 1.6779563426971436\n",
            "step 12937: loss = 1.7323824167251587\n",
            "step 12938: loss = 1.7703204154968262\n",
            "step 12939: loss = 1.9876853227615356\n",
            "step 12940: loss = 1.7805691957473755\n",
            "step 12941: loss = 1.6963640451431274\n",
            "step 12942: loss = 1.8453902006149292\n",
            "step 12943: loss = 1.866839051246643\n",
            "step 12944: loss = 1.7730793952941895\n",
            "step 12945: loss = 1.7125351428985596\n",
            "step 12946: loss = 2.0312767028808594\n",
            "step 12947: loss = 1.7153366804122925\n",
            "step 12948: loss = 1.7816598415374756\n",
            "step 12949: loss = 1.7328157424926758\n",
            "step 12950: loss = 2.0206820964813232\n",
            "step 12951: loss = 1.989817500114441\n",
            "step 12952: loss = 1.9747743606567383\n",
            "step 12953: loss = 1.6251821517944336\n",
            "step 12954: loss = 1.9484946727752686\n",
            "step 12955: loss = 1.768123984336853\n",
            "step 12956: loss = 1.7129817008972168\n",
            "step 12957: loss = 1.6931545734405518\n",
            "step 12958: loss = 1.7226793766021729\n",
            "step 12959: loss = 1.740422010421753\n",
            "step 12960: loss = 1.6777353286743164\n",
            "step 12961: loss = 1.727796196937561\n",
            "step 12962: loss = 1.6972980499267578\n",
            "step 12963: loss = 1.7957298755645752\n",
            "step 12964: loss = 1.8617714643478394\n",
            "step 12965: loss = 1.790138840675354\n",
            "step 12966: loss = 1.7398617267608643\n",
            "step 12967: loss = 1.7102957963943481\n",
            "step 12968: loss = 1.7972970008850098\n",
            "step 12969: loss = 1.9557771682739258\n",
            "step 12970: loss = 1.7165123224258423\n",
            "step 12971: loss = 1.9268618822097778\n",
            "step 12972: loss = 1.8612340688705444\n",
            "step 12973: loss = 1.5359359979629517\n",
            "step 12974: loss = 1.9791197776794434\n",
            "step 12975: loss = 1.5314956903457642\n",
            "step 12976: loss = 1.6944044828414917\n",
            "step 12977: loss = 1.9150904417037964\n",
            "step 12978: loss = 1.8729217052459717\n",
            "step 12979: loss = 1.8627701997756958\n",
            "step 12980: loss = 1.689769983291626\n",
            "step 12981: loss = 1.925236701965332\n",
            "step 12982: loss = 1.654017686843872\n",
            "step 12983: loss = 1.7293657064437866\n",
            "step 12984: loss = 1.663169026374817\n",
            "step 12985: loss = 1.8438034057617188\n",
            "step 12986: loss = 1.8993741273880005\n",
            "step 12987: loss = 1.7299437522888184\n",
            "step 12988: loss = 1.8008235692977905\n",
            "step 12989: loss = 1.6896145343780518\n",
            "step 12990: loss = 1.5383574962615967\n",
            "step 12991: loss = 2.050156354904175\n",
            "step 12992: loss = 1.9153400659561157\n",
            "step 12993: loss = 1.7685728073120117\n",
            "step 12994: loss = 1.737370252609253\n",
            "step 12995: loss = 1.570342779159546\n",
            "step 12996: loss = 1.6427050828933716\n",
            "step 12997: loss = 1.666641354560852\n",
            "step 12998: loss = 1.7832293510437012\n",
            "step 12999: loss = 1.9350775480270386\n",
            "step 13000: loss = 1.729386329650879\n",
            "step 13001: loss = 1.9463227987289429\n",
            "step 13002: loss = 1.7834819555282593\n",
            "step 13003: loss = 1.764216423034668\n",
            "step 13004: loss = 1.6627000570297241\n",
            "step 13005: loss = 1.6752616167068481\n",
            "step 13006: loss = 1.7065629959106445\n",
            "step 13007: loss = 2.116847038269043\n",
            "step 13008: loss = 1.8024550676345825\n",
            "step 13009: loss = 1.7756596803665161\n",
            "step 13010: loss = 2.011772632598877\n",
            "step 13011: loss = 1.734439730644226\n",
            "step 13012: loss = 1.8358566761016846\n",
            "step 13013: loss = 1.7224897146224976\n",
            "step 13014: loss = 1.8070265054702759\n",
            "step 13015: loss = 1.9131766557693481\n",
            "step 13016: loss = 1.9103509187698364\n",
            "step 13017: loss = 1.599180817604065\n",
            "step 13018: loss = 1.9262398481369019\n",
            "step 13019: loss = 1.7032352685928345\n",
            "step 13020: loss = 1.5816690921783447\n",
            "step 13021: loss = 2.103545904159546\n",
            "step 13022: loss = 1.9590119123458862\n",
            "step 13023: loss = 1.919709324836731\n",
            "step 13024: loss = 1.9039220809936523\n",
            "step 13025: loss = 1.7529829740524292\n",
            "step 13026: loss = 1.8889251947402954\n",
            "step 13027: loss = 1.747046947479248\n",
            "step 13028: loss = 1.886281967163086\n",
            "step 13029: loss = 1.6022571325302124\n",
            "step 13030: loss = 1.7981910705566406\n",
            "step 13031: loss = 1.8823213577270508\n",
            "step 13032: loss = 1.777978777885437\n",
            "step 13033: loss = 1.8623758554458618\n",
            "step 13034: loss = 1.6267911195755005\n",
            "step 13035: loss = 1.8584990501403809\n",
            "step 13036: loss = 1.8580379486083984\n",
            "step 13037: loss = 1.8739694356918335\n",
            "step 13038: loss = 1.6201750040054321\n",
            "step 13039: loss = 1.8984215259552002\n",
            "step 13040: loss = 1.8320155143737793\n",
            "step 13041: loss = 1.9206719398498535\n",
            "step 13042: loss = 1.8738291263580322\n",
            "step 13043: loss = 1.6387275457382202\n",
            "step 13044: loss = 1.8506683111190796\n",
            "step 13045: loss = 1.7874599695205688\n",
            "step 13046: loss = 1.8712579011917114\n",
            "step 13047: loss = 1.9848185777664185\n",
            "step 13048: loss = 1.7794885635375977\n",
            "step 13049: loss = 1.7446527481079102\n",
            "step 13050: loss = 1.8063706159591675\n",
            "step 13051: loss = 1.7616325616836548\n",
            "step 13052: loss = 1.8139194250106812\n",
            "step 13053: loss = 1.8098407983779907\n",
            "step 13054: loss = 1.7733738422393799\n",
            "step 13055: loss = 2.021759271621704\n",
            "step 13056: loss = 1.6576154232025146\n",
            "step 13057: loss = 1.8643136024475098\n",
            "step 13058: loss = 1.8318759202957153\n",
            "step 13059: loss = 1.7475563287734985\n",
            "step 13060: loss = 1.5204650163650513\n",
            "step 13061: loss = 1.8345006704330444\n",
            "step 13062: loss = 1.7702566385269165\n",
            "step 13063: loss = 1.8631083965301514\n",
            "step 13064: loss = 1.8559948205947876\n",
            "step 13065: loss = 1.7210159301757812\n",
            "step 13066: loss = 1.6269737482070923\n",
            "step 13067: loss = 1.9202899932861328\n",
            "step 13068: loss = 1.8132964372634888\n",
            "step 13069: loss = 1.8667120933532715\n",
            "step 13070: loss = 1.79691743850708\n",
            "step 13071: loss = 1.8356956243515015\n",
            "step 13072: loss = 1.9289511442184448\n",
            "step 13073: loss = 2.0140016078948975\n",
            "step 13074: loss = 1.5828471183776855\n",
            "step 13075: loss = 1.764251947402954\n",
            "step 13076: loss = 1.76553475856781\n",
            "step 13077: loss = 1.7802338600158691\n",
            "step 13078: loss = 1.7066227197647095\n",
            "step 13079: loss = 1.7990423440933228\n",
            "step 13080: loss = 1.7661489248275757\n",
            "step 13081: loss = 1.806191086769104\n",
            "step 13082: loss = 1.855461835861206\n",
            "step 13083: loss = 1.9194468259811401\n",
            "step 13084: loss = 1.755057454109192\n",
            "step 13085: loss = 1.9377250671386719\n",
            "step 13086: loss = 1.9054434299468994\n",
            "step 13087: loss = 1.6732646226882935\n",
            "step 13088: loss = 1.9622656106948853\n",
            "step 13089: loss = 1.6801780462265015\n",
            "step 13090: loss = 1.6640692949295044\n",
            "step 13091: loss = 1.82792329788208\n",
            "step 13092: loss = 1.624735951423645\n",
            "step 13093: loss = 1.7267500162124634\n",
            "step 13094: loss = 1.6584948301315308\n",
            "step 13095: loss = 1.948931336402893\n",
            "step 13096: loss = 1.7948927879333496\n",
            "step 13097: loss = 1.9828351736068726\n",
            "step 13098: loss = 1.766347050666809\n",
            "step 13099: loss = 1.7555197477340698\n",
            "step 13100: loss = 1.8536840677261353\n",
            "step 13101: loss = 1.8219187259674072\n",
            "step 13102: loss = 2.0007128715515137\n",
            "step 13103: loss = 1.6474618911743164\n",
            "step 13104: loss = 1.6759953498840332\n",
            "step 13105: loss = 1.8973934650421143\n",
            "step 13106: loss = 1.7566765546798706\n",
            "step 13107: loss = 1.721483826637268\n",
            "step 13108: loss = 2.0251541137695312\n",
            "step 13109: loss = 1.9589321613311768\n",
            "step 13110: loss = 2.015554428100586\n",
            "step 13111: loss = 1.6653592586517334\n",
            "step 13112: loss = 1.5752238035202026\n",
            "step 13113: loss = 1.6504136323928833\n",
            "step 13114: loss = 1.8462716341018677\n",
            "step 13115: loss = 1.7556967735290527\n",
            "step 13116: loss = 1.8230013847351074\n",
            "step 13117: loss = 1.855992317199707\n",
            "step 13118: loss = 1.729221224784851\n",
            "step 13119: loss = 1.8582020998001099\n",
            "step 13120: loss = 1.6937335729599\n",
            "step 13121: loss = 1.7883546352386475\n",
            "step 13122: loss = 1.735132098197937\n",
            "step 13123: loss = 1.5061651468276978\n",
            "step 13124: loss = 1.813167929649353\n",
            "step 13125: loss = 1.826464056968689\n",
            "step 13126: loss = 1.7224912643432617\n",
            "step 13127: loss = 1.6824849843978882\n",
            "step 13128: loss = 1.8859152793884277\n",
            "step 13129: loss = 1.8426307439804077\n",
            "step 13130: loss = 1.8697240352630615\n",
            "step 13131: loss = 1.8682990074157715\n",
            "step 13132: loss = 1.7717913389205933\n",
            "step 13133: loss = 1.846567153930664\n",
            "step 13134: loss = 2.217548131942749\n",
            "step 13135: loss = 1.8781843185424805\n",
            "step 13136: loss = 1.8539825677871704\n",
            "step 13137: loss = 1.7052587270736694\n",
            "step 13138: loss = 1.9249110221862793\n",
            "step 13139: loss = 1.9717715978622437\n",
            "step 13140: loss = 1.73525071144104\n",
            "step 13141: loss = 1.9965343475341797\n",
            "step 13142: loss = 1.9519317150115967\n",
            "step 13143: loss = 1.6792231798171997\n",
            "step 13144: loss = 1.6199049949645996\n",
            "step 13145: loss = 1.7723592519760132\n",
            "step 13146: loss = 1.9599595069885254\n",
            "step 13147: loss = 2.0288197994232178\n",
            "step 13148: loss = 1.8060117959976196\n",
            "step 13149: loss = 1.797607421875\n",
            "step 13150: loss = 1.7078781127929688\n",
            "step 13151: loss = 1.8383461236953735\n",
            "step 13152: loss = 1.6077288389205933\n",
            "step 13153: loss = 1.8048993349075317\n",
            "step 13154: loss = 1.7793952226638794\n",
            "step 13155: loss = 1.8548129796981812\n",
            "step 13156: loss = 1.782509446144104\n",
            "step 13157: loss = 2.0630948543548584\n",
            "step 13158: loss = 1.7719266414642334\n",
            "step 13159: loss = 1.9542359113693237\n",
            "step 13160: loss = 1.7289211750030518\n",
            "step 13161: loss = 1.8543959856033325\n",
            "step 13162: loss = 1.676505446434021\n",
            "step 13163: loss = 2.006380558013916\n",
            "step 13164: loss = 1.8518544435501099\n",
            "step 13165: loss = 1.719618558883667\n",
            "step 13166: loss = 1.9246379137039185\n",
            "step 13167: loss = 1.7706338167190552\n",
            "step 13168: loss = 1.8563865423202515\n",
            "step 13169: loss = 1.8174153566360474\n",
            "step 13170: loss = 1.9318071603775024\n",
            "step 13171: loss = 1.6900042295455933\n",
            "step 13172: loss = 1.875274419784546\n",
            "step 13173: loss = 1.765575885772705\n",
            "step 13174: loss = 1.9979689121246338\n",
            "step 13175: loss = 1.7198097705841064\n",
            "step 13176: loss = 1.7782083749771118\n",
            "step 13177: loss = 1.8146095275878906\n",
            "step 13178: loss = 2.011763095855713\n",
            "step 13179: loss = 1.8824857473373413\n",
            "step 13180: loss = 1.6310803890228271\n",
            "step 13181: loss = 1.855730414390564\n",
            "step 13182: loss = 1.815019965171814\n",
            "step 13183: loss = 1.8807275295257568\n",
            "step 13184: loss = 1.9822065830230713\n",
            "step 13185: loss = 1.792037844657898\n",
            "step 13186: loss = 1.6477197408676147\n",
            "step 13187: loss = 1.9266685247421265\n",
            "step 13188: loss = 1.7926608324050903\n",
            "step 13189: loss = 1.8993521928787231\n",
            "step 13190: loss = 1.9430997371673584\n",
            "step 13191: loss = 1.7749351263046265\n",
            "step 13192: loss = 1.5121897459030151\n",
            "step 13193: loss = 1.9320822954177856\n",
            "step 13194: loss = 1.92440927028656\n",
            "step 13195: loss = 1.7456576824188232\n",
            "step 13196: loss = 1.8844562768936157\n",
            "step 13197: loss = 1.9895793199539185\n",
            "step 13198: loss = 1.89897882938385\n",
            "step 13199: loss = 1.694785714149475\n",
            "step 13200: loss = 1.8767731189727783\n",
            "step 13201: loss = 1.7367517948150635\n",
            "step 13202: loss = 1.8571195602416992\n",
            "step 13203: loss = 1.970979928970337\n",
            "step 13204: loss = 1.8501461744308472\n",
            "step 13205: loss = 1.796692132949829\n",
            "step 13206: loss = 1.69672429561615\n",
            "step 13207: loss = 1.847394347190857\n",
            "step 13208: loss = 1.831859827041626\n",
            "step 13209: loss = 1.9558796882629395\n",
            "step 13210: loss = 1.8604457378387451\n",
            "step 13211: loss = 1.7378686666488647\n",
            "step 13212: loss = 1.7017346620559692\n",
            "step 13213: loss = 1.7294349670410156\n",
            "step 13214: loss = 1.8833403587341309\n",
            "step 13215: loss = 1.6683917045593262\n",
            "step 13216: loss = 1.8776991367340088\n",
            "step 13217: loss = 1.9606002569198608\n",
            "step 13218: loss = 1.9784018993377686\n",
            "step 13219: loss = 1.929923176765442\n",
            "step 13220: loss = 1.7691599130630493\n",
            "step 13221: loss = 1.8874202966690063\n",
            "step 13222: loss = 1.8265889883041382\n",
            "step 13223: loss = 2.0505690574645996\n",
            "step 13224: loss = 1.821249008178711\n",
            "step 13225: loss = 1.730435848236084\n",
            "step 13226: loss = 1.8457870483398438\n",
            "step 13227: loss = 1.8003228902816772\n",
            "step 13228: loss = 2.054861068725586\n",
            "step 13229: loss = 1.8111026287078857\n",
            "step 13230: loss = 1.8630211353302002\n",
            "step 13231: loss = 1.7231415510177612\n",
            "step 13232: loss = 2.030005693435669\n",
            "step 13233: loss = 1.8117924928665161\n",
            "step 13234: loss = 1.6936455965042114\n",
            "step 13235: loss = 1.9215048551559448\n",
            "step 13236: loss = 1.648992657661438\n",
            "step 13237: loss = 1.7990695238113403\n",
            "step 13238: loss = 1.7803908586502075\n",
            "step 13239: loss = 1.7896003723144531\n",
            "step 13240: loss = 1.9739563465118408\n",
            "step 13241: loss = 1.904634952545166\n",
            "step 13242: loss = 1.6817409992218018\n",
            "step 13243: loss = 1.7696881294250488\n",
            "step 13244: loss = 1.8018876314163208\n",
            "step 13245: loss = 2.041382312774658\n",
            "step 13246: loss = 2.0020735263824463\n",
            "step 13247: loss = 1.7129884958267212\n",
            "step 13248: loss = 1.8623125553131104\n",
            "step 13249: loss = 1.567183494567871\n",
            "step 13250: loss = 1.933747410774231\n",
            "step 13251: loss = 1.663222074508667\n",
            "step 13252: loss = 1.899681568145752\n",
            "step 13253: loss = 1.949257254600525\n",
            "step 13254: loss = 1.9549006223678589\n",
            "step 13255: loss = 1.8087191581726074\n",
            "step 13256: loss = 1.7214772701263428\n",
            "step 13257: loss = 1.8751730918884277\n",
            "step 13258: loss = 2.030121088027954\n",
            "step 13259: loss = 2.0299177169799805\n",
            "step 13260: loss = 1.9232662916183472\n",
            "step 13261: loss = 1.7157344818115234\n",
            "step 13262: loss = 2.031186819076538\n",
            "step 13263: loss = 1.9438538551330566\n",
            "step 13264: loss = 1.6566838026046753\n",
            "step 13265: loss = 1.81257164478302\n",
            "step 13266: loss = 1.8866865634918213\n",
            "step 13267: loss = 1.6514825820922852\n",
            "step 13268: loss = 1.9804480075836182\n",
            "step 13269: loss = 2.12652325630188\n",
            "step 13270: loss = 1.7038427591323853\n",
            "step 13271: loss = 1.956722617149353\n",
            "step 13272: loss = 1.6789565086364746\n",
            "step 13273: loss = 1.4572019577026367\n",
            "step 13274: loss = 1.6551899909973145\n",
            "step 13275: loss = 1.7207331657409668\n",
            "step 13276: loss = 1.7061365842819214\n",
            "step 13277: loss = 1.8472925424575806\n",
            "step 13278: loss = 1.7789524793624878\n",
            "step 13279: loss = 1.5429884195327759\n",
            "step 13280: loss = 1.9521431922912598\n",
            "step 13281: loss = 1.8047206401824951\n",
            "step 13282: loss = 1.6280052661895752\n",
            "step 13283: loss = 1.9714956283569336\n",
            "step 13284: loss = 1.7733744382858276\n",
            "step 13285: loss = 2.139883518218994\n",
            "step 13286: loss = 1.8960397243499756\n",
            "step 13287: loss = 1.7125884294509888\n",
            "step 13288: loss = 1.9265578985214233\n",
            "step 13289: loss = 1.6424504518508911\n",
            "step 13290: loss = 1.7585160732269287\n",
            "step 13291: loss = 1.7142152786254883\n",
            "step 13292: loss = 1.7555005550384521\n",
            "step 13293: loss = 1.7641950845718384\n",
            "step 13294: loss = 1.9630054235458374\n",
            "step 13295: loss = 1.711262822151184\n",
            "step 13296: loss = 1.9136806726455688\n",
            "step 13297: loss = 1.7732341289520264\n",
            "step 13298: loss = 1.856289029121399\n",
            "step 13299: loss = 1.777016043663025\n",
            "step 13300: loss = 1.564102292060852\n",
            "step 13301: loss = 1.826655387878418\n",
            "step 13302: loss = 1.9760221242904663\n",
            "step 13303: loss = 1.7431429624557495\n",
            "step 13304: loss = 1.9318410158157349\n",
            "step 13305: loss = 1.8728760480880737\n",
            "step 13306: loss = 1.9222586154937744\n",
            "step 13307: loss = 1.7853124141693115\n",
            "step 13308: loss = 1.791324257850647\n",
            "step 13309: loss = 1.851739525794983\n",
            "step 13310: loss = 1.798351526260376\n",
            "step 13311: loss = 1.5904237031936646\n",
            "step 13312: loss = 1.7297850847244263\n",
            "step 13313: loss = 1.6388124227523804\n",
            "step 13314: loss = 1.646986722946167\n",
            "step 13315: loss = 1.930746078491211\n",
            "step 13316: loss = 1.7190847396850586\n",
            "step 13317: loss = 1.683969497680664\n",
            "step 13318: loss = 1.9067909717559814\n",
            "step 13319: loss = 1.7272330522537231\n",
            "step 13320: loss = 1.9295446872711182\n",
            "step 13321: loss = 1.492630124092102\n",
            "step 13322: loss = 1.665617823600769\n",
            "step 13323: loss = 1.932355523109436\n",
            "step 13324: loss = 1.7960196733474731\n",
            "step 13325: loss = 1.9129160642623901\n",
            "step 13326: loss = 1.9487488269805908\n",
            "step 13327: loss = 1.8830829858779907\n",
            "step 13328: loss = 1.7678114175796509\n",
            "step 13329: loss = 1.7692042589187622\n",
            "step 13330: loss = 1.7361682653427124\n",
            "step 13331: loss = 1.8307619094848633\n",
            "step 13332: loss = 1.6986294984817505\n",
            "step 13333: loss = 1.6981455087661743\n",
            "step 13334: loss = 1.9729028940200806\n",
            "step 13335: loss = 1.7685160636901855\n",
            "step 13336: loss = 1.943368911743164\n",
            "step 13337: loss = 1.9504653215408325\n",
            "step 13338: loss = 1.9629391431808472\n",
            "step 13339: loss = 1.8600647449493408\n",
            "step 13340: loss = 2.058799982070923\n",
            "step 13341: loss = 2.0398364067077637\n",
            "step 13342: loss = 1.715436577796936\n",
            "step 13343: loss = 1.8021541833877563\n",
            "step 13344: loss = 2.135504961013794\n",
            "step 13345: loss = 1.7783869504928589\n",
            "step 13346: loss = 1.8300223350524902\n",
            "step 13347: loss = 1.8228048086166382\n",
            "step 13348: loss = 1.89415442943573\n",
            "step 13349: loss = 1.9382096529006958\n",
            "step 13350: loss = 2.0428411960601807\n",
            "step 13351: loss = 1.9634714126586914\n",
            "step 13352: loss = 2.034470558166504\n",
            "step 13353: loss = 1.8396036624908447\n",
            "step 13354: loss = 1.7087074518203735\n",
            "step 13355: loss = 1.9021791219711304\n",
            "step 13356: loss = 1.9855173826217651\n",
            "step 13357: loss = 1.8493555784225464\n",
            "step 13358: loss = 2.243281126022339\n",
            "step 13359: loss = 2.097620964050293\n",
            "step 13360: loss = 1.8663445711135864\n",
            "step 13361: loss = 1.9907852411270142\n",
            "step 13362: loss = 1.5721125602722168\n",
            "step 13363: loss = 1.8291081190109253\n",
            "step 13364: loss = 1.7085161209106445\n",
            "step 13365: loss = 1.9044944047927856\n",
            "step 13366: loss = 1.9381903409957886\n",
            "step 13367: loss = 2.0927135944366455\n",
            "step 13368: loss = 1.9459894895553589\n",
            "step 13369: loss = 1.7133827209472656\n",
            "step 13370: loss = 1.7972315549850464\n",
            "step 13371: loss = 1.730425238609314\n",
            "step 13372: loss = 1.7809391021728516\n",
            "step 13373: loss = 1.7751197814941406\n",
            "step 13374: loss = 1.926007866859436\n",
            "step 13375: loss = 1.8069863319396973\n",
            "step 13376: loss = 1.7852394580841064\n",
            "step 13377: loss = 1.6321539878845215\n",
            "step 13378: loss = 2.072838068008423\n",
            "step 13379: loss = 1.9710556268692017\n",
            "step 13380: loss = 1.7893664836883545\n",
            "step 13381: loss = 1.935089349746704\n",
            "step 13382: loss = 1.818593144416809\n",
            "step 13383: loss = 1.873936653137207\n",
            "step 13384: loss = 1.712341070175171\n",
            "step 13385: loss = 1.7144057750701904\n",
            "step 13386: loss = 1.750784993171692\n",
            "step 13387: loss = 1.897452473640442\n",
            "step 13388: loss = 1.9069246053695679\n",
            "step 13389: loss = 1.9063307046890259\n",
            "step 13390: loss = 1.7459360361099243\n",
            "step 13391: loss = 1.781156063079834\n",
            "step 13392: loss = 1.9685590267181396\n",
            "step 13393: loss = 1.8577431440353394\n",
            "step 13394: loss = 1.8734171390533447\n",
            "step 13395: loss = 1.9999486207962036\n",
            "step 13396: loss = 1.7964048385620117\n",
            "step 13397: loss = 1.6028547286987305\n",
            "step 13398: loss = 1.921296238899231\n",
            "step 13399: loss = 1.7678773403167725\n",
            "step 13400: loss = 1.7977100610733032\n",
            "step 13401: loss = 1.8611143827438354\n",
            "step 13402: loss = 1.8152008056640625\n",
            "step 13403: loss = 1.9382879734039307\n",
            "step 13404: loss = 2.021160364151001\n",
            "step 13405: loss = 1.9048510789871216\n",
            "step 13406: loss = 2.0044655799865723\n",
            "step 13407: loss = 2.0484585762023926\n",
            "step 13408: loss = 1.8436452150344849\n",
            "step 13409: loss = 1.965684413909912\n",
            "step 13410: loss = 1.9133262634277344\n",
            "step 13411: loss = 1.8739943504333496\n",
            "step 13412: loss = 1.8093528747558594\n",
            "step 13413: loss = 1.6948187351226807\n",
            "step 13414: loss = 1.6726535558700562\n",
            "step 13415: loss = 1.9390764236450195\n",
            "step 13416: loss = 1.755926489830017\n",
            "step 13417: loss = 1.990159034729004\n",
            "step 13418: loss = 2.004049062728882\n",
            "step 13419: loss = 1.7947138547897339\n",
            "step 13420: loss = 1.6891194581985474\n",
            "step 13421: loss = 1.876043438911438\n",
            "step 13422: loss = 1.6788341999053955\n",
            "step 13423: loss = 1.853813648223877\n",
            "step 13424: loss = 1.9925212860107422\n",
            "step 13425: loss = 1.9851915836334229\n",
            "step 13426: loss = 1.724920392036438\n",
            "step 13427: loss = 1.772616982460022\n",
            "step 13428: loss = 1.6479188203811646\n",
            "step 13429: loss = 2.00458025932312\n",
            "step 13430: loss = 1.9953017234802246\n",
            "step 13431: loss = 1.7567425966262817\n",
            "step 13432: loss = 1.8784667253494263\n",
            "step 13433: loss = 1.7890537977218628\n",
            "step 13434: loss = 1.9781291484832764\n",
            "step 13435: loss = 1.747585654258728\n",
            "step 13436: loss = 2.0319809913635254\n",
            "step 13437: loss = 1.7375482320785522\n",
            "step 13438: loss = 1.8155169486999512\n",
            "step 13439: loss = 2.0676894187927246\n",
            "step 13440: loss = 2.1188273429870605\n",
            "step 13441: loss = 1.6828123331069946\n",
            "step 13442: loss = 2.1018571853637695\n",
            "step 13443: loss = 1.7537988424301147\n",
            "step 13444: loss = 1.9781256914138794\n",
            "step 13445: loss = 1.9847321510314941\n",
            "step 13446: loss = 1.9653065204620361\n",
            "step 13447: loss = 2.0579118728637695\n",
            "step 13448: loss = 1.7895214557647705\n",
            "step 13449: loss = 1.7976951599121094\n",
            "step 13450: loss = 1.7922351360321045\n",
            "step 13451: loss = 1.783515214920044\n",
            "step 13452: loss = 1.8312487602233887\n",
            "step 13453: loss = 1.9390543699264526\n",
            "step 13454: loss = 1.9382086992263794\n",
            "step 13455: loss = 1.9476158618927002\n",
            "step 13456: loss = 1.9765881299972534\n",
            "step 13457: loss = 1.7511208057403564\n",
            "step 13458: loss = 1.9733177423477173\n",
            "step 13459: loss = 1.8776935338974\n",
            "step 13460: loss = 1.9152308702468872\n",
            "step 13461: loss = 1.9966659545898438\n",
            "step 13462: loss = 1.8407926559448242\n",
            "step 13463: loss = 1.8968805074691772\n",
            "step 13464: loss = 1.9652715921401978\n",
            "step 13465: loss = 1.9916726350784302\n",
            "step 13466: loss = 1.7221519947052002\n",
            "step 13467: loss = 1.83027982711792\n",
            "step 13468: loss = 1.9120837450027466\n",
            "step 13469: loss = 2.0311264991760254\n",
            "step 13470: loss = 1.985633373260498\n",
            "step 13471: loss = 1.7120156288146973\n",
            "step 13472: loss = 2.0097475051879883\n",
            "step 13473: loss = 1.7553725242614746\n",
            "step 13474: loss = 1.73914635181427\n",
            "step 13475: loss = 1.702282190322876\n",
            "step 13476: loss = 1.764173150062561\n",
            "step 13477: loss = 1.8056273460388184\n",
            "step 13478: loss = 1.7777748107910156\n",
            "step 13479: loss = 1.8147563934326172\n",
            "step 13480: loss = 1.9753966331481934\n",
            "step 13481: loss = 2.003042221069336\n",
            "step 13482: loss = 1.8834481239318848\n",
            "step 13483: loss = 2.0094735622406006\n",
            "step 13484: loss = 1.809261441230774\n",
            "step 13485: loss = 1.7901504039764404\n",
            "step 13486: loss = 1.6842472553253174\n",
            "step 13487: loss = 1.8854740858078003\n",
            "step 13488: loss = 1.6603672504425049\n",
            "step 13489: loss = 1.910467505455017\n",
            "step 13490: loss = 2.075979471206665\n",
            "step 13491: loss = 2.1250994205474854\n",
            "step 13492: loss = 1.7936300039291382\n",
            "step 13493: loss = 1.7609143257141113\n",
            "step 13494: loss = 1.8867849111557007\n",
            "step 13495: loss = 1.5583720207214355\n",
            "step 13496: loss = 2.0386202335357666\n",
            "step 13497: loss = 2.046125650405884\n",
            "step 13498: loss = 1.913384199142456\n",
            "step 13499: loss = 1.757612943649292\n",
            "step 13500: loss = 1.8229434490203857\n",
            "step 13501: loss = 1.9346473217010498\n",
            "step 13502: loss = 1.8179457187652588\n",
            "step 13503: loss = 1.940746784210205\n",
            "step 13504: loss = 1.970896601676941\n",
            "step 13505: loss = 1.9752832651138306\n",
            "step 13506: loss = 1.8889281749725342\n",
            "step 13507: loss = 1.7815089225769043\n",
            "step 13508: loss = 1.8623424768447876\n",
            "step 13509: loss = 1.8363028764724731\n",
            "step 13510: loss = 1.711495280265808\n",
            "step 13511: loss = 1.6907492876052856\n",
            "step 13512: loss = 1.958172082901001\n",
            "step 13513: loss = 1.878104329109192\n",
            "step 13514: loss = 1.6778290271759033\n",
            "step 13515: loss = 2.1007637977600098\n",
            "step 13516: loss = 1.5427610874176025\n",
            "step 13517: loss = 1.7281548976898193\n",
            "step 13518: loss = 1.8761667013168335\n",
            "step 13519: loss = 2.1689136028289795\n",
            "step 13520: loss = 1.6774581670761108\n",
            "step 13521: loss = 1.9836387634277344\n",
            "step 13522: loss = 2.085642099380493\n",
            "step 13523: loss = 1.958369493484497\n",
            "step 13524: loss = 2.029507875442505\n",
            "step 13525: loss = 2.1484575271606445\n",
            "step 13526: loss = 1.796519160270691\n",
            "step 13527: loss = 1.9419422149658203\n",
            "step 13528: loss = 1.8469828367233276\n",
            "step 13529: loss = 1.9182689189910889\n",
            "step 13530: loss = 1.735642671585083\n",
            "step 13531: loss = 1.8950303792953491\n",
            "step 13532: loss = 1.8669883012771606\n",
            "step 13533: loss = 1.7663540840148926\n",
            "step 13534: loss = 1.8917455673217773\n",
            "step 13535: loss = 1.781394362449646\n",
            "step 13536: loss = 1.825568437576294\n",
            "step 13537: loss = 1.8557324409484863\n",
            "step 13538: loss = 1.7774440050125122\n",
            "step 13539: loss = 1.7818248271942139\n",
            "step 13540: loss = 1.8417896032333374\n",
            "step 13541: loss = 1.9247232675552368\n",
            "step 13542: loss = 2.099806070327759\n",
            "step 13543: loss = 1.830822229385376\n",
            "step 13544: loss = 1.881988286972046\n",
            "step 13545: loss = 2.1437575817108154\n",
            "step 13546: loss = 1.6820911169052124\n",
            "step 13547: loss = 1.9348655939102173\n",
            "step 13548: loss = 2.215265989303589\n",
            "step 13549: loss = 1.8841617107391357\n",
            "step 13550: loss = 1.8905588388442993\n",
            "step 13551: loss = 1.6760281324386597\n",
            "step 13552: loss = 2.250441551208496\n",
            "step 13553: loss = 1.778876543045044\n",
            "step 13554: loss = 1.8006502389907837\n",
            "step 13555: loss = 1.7315391302108765\n",
            "step 13556: loss = 1.9476408958435059\n",
            "step 13557: loss = 1.926102638244629\n",
            "step 13558: loss = 1.8521281480789185\n",
            "step 13559: loss = 1.9286950826644897\n",
            "step 13560: loss = 1.883805274963379\n",
            "step 13561: loss = 1.6694650650024414\n",
            "step 13562: loss = 1.6924676895141602\n",
            "step 13563: loss = 2.0541586875915527\n",
            "step 13564: loss = 1.8751379251480103\n",
            "step 13565: loss = 1.817313313484192\n",
            "step 13566: loss = 1.7764787673950195\n",
            "step 13567: loss = 1.9830992221832275\n",
            "step 13568: loss = 1.9653387069702148\n",
            "step 13569: loss = 1.9982901811599731\n",
            "step 13570: loss = 1.7885233163833618\n",
            "step 13571: loss = 1.9433265924453735\n",
            "step 13572: loss = 1.679347038269043\n",
            "step 13573: loss = 1.8387796878814697\n",
            "step 13574: loss = 1.9620048999786377\n",
            "step 13575: loss = 1.5554598569869995\n",
            "step 13576: loss = 1.7281721830368042\n",
            "step 13577: loss = 1.7463300228118896\n",
            "step 13578: loss = 1.9786165952682495\n",
            "step 13579: loss = 1.779329538345337\n",
            "step 13580: loss = 2.1066997051239014\n",
            "step 13581: loss = 2.0738320350646973\n",
            "step 13582: loss = 2.2276813983917236\n",
            "step 13583: loss = 1.8573263883590698\n",
            "step 13584: loss = 1.808449387550354\n",
            "step 13585: loss = 2.0666961669921875\n",
            "step 13586: loss = 2.097836494445801\n",
            "step 13587: loss = 1.722281575202942\n",
            "step 13588: loss = 1.8196052312850952\n",
            "step 13589: loss = 1.9079593420028687\n",
            "step 13590: loss = 2.0884838104248047\n",
            "step 13591: loss = 1.6738557815551758\n",
            "step 13592: loss = 1.9593827724456787\n",
            "step 13593: loss = 1.5751937627792358\n",
            "step 13594: loss = 1.7787636518478394\n",
            "step 13595: loss = 1.8830249309539795\n",
            "step 13596: loss = 1.7765835523605347\n",
            "step 13597: loss = 1.7591698169708252\n",
            "step 13598: loss = 1.9934182167053223\n",
            "step 13599: loss = 1.918118953704834\n",
            "step 13600: loss = 1.6867369413375854\n",
            "step 13601: loss = 1.992064118385315\n",
            "step 13602: loss = 1.7726387977600098\n",
            "step 13603: loss = 2.235124349594116\n",
            "step 13604: loss = 1.759720802307129\n",
            "step 13605: loss = 1.8822890520095825\n",
            "step 13606: loss = 1.7989904880523682\n",
            "step 13607: loss = 1.996409296989441\n",
            "step 13608: loss = 2.0059452056884766\n",
            "step 13609: loss = 2.0662293434143066\n",
            "step 13610: loss = 2.0504775047302246\n",
            "step 13611: loss = 1.8757641315460205\n",
            "step 13612: loss = 1.9448760747909546\n",
            "step 13613: loss = 1.7520782947540283\n",
            "step 13614: loss = 1.6923109292984009\n",
            "step 13615: loss = 1.7878146171569824\n",
            "step 13616: loss = 1.8842055797576904\n",
            "step 13617: loss = 2.058089017868042\n",
            "step 13618: loss = 1.7648401260375977\n",
            "step 13619: loss = 1.6484243869781494\n",
            "step 13620: loss = 1.8895773887634277\n",
            "step 13621: loss = 1.8436182737350464\n",
            "step 13622: loss = 1.8507437705993652\n",
            "step 13623: loss = 1.8390694856643677\n",
            "step 13624: loss = 1.9187334775924683\n",
            "step 13625: loss = 1.854565978050232\n",
            "step 13626: loss = 1.859659194946289\n",
            "step 13627: loss = 1.9638818502426147\n",
            "step 13628: loss = 1.8462578058242798\n",
            "step 13629: loss = 1.734893560409546\n",
            "step 13630: loss = 1.809582233428955\n",
            "step 13631: loss = 2.243257522583008\n",
            "step 13632: loss = 1.9536844491958618\n",
            "step 13633: loss = 1.7895148992538452\n",
            "step 13634: loss = 1.979621171951294\n",
            "step 13635: loss = 1.9308527708053589\n",
            "step 13636: loss = 1.8522135019302368\n",
            "step 13637: loss = 1.9681249856948853\n",
            "step 13638: loss = 1.8972324132919312\n",
            "step 13639: loss = 1.9608269929885864\n",
            "step 13640: loss = 1.734596848487854\n",
            "step 13641: loss = 1.963300108909607\n",
            "step 13642: loss = 1.945708990097046\n",
            "step 13643: loss = 1.7372483015060425\n",
            "step 13644: loss = 1.7317813634872437\n",
            "step 13645: loss = 1.8794828653335571\n",
            "step 13646: loss = 1.9742679595947266\n",
            "step 13647: loss = 2.1077682971954346\n",
            "step 13648: loss = 1.7930155992507935\n",
            "step 13649: loss = 1.8957653045654297\n",
            "step 13650: loss = 1.6847845315933228\n",
            "step 13651: loss = 1.7547885179519653\n",
            "step 13652: loss = 1.8455816507339478\n",
            "step 13653: loss = 1.9348528385162354\n",
            "step 13654: loss = 2.091557025909424\n",
            "step 13655: loss = 2.0776941776275635\n",
            "step 13656: loss = 1.7872999906539917\n",
            "step 13657: loss = 1.7003085613250732\n",
            "step 13658: loss = 1.8266992568969727\n",
            "step 13659: loss = 2.005115032196045\n",
            "step 13660: loss = 1.9174044132232666\n",
            "step 13661: loss = 2.0809075832366943\n",
            "step 13662: loss = 1.8518471717834473\n",
            "step 13663: loss = 1.9374617338180542\n",
            "step 13664: loss = 1.8605231046676636\n",
            "step 13665: loss = 2.0187675952911377\n",
            "step 13666: loss = 1.648146629333496\n",
            "step 13667: loss = 1.863655686378479\n",
            "step 13668: loss = 1.884490728378296\n",
            "step 13669: loss = 1.7525959014892578\n",
            "step 13670: loss = 1.9750685691833496\n",
            "step 13671: loss = 1.7886602878570557\n",
            "step 13672: loss = 1.7893997430801392\n",
            "step 13673: loss = 1.894192099571228\n",
            "step 13674: loss = 1.8396061658859253\n",
            "step 13675: loss = 1.8643229007720947\n",
            "step 13676: loss = 1.9226964712142944\n",
            "step 13677: loss = 1.6739633083343506\n",
            "step 13678: loss = 1.7164239883422852\n",
            "step 13679: loss = 2.183542251586914\n",
            "step 13680: loss = 2.0997347831726074\n",
            "step 13681: loss = 1.9340890645980835\n",
            "step 13682: loss = 1.8456594944000244\n",
            "step 13683: loss = 1.9696592092514038\n",
            "step 13684: loss = 1.723694086074829\n",
            "step 13685: loss = 1.8553922176361084\n",
            "step 13686: loss = 1.9552491903305054\n",
            "step 13687: loss = 1.9756948947906494\n",
            "step 13688: loss = 2.0171427726745605\n",
            "step 13689: loss = 1.809164047241211\n",
            "step 13690: loss = 1.8867428302764893\n",
            "step 13691: loss = 1.9351481199264526\n",
            "step 13692: loss = 1.8438774347305298\n",
            "step 13693: loss = 1.9213225841522217\n",
            "step 13694: loss = 1.9069113731384277\n",
            "step 13695: loss = 1.9688799381256104\n",
            "step 13696: loss = 1.9754743576049805\n",
            "step 13697: loss = 1.9436646699905396\n",
            "step 13698: loss = 1.7490687370300293\n",
            "step 13699: loss = 1.701045036315918\n",
            "step 13700: loss = 1.8473440408706665\n",
            "step 13701: loss = 1.8291921615600586\n",
            "step 13702: loss = 2.157074213027954\n",
            "step 13703: loss = 1.9830646514892578\n",
            "step 13704: loss = 1.8256655931472778\n",
            "step 13705: loss = 1.8224196434020996\n",
            "step 13706: loss = 1.915531039237976\n",
            "step 13707: loss = 1.9155809879302979\n",
            "step 13708: loss = 1.6796849966049194\n",
            "step 13709: loss = 1.8957070112228394\n",
            "step 13710: loss = 1.9571044445037842\n",
            "step 13711: loss = 2.0724985599517822\n",
            "step 13712: loss = 1.8530659675598145\n",
            "step 13713: loss = 1.8863242864608765\n",
            "step 13714: loss = 1.922461748123169\n",
            "step 13715: loss = 2.012369155883789\n",
            "step 13716: loss = 1.8527309894561768\n",
            "step 13717: loss = 1.8726836442947388\n",
            "step 13718: loss = 1.9383876323699951\n",
            "step 13719: loss = 1.977906346321106\n",
            "step 13720: loss = 1.9417119026184082\n",
            "step 13721: loss = 2.0893449783325195\n",
            "step 13722: loss = 1.81447172164917\n",
            "step 13723: loss = 1.9106839895248413\n",
            "step 13724: loss = 1.8411670923233032\n",
            "step 13725: loss = 1.878172516822815\n",
            "step 13726: loss = 2.045072555541992\n",
            "step 13727: loss = 1.5570392608642578\n",
            "step 13728: loss = 1.985598087310791\n",
            "step 13729: loss = 1.9206796884536743\n",
            "step 13730: loss = 1.8298448324203491\n",
            "step 13731: loss = 1.8607823848724365\n",
            "step 13732: loss = 1.9700709581375122\n",
            "step 13733: loss = 1.6457535028457642\n",
            "step 13734: loss = 1.9554589986801147\n",
            "step 13735: loss = 1.6827863454818726\n",
            "step 13736: loss = 2.1661863327026367\n",
            "step 13737: loss = 1.653820276260376\n",
            "step 13738: loss = 1.860411524772644\n",
            "step 13739: loss = 1.8968141078948975\n",
            "step 13740: loss = 2.1381359100341797\n",
            "step 13741: loss = 2.175628662109375\n",
            "step 13742: loss = 1.943947434425354\n",
            "step 13743: loss = 2.0109105110168457\n",
            "step 13744: loss = 1.8999873399734497\n",
            "step 13745: loss = 1.9811090230941772\n",
            "step 13746: loss = 1.865856647491455\n",
            "step 13747: loss = 1.7046126127243042\n",
            "step 13748: loss = 2.0581841468811035\n",
            "step 13749: loss = 1.9470844268798828\n",
            "step 13750: loss = 2.0870141983032227\n",
            "step 13751: loss = 2.0791406631469727\n",
            "step 13752: loss = 1.8675678968429565\n",
            "step 13753: loss = 1.7298178672790527\n",
            "step 13754: loss = 1.9999603033065796\n",
            "step 13755: loss = 1.9113852977752686\n",
            "step 13756: loss = 1.816861629486084\n",
            "step 13757: loss = 2.0083160400390625\n",
            "step 13758: loss = 1.8890552520751953\n",
            "step 13759: loss = 1.963301658630371\n",
            "step 13760: loss = 1.8862899541854858\n",
            "step 13761: loss = 1.9765598773956299\n",
            "step 13762: loss = 1.7558637857437134\n",
            "step 13763: loss = 1.9454973936080933\n",
            "step 13764: loss = 1.9616451263427734\n",
            "step 13765: loss = 1.963995337486267\n",
            "step 13766: loss = 1.7740947008132935\n",
            "step 13767: loss = 2.0181798934936523\n",
            "step 13768: loss = 2.024430751800537\n",
            "step 13769: loss = 1.9935611486434937\n",
            "step 13770: loss = 1.9973869323730469\n",
            "step 13771: loss = 1.7489993572235107\n",
            "step 13772: loss = 1.9866594076156616\n",
            "step 13773: loss = 1.963882327079773\n",
            "step 13774: loss = 2.132113456726074\n",
            "step 13775: loss = 1.6832783222198486\n",
            "step 13776: loss = 2.017666816711426\n",
            "step 13777: loss = 2.0038111209869385\n",
            "step 13778: loss = 1.8575057983398438\n",
            "step 13779: loss = 1.9555528163909912\n",
            "step 13780: loss = 1.800152063369751\n",
            "step 13781: loss = 1.8574166297912598\n",
            "step 13782: loss = 1.993104338645935\n",
            "step 13783: loss = 1.776447057723999\n",
            "step 13784: loss = 1.9598894119262695\n",
            "step 13785: loss = 1.8319889307022095\n",
            "step 13786: loss = 2.004443883895874\n",
            "step 13787: loss = 1.922242283821106\n",
            "step 13788: loss = 1.88668954372406\n",
            "step 13789: loss = 1.9577476978302002\n",
            "step 13790: loss = 1.8286113739013672\n",
            "step 13791: loss = 1.71742844581604\n",
            "step 13792: loss = 1.9090309143066406\n",
            "step 13793: loss = 1.7327053546905518\n",
            "step 13794: loss = 1.7209299802780151\n",
            "step 13795: loss = 1.7917739152908325\n",
            "step 13796: loss = 1.9277933835983276\n",
            "step 13797: loss = 1.798331618309021\n",
            "step 13798: loss = 2.0703814029693604\n",
            "step 13799: loss = 1.7062081098556519\n",
            "step 13800: loss = 1.8975422382354736\n",
            "step 13801: loss = 2.014387607574463\n",
            "step 13802: loss = 1.7421903610229492\n",
            "step 13803: loss = 1.869924783706665\n",
            "step 13804: loss = 1.991368055343628\n",
            "step 13805: loss = 2.141483783721924\n",
            "step 13806: loss = 2.049429178237915\n",
            "step 13807: loss = 1.8996347188949585\n",
            "step 13808: loss = 1.9456523656845093\n",
            "step 13809: loss = 1.9420439004898071\n",
            "step 13810: loss = 1.8579095602035522\n",
            "step 13811: loss = 2.0534255504608154\n",
            "step 13812: loss = 1.704831838607788\n",
            "step 13813: loss = 1.793480396270752\n",
            "step 13814: loss = 1.9045639038085938\n",
            "step 13815: loss = 1.8945860862731934\n",
            "step 13816: loss = 1.8541405200958252\n",
            "step 13817: loss = 1.8815767765045166\n",
            "step 13818: loss = 1.8071880340576172\n",
            "step 13819: loss = 1.9646745920181274\n",
            "step 13820: loss = 1.8912409543991089\n",
            "step 13821: loss = 2.027221441268921\n",
            "step 13822: loss = 1.776120901107788\n",
            "step 13823: loss = 1.8783092498779297\n",
            "step 13824: loss = 1.9020735025405884\n",
            "step 13825: loss = 1.7406952381134033\n",
            "step 13826: loss = 1.8607758283615112\n",
            "step 13827: loss = 2.059356927871704\n",
            "step 13828: loss = 2.0394394397735596\n",
            "step 13829: loss = 1.9843533039093018\n",
            "step 13830: loss = 1.9498640298843384\n",
            "step 13831: loss = 1.81527578830719\n",
            "step 13832: loss = 1.801843285560608\n",
            "step 13833: loss = 1.9962421655654907\n",
            "step 13834: loss = 1.7969017028808594\n",
            "step 13835: loss = 1.9969414472579956\n",
            "step 13836: loss = 2.143217086791992\n",
            "step 13837: loss = 1.9111738204956055\n",
            "step 13838: loss = 1.8653885126113892\n",
            "step 13839: loss = 2.0428855419158936\n",
            "step 13840: loss = 1.771010398864746\n",
            "step 13841: loss = 1.8685230016708374\n",
            "step 13842: loss = 1.9433844089508057\n",
            "step 13843: loss = 1.7203681468963623\n",
            "step 13844: loss = 2.078779697418213\n",
            "step 13845: loss = 1.9525668621063232\n",
            "step 13846: loss = 1.8911850452423096\n",
            "step 13847: loss = 2.0140585899353027\n",
            "step 13848: loss = 1.9531606435775757\n",
            "step 13849: loss = 2.075916290283203\n",
            "step 13850: loss = 2.127739429473877\n",
            "step 13851: loss = 2.03529691696167\n",
            "step 13852: loss = 1.841686487197876\n",
            "step 13853: loss = 2.2387850284576416\n",
            "step 13854: loss = 2.0683834552764893\n",
            "step 13855: loss = 1.9848426580429077\n",
            "step 13856: loss = 1.878485083580017\n",
            "step 13857: loss = 2.179926633834839\n",
            "step 13858: loss = 2.0180563926696777\n",
            "step 13859: loss = 1.829485535621643\n",
            "step 13860: loss = 1.8960589170455933\n",
            "step 13861: loss = 2.142014265060425\n",
            "step 13862: loss = 1.8747085332870483\n",
            "step 13863: loss = 1.9966816902160645\n",
            "step 13864: loss = 1.8244003057479858\n",
            "step 13865: loss = 1.7372469902038574\n",
            "step 13866: loss = 1.771477222442627\n",
            "step 13867: loss = 1.7292386293411255\n",
            "step 13868: loss = 2.0186843872070312\n",
            "step 13869: loss = 1.9663184881210327\n",
            "step 13870: loss = 2.0617356300354004\n",
            "step 13871: loss = 1.8436334133148193\n",
            "step 13872: loss = 2.0038692951202393\n",
            "step 13873: loss = 1.6488621234893799\n",
            "step 13874: loss = 2.000967264175415\n",
            "step 13875: loss = 2.25951886177063\n",
            "step 13876: loss = 1.8339698314666748\n",
            "step 13877: loss = 2.1341116428375244\n",
            "step 13878: loss = 1.839877963066101\n",
            "step 13879: loss = 1.6968897581100464\n",
            "step 13880: loss = 1.7552937269210815\n",
            "step 13881: loss = 2.179344892501831\n",
            "step 13882: loss = 1.8669604063034058\n",
            "step 13883: loss = 1.8763982057571411\n",
            "step 13884: loss = 1.7464094161987305\n",
            "step 13885: loss = 2.0051403045654297\n",
            "step 13886: loss = 1.8954607248306274\n",
            "step 13887: loss = 1.9269622564315796\n",
            "step 13888: loss = 1.8423796892166138\n",
            "step 13889: loss = 2.0975348949432373\n",
            "step 13890: loss = 1.8233973979949951\n",
            "step 13891: loss = 2.0058984756469727\n",
            "step 13892: loss = 1.8925637006759644\n",
            "step 13893: loss = 2.0367565155029297\n",
            "step 13894: loss = 1.924924373626709\n",
            "step 13895: loss = 1.825617790222168\n",
            "step 13896: loss = 1.6483551263809204\n",
            "step 13897: loss = 2.147010564804077\n",
            "step 13898: loss = 1.6626609563827515\n",
            "step 13899: loss = 2.002681016921997\n",
            "step 13900: loss = 1.8594493865966797\n",
            "step 13901: loss = 2.003333568572998\n",
            "step 13902: loss = 1.997624158859253\n",
            "step 13903: loss = 1.7104865312576294\n",
            "step 13904: loss = 1.7625013589859009\n",
            "step 13905: loss = 1.9881255626678467\n",
            "step 13906: loss = 1.9341280460357666\n",
            "step 13907: loss = 1.7470269203186035\n",
            "step 13908: loss = 2.0145223140716553\n",
            "step 13909: loss = 1.91562819480896\n",
            "step 13910: loss = 1.8144900798797607\n",
            "step 13911: loss = 1.8839443922042847\n",
            "step 13912: loss = 2.1013903617858887\n",
            "step 13913: loss = 1.8077858686447144\n",
            "step 13914: loss = 2.1335716247558594\n",
            "step 13915: loss = 1.867262601852417\n",
            "step 13916: loss = 1.7804204225540161\n",
            "step 13917: loss = 1.8994513750076294\n",
            "step 13918: loss = 1.9046430587768555\n",
            "step 13919: loss = 1.9218497276306152\n",
            "step 13920: loss = 1.8361742496490479\n",
            "step 13921: loss = 1.7218496799468994\n",
            "step 13922: loss = 1.9518853425979614\n",
            "step 13923: loss = 1.87130868434906\n",
            "step 13924: loss = 1.9459348917007446\n",
            "step 13925: loss = 1.8129198551177979\n",
            "step 13926: loss = 2.0011115074157715\n",
            "step 13927: loss = 1.937391996383667\n",
            "step 13928: loss = 1.807571530342102\n",
            "step 13929: loss = 1.9364138841629028\n",
            "step 13930: loss = 1.9746439456939697\n",
            "step 13931: loss = 1.8184727430343628\n",
            "step 13932: loss = 1.9382660388946533\n",
            "step 13933: loss = 1.8381907939910889\n",
            "step 13934: loss = 1.8598772287368774\n",
            "step 13935: loss = 1.9938863515853882\n",
            "step 13936: loss = 1.7793360948562622\n",
            "step 13937: loss = 2.0110790729522705\n",
            "step 13938: loss = 1.8946558237075806\n",
            "step 13939: loss = 1.882878303527832\n",
            "step 13940: loss = 1.8916534185409546\n",
            "step 13941: loss = 2.0116515159606934\n",
            "step 13942: loss = 2.1235506534576416\n",
            "step 13943: loss = 1.9452427625656128\n",
            "step 13944: loss = 2.010793924331665\n",
            "step 13945: loss = 2.087719440460205\n",
            "step 13946: loss = 1.9415998458862305\n",
            "step 13947: loss = 1.973891019821167\n",
            "step 13948: loss = 1.7648627758026123\n",
            "step 13949: loss = 1.8993632793426514\n",
            "step 13950: loss = 1.840073823928833\n",
            "step 13951: loss = 1.7056844234466553\n",
            "step 13952: loss = 1.9061685800552368\n",
            "step 13953: loss = 1.727128028869629\n",
            "step 13954: loss = 2.085334539413452\n",
            "step 13955: loss = 2.127573013305664\n",
            "step 13956: loss = 2.121426820755005\n",
            "step 13957: loss = 2.062713623046875\n",
            "step 13958: loss = 1.9845565557479858\n",
            "step 13959: loss = 1.684821605682373\n",
            "step 13960: loss = 2.1130733489990234\n",
            "step 13961: loss = 1.8560296297073364\n",
            "step 13962: loss = 2.1055002212524414\n",
            "step 13963: loss = 1.8609488010406494\n",
            "step 13964: loss = 1.9185895919799805\n",
            "step 13965: loss = 1.950521469116211\n",
            "step 13966: loss = 1.8512874841690063\n",
            "step 13967: loss = 2.0250256061553955\n",
            "step 13968: loss = 1.8624333143234253\n",
            "step 13969: loss = 2.103634834289551\n",
            "step 13970: loss = 1.7945913076400757\n",
            "step 13971: loss = 2.0225517749786377\n",
            "step 13972: loss = 1.804197072982788\n",
            "step 13973: loss = 1.7335032224655151\n",
            "step 13974: loss = 1.9114203453063965\n",
            "step 13975: loss = 1.857549786567688\n",
            "step 13976: loss = 1.9804272651672363\n",
            "step 13977: loss = 1.7133184671401978\n",
            "step 13978: loss = 2.0468976497650146\n",
            "step 13979: loss = 2.099048137664795\n",
            "step 13980: loss = 1.4641270637512207\n",
            "step 13981: loss = 1.9072160720825195\n",
            "step 13982: loss = 2.208620071411133\n",
            "step 13983: loss = 2.0442137718200684\n",
            "step 13984: loss = 1.994774341583252\n",
            "step 13985: loss = 1.7589597702026367\n",
            "step 13986: loss = 1.8679977655410767\n",
            "step 13987: loss = 1.8643609285354614\n",
            "step 13988: loss = 1.8873708248138428\n",
            "step 13989: loss = 1.8605406284332275\n",
            "step 13990: loss = 1.7472573518753052\n",
            "step 13991: loss = 1.914776086807251\n",
            "step 13992: loss = 2.0927839279174805\n",
            "step 13993: loss = 2.0050415992736816\n",
            "step 13994: loss = 2.0166773796081543\n",
            "step 13995: loss = 1.9730662107467651\n",
            "step 13996: loss = 1.8996134996414185\n",
            "step 13997: loss = 2.1188299655914307\n",
            "step 13998: loss = 1.9419448375701904\n",
            "step 13999: loss = 1.9284989833831787\n",
            "step 14000: loss = 1.8691045045852661\n",
            "step 14001: loss = 1.7169123888015747\n",
            "step 14002: loss = 2.024925470352173\n",
            "step 14003: loss = 1.9417771100997925\n",
            "step 14004: loss = 2.0066590309143066\n",
            "step 14005: loss = 1.9787116050720215\n",
            "step 14006: loss = 2.0733370780944824\n",
            "step 14007: loss = 1.8443777561187744\n",
            "step 14008: loss = 1.836338758468628\n",
            "step 14009: loss = 2.2837538719177246\n",
            "step 14010: loss = 2.024390697479248\n",
            "step 14011: loss = 2.2006897926330566\n",
            "step 14012: loss = 1.8236572742462158\n",
            "step 14013: loss = 1.92385995388031\n",
            "step 14014: loss = 2.073657274246216\n",
            "step 14015: loss = 1.9505916833877563\n",
            "step 14016: loss = 2.1311964988708496\n",
            "step 14017: loss = 1.7948675155639648\n",
            "step 14018: loss = 1.7188798189163208\n",
            "step 14019: loss = 1.9615978002548218\n",
            "step 14020: loss = 2.0379559993743896\n",
            "step 14021: loss = 2.095874786376953\n",
            "step 14022: loss = 1.9786063432693481\n",
            "step 14023: loss = 1.739343285560608\n",
            "step 14024: loss = 1.960659146308899\n",
            "step 14025: loss = 1.8635441064834595\n",
            "step 14026: loss = 2.12522292137146\n",
            "step 14027: loss = 1.9320220947265625\n",
            "step 14028: loss = 2.001304864883423\n",
            "step 14029: loss = 1.8610252141952515\n",
            "step 14030: loss = 1.9123345613479614\n",
            "step 14031: loss = 1.9155340194702148\n",
            "step 14032: loss = 1.9657506942749023\n",
            "step 14033: loss = 1.8933528661727905\n",
            "step 14034: loss = 2.0666072368621826\n",
            "step 14035: loss = 1.8904900550842285\n",
            "step 14036: loss = 1.8196632862091064\n",
            "step 14037: loss = 1.908376932144165\n",
            "step 14038: loss = 1.8931314945220947\n",
            "step 14039: loss = 1.9261866807937622\n",
            "step 14040: loss = 1.6729762554168701\n",
            "step 14041: loss = 1.8876070976257324\n",
            "step 14042: loss = 2.020580768585205\n",
            "step 14043: loss = 2.0066308975219727\n",
            "step 14044: loss = 1.8931692838668823\n",
            "step 14045: loss = 2.063683271408081\n",
            "step 14046: loss = 2.024195432662964\n",
            "step 14047: loss = 1.9643266201019287\n",
            "step 14048: loss = 1.6499764919281006\n",
            "step 14049: loss = 1.7297825813293457\n",
            "step 14050: loss = 1.5634618997573853\n",
            "step 14051: loss = 1.7833954095840454\n",
            "step 14052: loss = 1.6906812191009521\n",
            "step 14053: loss = 1.8263368606567383\n",
            "step 14054: loss = 1.8122174739837646\n",
            "step 14055: loss = 1.8840994834899902\n",
            "step 14056: loss = 1.7714143991470337\n",
            "step 14057: loss = 2.0395987033843994\n",
            "step 14058: loss = 1.8577178716659546\n",
            "Finish epoch 9\n",
            "New model saved, minimum loss: 1.8205992379506022 \n",
            "\n",
            "step 14059: loss = 1.5812764167785645\n",
            "step 14060: loss = 1.4020963907241821\n",
            "step 14061: loss = 1.4205996990203857\n",
            "step 14062: loss = 1.3633217811584473\n",
            "step 14063: loss = 1.6677045822143555\n",
            "step 14064: loss = 1.3940200805664062\n",
            "step 14065: loss = 1.5437328815460205\n",
            "step 14066: loss = 1.3590954542160034\n",
            "step 14067: loss = 1.5219531059265137\n",
            "step 14068: loss = 1.4365627765655518\n",
            "step 14069: loss = 1.6239333152770996\n",
            "step 14070: loss = 1.4313955307006836\n",
            "step 14071: loss = 1.5480707883834839\n",
            "step 14072: loss = 1.3120137453079224\n",
            "step 14073: loss = 1.5834667682647705\n",
            "step 14074: loss = 1.678568720817566\n",
            "step 14075: loss = 1.4700822830200195\n",
            "step 14076: loss = 1.743222951889038\n",
            "step 14077: loss = 1.4638609886169434\n",
            "step 14078: loss = 1.4977226257324219\n",
            "step 14079: loss = 1.5279927253723145\n",
            "step 14080: loss = 1.62594473361969\n",
            "step 14081: loss = 1.5300524234771729\n",
            "step 14082: loss = 1.5567365884780884\n",
            "step 14083: loss = 1.4314510822296143\n",
            "step 14084: loss = 1.4001469612121582\n",
            "step 14085: loss = 1.7277977466583252\n",
            "step 14086: loss = 1.5338937044143677\n",
            "step 14087: loss = 1.7385402917861938\n",
            "step 14088: loss = 1.4161661863327026\n",
            "step 14089: loss = 1.5483183860778809\n",
            "step 14090: loss = 1.6527906656265259\n",
            "step 14091: loss = 1.4811487197875977\n",
            "step 14092: loss = 1.4770230054855347\n",
            "step 14093: loss = 1.4382044076919556\n",
            "step 14094: loss = 1.4634532928466797\n",
            "step 14095: loss = 1.4904999732971191\n",
            "step 14096: loss = 1.4047613143920898\n",
            "step 14097: loss = 1.4585434198379517\n",
            "step 14098: loss = 1.2655576467514038\n",
            "step 14099: loss = 1.5057986974716187\n",
            "step 14100: loss = 1.4059449434280396\n",
            "step 14101: loss = 1.5899983644485474\n",
            "step 14102: loss = 1.6273231506347656\n",
            "step 14103: loss = 1.5521434545516968\n",
            "step 14104: loss = 1.5866445302963257\n",
            "step 14105: loss = 1.6730551719665527\n",
            "step 14106: loss = 1.4874062538146973\n",
            "step 14107: loss = 1.7506598234176636\n",
            "step 14108: loss = 1.5879697799682617\n",
            "step 14109: loss = 1.4920623302459717\n",
            "step 14110: loss = 1.4954828023910522\n",
            "step 14111: loss = 1.473824143409729\n",
            "step 14112: loss = 1.450657844543457\n",
            "step 14113: loss = 1.602311611175537\n",
            "step 14114: loss = 1.4642900228500366\n",
            "step 14115: loss = 1.5568139553070068\n",
            "step 14116: loss = 1.5897349119186401\n",
            "step 14117: loss = 1.6189442873001099\n",
            "step 14118: loss = 1.465560793876648\n",
            "step 14119: loss = 1.6533381938934326\n",
            "step 14120: loss = 1.4526420831680298\n",
            "step 14121: loss = 1.526134967803955\n",
            "step 14122: loss = 1.5862956047058105\n",
            "step 14123: loss = 1.6502068042755127\n",
            "step 14124: loss = 1.6954526901245117\n",
            "step 14125: loss = 1.45701003074646\n",
            "step 14126: loss = 1.4342296123504639\n",
            "step 14127: loss = 1.5240575075149536\n",
            "step 14128: loss = 1.4725749492645264\n",
            "step 14129: loss = 1.4297106266021729\n",
            "step 14130: loss = 1.5080779790878296\n",
            "step 14131: loss = 1.4768646955490112\n",
            "step 14132: loss = 1.5034641027450562\n",
            "step 14133: loss = 1.493829369544983\n",
            "step 14134: loss = 1.525972843170166\n",
            "step 14135: loss = 1.5625779628753662\n",
            "step 14136: loss = 1.2505452632904053\n",
            "step 14137: loss = 1.5004781484603882\n",
            "step 14138: loss = 1.5436631441116333\n",
            "step 14139: loss = 1.5154149532318115\n",
            "step 14140: loss = 1.5103511810302734\n",
            "step 14141: loss = 1.4201604127883911\n",
            "step 14142: loss = 1.4357643127441406\n",
            "step 14143: loss = 1.5037611722946167\n",
            "step 14144: loss = 1.634508490562439\n",
            "step 14145: loss = 1.3194581270217896\n",
            "step 14146: loss = 1.4672333002090454\n",
            "step 14147: loss = 1.4775753021240234\n",
            "step 14148: loss = 1.451711654663086\n",
            "step 14149: loss = 1.5385570526123047\n",
            "step 14150: loss = 1.6348389387130737\n",
            "step 14151: loss = 1.5796258449554443\n",
            "step 14152: loss = 1.6022673845291138\n",
            "step 14153: loss = 1.6298885345458984\n",
            "step 14154: loss = 1.4914460182189941\n",
            "step 14155: loss = 1.4550232887268066\n",
            "step 14156: loss = 1.4914942979812622\n",
            "step 14157: loss = 1.6239885091781616\n",
            "step 14158: loss = 1.7368583679199219\n",
            "step 14159: loss = 1.5807234048843384\n",
            "step 14160: loss = 1.514823317527771\n",
            "step 14161: loss = 1.5653352737426758\n",
            "step 14162: loss = 1.5373471975326538\n",
            "step 14163: loss = 1.432386875152588\n",
            "step 14164: loss = 1.4351004362106323\n",
            "step 14165: loss = 1.3784973621368408\n",
            "step 14166: loss = 1.400263786315918\n",
            "step 14167: loss = 1.5223150253295898\n",
            "step 14168: loss = 1.5363754034042358\n",
            "step 14169: loss = 1.5327039957046509\n",
            "step 14170: loss = 1.6386973857879639\n",
            "step 14171: loss = 1.4926190376281738\n",
            "step 14172: loss = 1.438084363937378\n",
            "step 14173: loss = 1.506502628326416\n",
            "step 14174: loss = 1.461164116859436\n",
            "step 14175: loss = 1.423056721687317\n",
            "step 14176: loss = 1.385723352432251\n",
            "step 14177: loss = 1.4375258684158325\n",
            "step 14178: loss = 1.5546109676361084\n",
            "step 14179: loss = 1.3050076961517334\n",
            "step 14180: loss = 1.6409342288970947\n",
            "step 14181: loss = 1.7578699588775635\n",
            "step 14182: loss = 1.463132381439209\n",
            "step 14183: loss = 1.6341735124588013\n",
            "step 14184: loss = 1.5272735357284546\n",
            "step 14185: loss = 1.4812147617340088\n",
            "step 14186: loss = 1.6091015338897705\n",
            "step 14187: loss = 1.486612319946289\n",
            "step 14188: loss = 1.7074238061904907\n",
            "step 14189: loss = 1.7487813234329224\n",
            "step 14190: loss = 1.5166544914245605\n",
            "step 14191: loss = 1.6130497455596924\n",
            "step 14192: loss = 1.8662141561508179\n",
            "step 14193: loss = 1.3816587924957275\n",
            "step 14194: loss = 1.7816424369812012\n",
            "step 14195: loss = 1.5821934938430786\n",
            "step 14196: loss = 1.4183074235916138\n",
            "step 14197: loss = 1.6398802995681763\n",
            "step 14198: loss = 1.583883285522461\n",
            "step 14199: loss = 1.5365837812423706\n",
            "step 14200: loss = 1.5711979866027832\n",
            "step 14201: loss = 1.4567084312438965\n",
            "step 14202: loss = 1.5657702684402466\n",
            "step 14203: loss = 1.5504106283187866\n",
            "step 14204: loss = 1.4596405029296875\n",
            "step 14205: loss = 1.871202826499939\n",
            "step 14206: loss = 1.5373600721359253\n",
            "step 14207: loss = 1.530288577079773\n",
            "step 14208: loss = 1.5905007123947144\n",
            "step 14209: loss = 1.6773669719696045\n",
            "step 14210: loss = 1.551795244216919\n",
            "step 14211: loss = 1.6118892431259155\n",
            "step 14212: loss = 1.4000695943832397\n",
            "step 14213: loss = 1.4218590259552002\n",
            "step 14214: loss = 1.5657918453216553\n",
            "step 14215: loss = 1.588529109954834\n",
            "step 14216: loss = 1.5382201671600342\n",
            "step 14217: loss = 1.678673267364502\n",
            "step 14218: loss = 1.6412585973739624\n",
            "step 14219: loss = 1.4467251300811768\n",
            "step 14220: loss = 1.400620460510254\n",
            "step 14221: loss = 1.5159580707550049\n",
            "step 14222: loss = 1.429241418838501\n",
            "step 14223: loss = 1.6118727922439575\n",
            "step 14224: loss = 1.6272023916244507\n",
            "step 14225: loss = 1.72244393825531\n",
            "step 14226: loss = 1.6148513555526733\n",
            "step 14227: loss = 1.6092956066131592\n",
            "step 14228: loss = 1.5956424474716187\n",
            "step 14229: loss = 1.596469521522522\n",
            "step 14230: loss = 1.6230307817459106\n",
            "step 14231: loss = 1.634064793586731\n",
            "step 14232: loss = 1.5789988040924072\n",
            "step 14233: loss = 1.4750726222991943\n",
            "step 14234: loss = 1.547979712486267\n",
            "step 14235: loss = 1.3647041320800781\n",
            "step 14236: loss = 1.7275437116622925\n",
            "step 14237: loss = 1.6623517274856567\n",
            "step 14238: loss = 1.3947101831436157\n",
            "step 14239: loss = 1.794089436531067\n",
            "step 14240: loss = 1.618648886680603\n",
            "step 14241: loss = 1.732189655303955\n",
            "step 14242: loss = 1.6204065084457397\n",
            "step 14243: loss = 1.6807609796524048\n",
            "step 14244: loss = 1.6084587574005127\n",
            "step 14245: loss = 1.7146456241607666\n",
            "step 14246: loss = 1.4736047983169556\n",
            "step 14247: loss = 1.4013065099716187\n",
            "step 14248: loss = 1.562718152999878\n",
            "step 14249: loss = 1.523223876953125\n",
            "step 14250: loss = 1.33842933177948\n",
            "step 14251: loss = 1.6878786087036133\n",
            "step 14252: loss = 1.620596170425415\n",
            "step 14253: loss = 1.6696925163269043\n",
            "step 14254: loss = 1.3698209524154663\n",
            "step 14255: loss = 1.503696084022522\n",
            "step 14256: loss = 1.519094467163086\n",
            "step 14257: loss = 1.6392453908920288\n",
            "step 14258: loss = 1.5660228729248047\n",
            "step 14259: loss = 1.4978439807891846\n",
            "step 14260: loss = 1.4779658317565918\n",
            "step 14261: loss = 1.6123870611190796\n",
            "step 14262: loss = 1.4202593564987183\n",
            "step 14263: loss = 1.4711803197860718\n",
            "step 14264: loss = 1.448875069618225\n",
            "step 14265: loss = 1.6357823610305786\n",
            "step 14266: loss = 1.6130542755126953\n",
            "step 14267: loss = 1.5854018926620483\n",
            "step 14268: loss = 1.6475672721862793\n",
            "step 14269: loss = 1.4406105279922485\n",
            "step 14270: loss = 1.79067063331604\n",
            "step 14271: loss = 1.6300170421600342\n",
            "step 14272: loss = 1.6309268474578857\n",
            "step 14273: loss = 1.5758968591690063\n",
            "step 14274: loss = 1.5073245763778687\n",
            "step 14275: loss = 1.6979795694351196\n",
            "step 14276: loss = 1.558157205581665\n",
            "step 14277: loss = 1.644237756729126\n",
            "step 14278: loss = 1.5990773439407349\n",
            "step 14279: loss = 1.459836483001709\n",
            "step 14280: loss = 1.7755374908447266\n",
            "step 14281: loss = 1.586936116218567\n",
            "step 14282: loss = 1.6170140504837036\n",
            "step 14283: loss = 1.6378486156463623\n",
            "step 14284: loss = 1.6080883741378784\n",
            "step 14285: loss = 1.435742974281311\n",
            "step 14286: loss = 1.6678533554077148\n",
            "step 14287: loss = 1.5482313632965088\n",
            "step 14288: loss = 1.484272837638855\n",
            "step 14289: loss = 1.374385118484497\n",
            "step 14290: loss = 1.678711175918579\n",
            "step 14291: loss = 1.572025179862976\n",
            "step 14292: loss = 1.440420389175415\n",
            "step 14293: loss = 1.699718952178955\n",
            "step 14294: loss = 1.4767574071884155\n",
            "step 14295: loss = 1.601543664932251\n",
            "step 14296: loss = 1.6645551919937134\n",
            "step 14297: loss = 1.6872730255126953\n",
            "step 14298: loss = 1.5934467315673828\n",
            "step 14299: loss = 1.5616525411605835\n",
            "step 14300: loss = 1.5714406967163086\n",
            "step 14301: loss = 1.4729231595993042\n",
            "step 14302: loss = 1.430310845375061\n",
            "step 14303: loss = 1.7805973291397095\n",
            "step 14304: loss = 1.2826664447784424\n",
            "step 14305: loss = 1.6782677173614502\n",
            "step 14306: loss = 1.962599277496338\n",
            "step 14307: loss = 1.4600787162780762\n",
            "step 14308: loss = 1.6596400737762451\n",
            "step 14309: loss = 1.7171707153320312\n",
            "step 14310: loss = 1.4768041372299194\n",
            "step 14311: loss = 1.7340189218521118\n",
            "step 14312: loss = 1.5681840181350708\n",
            "step 14313: loss = 1.4627450704574585\n",
            "step 14314: loss = 1.5782514810562134\n",
            "step 14315: loss = 1.669412612915039\n",
            "step 14316: loss = 1.5114681720733643\n",
            "step 14317: loss = 1.5164533853530884\n",
            "step 14318: loss = 1.6133257150650024\n",
            "step 14319: loss = 1.6794865131378174\n",
            "step 14320: loss = 1.8459383249282837\n",
            "step 14321: loss = 1.5900812149047852\n",
            "step 14322: loss = 1.7419596910476685\n",
            "step 14323: loss = 1.629667043685913\n",
            "step 14324: loss = 1.499558925628662\n",
            "step 14325: loss = 1.5803271532058716\n",
            "step 14326: loss = 1.6089426279067993\n",
            "step 14327: loss = 1.6898072957992554\n",
            "step 14328: loss = 1.512742280960083\n",
            "step 14329: loss = 1.781766414642334\n",
            "step 14330: loss = 1.7354568243026733\n",
            "step 14331: loss = 1.4677575826644897\n",
            "step 14332: loss = 1.6136459112167358\n",
            "step 14333: loss = 1.658162236213684\n",
            "step 14334: loss = 1.7997459173202515\n",
            "step 14335: loss = 1.6874210834503174\n",
            "step 14336: loss = 1.6392111778259277\n",
            "step 14337: loss = 1.5607212781906128\n",
            "step 14338: loss = 1.5969696044921875\n",
            "step 14339: loss = 1.6528518199920654\n",
            "step 14340: loss = 1.677010178565979\n",
            "step 14341: loss = 1.5635523796081543\n",
            "step 14342: loss = 1.548471212387085\n",
            "step 14343: loss = 1.6958848237991333\n",
            "step 14344: loss = 1.4651976823806763\n",
            "step 14345: loss = 1.533941626548767\n",
            "step 14346: loss = 1.6440424919128418\n",
            "step 14347: loss = 1.5531786680221558\n",
            "step 14348: loss = 1.7247973680496216\n",
            "step 14349: loss = 1.4123026132583618\n",
            "step 14350: loss = 1.5614700317382812\n",
            "step 14351: loss = 1.532227635383606\n",
            "step 14352: loss = 1.5284782648086548\n",
            "step 14353: loss = 1.5007649660110474\n",
            "step 14354: loss = 1.5396894216537476\n",
            "step 14355: loss = 1.5306400060653687\n",
            "step 14356: loss = 1.6781224012374878\n",
            "step 14357: loss = 1.5738670825958252\n",
            "step 14358: loss = 1.7055552005767822\n",
            "step 14359: loss = 1.5100587606430054\n",
            "step 14360: loss = 1.6371421813964844\n",
            "step 14361: loss = 1.5638539791107178\n",
            "step 14362: loss = 1.6184834241867065\n",
            "step 14363: loss = 1.3325154781341553\n",
            "step 14364: loss = 1.653984546661377\n",
            "step 14365: loss = 1.6279207468032837\n",
            "step 14366: loss = 1.4874238967895508\n",
            "step 14367: loss = 1.6965194940567017\n",
            "step 14368: loss = 1.6586884260177612\n",
            "step 14369: loss = 1.6379327774047852\n",
            "step 14370: loss = 1.6636768579483032\n",
            "step 14371: loss = 1.465311884880066\n",
            "step 14372: loss = 1.4193042516708374\n",
            "step 14373: loss = 1.509015440940857\n",
            "step 14374: loss = 1.682044506072998\n",
            "step 14375: loss = 1.8036935329437256\n",
            "step 14376: loss = 1.5809075832366943\n",
            "step 14377: loss = 1.5497628450393677\n",
            "step 14378: loss = 1.6451822519302368\n",
            "step 14379: loss = 1.5604733228683472\n",
            "step 14380: loss = 1.5685594081878662\n",
            "step 14381: loss = 1.617514967918396\n",
            "step 14382: loss = 1.7413599491119385\n",
            "step 14383: loss = 1.514951229095459\n",
            "step 14384: loss = 1.6648485660552979\n",
            "step 14385: loss = 1.6128748655319214\n",
            "step 14386: loss = 1.60455322265625\n",
            "step 14387: loss = 1.4050590991973877\n",
            "step 14388: loss = 1.6638033390045166\n",
            "step 14389: loss = 1.9615955352783203\n",
            "step 14390: loss = 1.6792553663253784\n",
            "step 14391: loss = 1.5051239728927612\n",
            "step 14392: loss = 1.35749089717865\n",
            "step 14393: loss = 1.6904551982879639\n",
            "step 14394: loss = 1.5236245393753052\n",
            "step 14395: loss = 1.5420974493026733\n",
            "step 14396: loss = 1.8474011421203613\n",
            "step 14397: loss = 1.6793771982192993\n",
            "step 14398: loss = 1.7318083047866821\n",
            "step 14399: loss = 1.5133709907531738\n",
            "step 14400: loss = 1.5649147033691406\n",
            "step 14401: loss = 1.5253337621688843\n",
            "step 14402: loss = 1.5331735610961914\n",
            "step 14403: loss = 1.7625280618667603\n",
            "step 14404: loss = 1.515114665031433\n",
            "step 14405: loss = 1.5207157135009766\n",
            "step 14406: loss = 1.7224434614181519\n",
            "step 14407: loss = 1.7003023624420166\n",
            "step 14408: loss = 1.714991807937622\n",
            "step 14409: loss = 1.543593406677246\n",
            "step 14410: loss = 1.5909812450408936\n",
            "step 14411: loss = 1.6461917161941528\n",
            "step 14412: loss = 1.7382994890213013\n",
            "step 14413: loss = 1.611615777015686\n",
            "step 14414: loss = 1.7300852537155151\n",
            "step 14415: loss = 1.509405493736267\n",
            "step 14416: loss = 1.6522706747055054\n",
            "step 14417: loss = 1.6928061246871948\n",
            "step 14418: loss = 1.548163890838623\n",
            "step 14419: loss = 1.5528652667999268\n",
            "step 14420: loss = 1.561854362487793\n",
            "step 14421: loss = 1.5604993104934692\n",
            "step 14422: loss = 1.833432674407959\n",
            "step 14423: loss = 1.6875895261764526\n",
            "step 14424: loss = 1.552210807800293\n",
            "step 14425: loss = 1.5986042022705078\n",
            "step 14426: loss = 1.6436331272125244\n",
            "step 14427: loss = 1.4597598314285278\n",
            "step 14428: loss = 1.6063575744628906\n",
            "step 14429: loss = 1.399192214012146\n",
            "step 14430: loss = 1.6499888896942139\n",
            "step 14431: loss = 1.5267847776412964\n",
            "step 14432: loss = 1.7596081495285034\n",
            "step 14433: loss = 1.8374799489974976\n",
            "step 14434: loss = 1.4208770990371704\n",
            "step 14435: loss = 1.7255496978759766\n",
            "step 14436: loss = 1.8390846252441406\n",
            "step 14437: loss = 1.551008701324463\n",
            "step 14438: loss = 1.4729729890823364\n",
            "step 14439: loss = 1.7701724767684937\n",
            "step 14440: loss = 1.5437604188919067\n",
            "step 14441: loss = 1.592830777168274\n",
            "step 14442: loss = 1.649827241897583\n",
            "step 14443: loss = 1.6534335613250732\n",
            "step 14444: loss = 1.5280746221542358\n",
            "step 14445: loss = 1.8919745683670044\n",
            "step 14446: loss = 1.4901680946350098\n",
            "step 14447: loss = 1.5818251371383667\n",
            "step 14448: loss = 1.7951327562332153\n",
            "step 14449: loss = 1.4631526470184326\n",
            "step 14450: loss = 1.7651249170303345\n",
            "step 14451: loss = 1.5596033334732056\n",
            "step 14452: loss = 1.529719352722168\n",
            "step 14453: loss = 1.4903960227966309\n",
            "step 14454: loss = 1.7125539779663086\n",
            "step 14455: loss = 1.614737868309021\n",
            "step 14456: loss = 1.700872778892517\n",
            "step 14457: loss = 1.6536884307861328\n",
            "step 14458: loss = 1.7179890871047974\n",
            "step 14459: loss = 1.7394378185272217\n",
            "step 14460: loss = 1.4612195491790771\n",
            "step 14461: loss = 1.503116250038147\n",
            "step 14462: loss = 1.843389868736267\n",
            "step 14463: loss = 1.9523823261260986\n",
            "step 14464: loss = 1.7269492149353027\n",
            "step 14465: loss = 1.617435097694397\n",
            "step 14466: loss = 1.5104758739471436\n",
            "step 14467: loss = 1.7567367553710938\n",
            "step 14468: loss = 1.5648994445800781\n",
            "step 14469: loss = 1.682966709136963\n",
            "step 14470: loss = 1.953762173652649\n",
            "step 14471: loss = 1.6184903383255005\n",
            "step 14472: loss = 1.7042081356048584\n",
            "step 14473: loss = 1.775342583656311\n",
            "step 14474: loss = 1.5592607259750366\n",
            "step 14475: loss = 1.4149134159088135\n",
            "step 14476: loss = 1.8034441471099854\n",
            "step 14477: loss = 1.8142023086547852\n",
            "step 14478: loss = 1.5698678493499756\n",
            "step 14479: loss = 1.4697754383087158\n",
            "step 14480: loss = 1.8369731903076172\n",
            "step 14481: loss = 1.563338041305542\n",
            "step 14482: loss = 1.5928109884262085\n",
            "step 14483: loss = 1.7575820684432983\n",
            "step 14484: loss = 1.5856002569198608\n",
            "step 14485: loss = 1.4504371881484985\n",
            "step 14486: loss = 1.4999961853027344\n",
            "step 14487: loss = 1.5118904113769531\n",
            "step 14488: loss = 1.463326334953308\n",
            "step 14489: loss = 1.6548740863800049\n",
            "step 14490: loss = 1.8405765295028687\n",
            "step 14491: loss = 1.6431528329849243\n",
            "step 14492: loss = 1.7649399042129517\n",
            "step 14493: loss = 1.6134408712387085\n",
            "step 14494: loss = 1.515703558921814\n",
            "step 14495: loss = 1.8272112607955933\n",
            "step 14496: loss = 1.8473693132400513\n",
            "step 14497: loss = 1.8119703531265259\n",
            "step 14498: loss = 1.661664605140686\n",
            "step 14499: loss = 1.640194058418274\n",
            "step 14500: loss = 1.754433274269104\n",
            "step 14501: loss = 1.4245387315750122\n",
            "step 14502: loss = 1.6097115278244019\n",
            "step 14503: loss = 1.6828352212905884\n",
            "step 14504: loss = 1.5232778787612915\n",
            "step 14505: loss = 1.6645963191986084\n",
            "step 14506: loss = 1.483216643333435\n",
            "step 14507: loss = 1.4173063039779663\n",
            "step 14508: loss = 1.6140081882476807\n",
            "step 14509: loss = 1.591038703918457\n",
            "step 14510: loss = 1.4500646591186523\n",
            "step 14511: loss = 1.70724356174469\n",
            "step 14512: loss = 1.73573899269104\n",
            "step 14513: loss = 1.703766942024231\n",
            "step 14514: loss = 1.6885184049606323\n",
            "step 14515: loss = 1.6278868913650513\n",
            "step 14516: loss = 1.5520086288452148\n",
            "step 14517: loss = 1.590585470199585\n",
            "step 14518: loss = 1.8095625638961792\n",
            "step 14519: loss = 1.6264727115631104\n",
            "step 14520: loss = 1.577148199081421\n",
            "step 14521: loss = 1.4694527387619019\n",
            "step 14522: loss = 1.7585514783859253\n",
            "step 14523: loss = 1.6132124662399292\n",
            "step 14524: loss = 1.5486479997634888\n",
            "step 14525: loss = 1.6289829015731812\n",
            "step 14526: loss = 1.56314218044281\n",
            "step 14527: loss = 1.7123521566390991\n",
            "step 14528: loss = 1.4811633825302124\n",
            "step 14529: loss = 1.5281702280044556\n",
            "step 14530: loss = 1.6116589307785034\n",
            "step 14531: loss = 1.5063979625701904\n",
            "step 14532: loss = 1.656294345855713\n",
            "step 14533: loss = 1.7916301488876343\n",
            "step 14534: loss = 1.519616723060608\n",
            "step 14535: loss = 1.4773635864257812\n",
            "step 14536: loss = 1.5899384021759033\n",
            "step 14537: loss = 1.487661600112915\n",
            "step 14538: loss = 1.6768606901168823\n",
            "step 14539: loss = 1.6162467002868652\n",
            "step 14540: loss = 1.5670937299728394\n",
            "step 14541: loss = 1.6657413244247437\n",
            "step 14542: loss = 1.490480899810791\n",
            "step 14543: loss = 1.8196667432785034\n",
            "step 14544: loss = 1.6465455293655396\n",
            "step 14545: loss = 1.6213672161102295\n",
            "step 14546: loss = 1.6299567222595215\n",
            "step 14547: loss = 1.63874351978302\n",
            "step 14548: loss = 1.674864649772644\n",
            "step 14549: loss = 1.6777219772338867\n",
            "step 14550: loss = 1.597957968711853\n",
            "step 14551: loss = 1.5889149904251099\n",
            "step 14552: loss = 1.5136983394622803\n",
            "step 14553: loss = 1.7948181629180908\n",
            "step 14554: loss = 1.727295994758606\n",
            "step 14555: loss = 1.8884103298187256\n",
            "step 14556: loss = 1.5181758403778076\n",
            "step 14557: loss = 1.6724833250045776\n",
            "step 14558: loss = 1.492484450340271\n",
            "step 14559: loss = 1.6365817785263062\n",
            "step 14560: loss = 1.7719295024871826\n",
            "step 14561: loss = 1.5158443450927734\n",
            "step 14562: loss = 1.5378334522247314\n",
            "step 14563: loss = 1.7198402881622314\n",
            "step 14564: loss = 1.7443480491638184\n",
            "step 14565: loss = 1.641688585281372\n",
            "step 14566: loss = 1.5298658609390259\n",
            "step 14567: loss = 1.6694526672363281\n",
            "step 14568: loss = 1.6345369815826416\n",
            "step 14569: loss = 1.7970932722091675\n",
            "step 14570: loss = 1.754058837890625\n",
            "step 14571: loss = 1.4140738248825073\n",
            "step 14572: loss = 1.6834862232208252\n",
            "step 14573: loss = 1.7213263511657715\n",
            "step 14574: loss = 1.6541417837142944\n",
            "step 14575: loss = 1.5201770067214966\n",
            "step 14576: loss = 1.7187913656234741\n",
            "step 14577: loss = 1.8539247512817383\n",
            "step 14578: loss = 1.5558942556381226\n",
            "step 14579: loss = 1.4823869466781616\n",
            "step 14580: loss = 1.6270592212677002\n",
            "step 14581: loss = 1.490881085395813\n",
            "step 14582: loss = 1.792494773864746\n",
            "step 14583: loss = 1.8541650772094727\n",
            "step 14584: loss = 1.6909562349319458\n",
            "step 14585: loss = 1.5717474222183228\n",
            "step 14586: loss = 1.6961276531219482\n",
            "step 14587: loss = 1.62918221950531\n",
            "step 14588: loss = 1.689534068107605\n",
            "step 14589: loss = 1.5071046352386475\n",
            "step 14590: loss = 1.7256946563720703\n",
            "step 14591: loss = 1.4992378950119019\n",
            "step 14592: loss = 1.6393665075302124\n",
            "step 14593: loss = 1.4408003091812134\n",
            "step 14594: loss = 1.8502857685089111\n",
            "step 14595: loss = 1.8078289031982422\n",
            "step 14596: loss = 1.7409262657165527\n",
            "step 14597: loss = 1.6076244115829468\n",
            "step 14598: loss = 1.8023662567138672\n",
            "step 14599: loss = 1.6283162832260132\n",
            "step 14600: loss = 1.6112176179885864\n",
            "step 14601: loss = 1.7440955638885498\n",
            "step 14602: loss = 1.9750065803527832\n",
            "step 14603: loss = 1.727840781211853\n",
            "step 14604: loss = 1.5267215967178345\n",
            "step 14605: loss = 1.5902624130249023\n",
            "step 14606: loss = 1.5311520099639893\n",
            "step 14607: loss = 1.5898979902267456\n",
            "step 14608: loss = 1.4807993173599243\n",
            "step 14609: loss = 1.5796195268630981\n",
            "step 14610: loss = 1.6506611108779907\n",
            "step 14611: loss = 1.6602115631103516\n",
            "step 14612: loss = 1.6823519468307495\n",
            "step 14613: loss = 1.9007080793380737\n",
            "step 14614: loss = 1.6508022546768188\n",
            "step 14615: loss = 1.6451786756515503\n",
            "step 14616: loss = 1.568174958229065\n",
            "step 14617: loss = 1.6637157201766968\n",
            "step 14618: loss = 1.629106879234314\n",
            "step 14619: loss = 1.7830407619476318\n",
            "step 14620: loss = 1.8309379816055298\n",
            "step 14621: loss = 1.6908940076828003\n",
            "step 14622: loss = 1.8644582033157349\n",
            "step 14623: loss = 1.8242411613464355\n",
            "step 14624: loss = 1.6521236896514893\n",
            "step 14625: loss = 1.739948034286499\n",
            "step 14626: loss = 1.6818541288375854\n",
            "step 14627: loss = 1.6649705171585083\n",
            "step 14628: loss = 1.923980474472046\n",
            "step 14629: loss = 1.6148627996444702\n",
            "step 14630: loss = 1.7081516981124878\n",
            "step 14631: loss = 1.6397755146026611\n",
            "step 14632: loss = 1.5042600631713867\n",
            "step 14633: loss = 1.721214771270752\n",
            "step 14634: loss = 1.6902923583984375\n",
            "step 14635: loss = 1.5703290700912476\n",
            "step 14636: loss = 1.5987157821655273\n",
            "step 14637: loss = 1.7656911611557007\n",
            "step 14638: loss = 1.5314433574676514\n",
            "step 14639: loss = 1.6745271682739258\n",
            "step 14640: loss = 1.590605616569519\n",
            "step 14641: loss = 1.4765311479568481\n",
            "step 14642: loss = 1.7427449226379395\n",
            "step 14643: loss = 1.4872615337371826\n",
            "step 14644: loss = 1.5296733379364014\n",
            "step 14645: loss = 1.776314616203308\n",
            "step 14646: loss = 1.6868865489959717\n",
            "step 14647: loss = 1.488783359527588\n",
            "step 14648: loss = 1.4644582271575928\n",
            "step 14649: loss = 1.6653286218643188\n",
            "step 14650: loss = 1.4559749364852905\n",
            "step 14651: loss = 1.6354304552078247\n",
            "step 14652: loss = 1.6867823600769043\n",
            "step 14653: loss = 1.6614048480987549\n",
            "step 14654: loss = 1.7146563529968262\n",
            "step 14655: loss = 1.8704798221588135\n",
            "step 14656: loss = 1.6576862335205078\n",
            "step 14657: loss = 1.7133125066757202\n",
            "step 14658: loss = 1.5597949028015137\n",
            "step 14659: loss = 1.638513207435608\n",
            "step 14660: loss = 1.4963496923446655\n",
            "step 14661: loss = 1.7536355257034302\n",
            "step 14662: loss = 1.658934473991394\n",
            "step 14663: loss = 1.741917371749878\n",
            "step 14664: loss = 1.7287616729736328\n",
            "step 14665: loss = 1.6395721435546875\n",
            "step 14666: loss = 1.5311238765716553\n",
            "step 14667: loss = 1.6356273889541626\n",
            "step 14668: loss = 1.4802535772323608\n",
            "step 14669: loss = 1.7157034873962402\n",
            "step 14670: loss = 1.6141459941864014\n",
            "step 14671: loss = 1.7479526996612549\n",
            "step 14672: loss = 1.731559157371521\n",
            "step 14673: loss = 1.434059739112854\n",
            "step 14674: loss = 1.6020526885986328\n",
            "step 14675: loss = 1.758589267730713\n",
            "step 14676: loss = 1.6584649085998535\n",
            "step 14677: loss = 1.5200968980789185\n",
            "step 14678: loss = 1.6667932271957397\n",
            "step 14679: loss = 1.7612899541854858\n",
            "step 14680: loss = 1.8491603136062622\n",
            "step 14681: loss = 1.4783016443252563\n",
            "step 14682: loss = 1.6585935354232788\n",
            "step 14683: loss = 1.6782456636428833\n",
            "step 14684: loss = 1.5398001670837402\n",
            "step 14685: loss = 1.7276837825775146\n",
            "step 14686: loss = 1.7734992504119873\n",
            "step 14687: loss = 1.7829660177230835\n",
            "step 14688: loss = 1.6304478645324707\n",
            "step 14689: loss = 1.8593504428863525\n",
            "step 14690: loss = 1.7264795303344727\n",
            "step 14691: loss = 1.6419929265975952\n",
            "step 14692: loss = 1.725270390510559\n",
            "step 14693: loss = 1.6276248693466187\n",
            "step 14694: loss = 1.8217827081680298\n",
            "step 14695: loss = 1.558640480041504\n",
            "step 14696: loss = 1.9353673458099365\n",
            "step 14697: loss = 1.5299152135849\n",
            "step 14698: loss = 1.7084051370620728\n",
            "step 14699: loss = 1.7016611099243164\n",
            "step 14700: loss = 1.9181859493255615\n",
            "step 14701: loss = 1.7896885871887207\n",
            "step 14702: loss = 1.815913438796997\n",
            "step 14703: loss = 1.5982815027236938\n",
            "step 14704: loss = 1.6501879692077637\n",
            "step 14705: loss = 1.8419837951660156\n",
            "step 14706: loss = 1.8701138496398926\n",
            "step 14707: loss = 1.7953295707702637\n",
            "step 14708: loss = 1.5814582109451294\n",
            "step 14709: loss = 1.7401913404464722\n",
            "step 14710: loss = 1.648311972618103\n",
            "step 14711: loss = 1.604205846786499\n",
            "step 14712: loss = 1.6971536874771118\n",
            "step 14713: loss = 1.456955909729004\n",
            "step 14714: loss = 1.6502455472946167\n",
            "step 14715: loss = 1.8179596662521362\n",
            "step 14716: loss = 1.7501232624053955\n",
            "step 14717: loss = 1.4973549842834473\n",
            "step 14718: loss = 1.4865792989730835\n",
            "step 14719: loss = 1.7592217922210693\n",
            "step 14720: loss = 1.5215644836425781\n",
            "step 14721: loss = 1.766859769821167\n",
            "step 14722: loss = 1.669195294380188\n",
            "step 14723: loss = 1.4922947883605957\n",
            "step 14724: loss = 1.6993821859359741\n",
            "step 14725: loss = 1.81564462184906\n",
            "step 14726: loss = 1.5626908540725708\n",
            "step 14727: loss = 1.618571400642395\n",
            "step 14728: loss = 1.7526838779449463\n",
            "step 14729: loss = 1.6211700439453125\n",
            "step 14730: loss = 1.7156840562820435\n",
            "step 14731: loss = 1.8529313802719116\n",
            "step 14732: loss = 1.5523219108581543\n",
            "step 14733: loss = 1.6683400869369507\n",
            "step 14734: loss = 1.7185529470443726\n",
            "step 14735: loss = 1.7386301755905151\n",
            "step 14736: loss = 1.6671123504638672\n",
            "step 14737: loss = 1.6337695121765137\n",
            "step 14738: loss = 1.6722263097763062\n",
            "step 14739: loss = 1.6651380062103271\n",
            "step 14740: loss = 1.8662136793136597\n",
            "step 14741: loss = 1.6730883121490479\n",
            "step 14742: loss = 1.7899854183197021\n",
            "step 14743: loss = 1.895994782447815\n",
            "step 14744: loss = 1.6416791677474976\n",
            "step 14745: loss = 1.8748397827148438\n",
            "step 14746: loss = 1.5998367071151733\n",
            "step 14747: loss = 1.7170442342758179\n",
            "step 14748: loss = 1.7163375616073608\n",
            "step 14749: loss = 1.6235237121582031\n",
            "step 14750: loss = 1.7488290071487427\n",
            "step 14751: loss = 1.7666395902633667\n",
            "step 14752: loss = 1.6686407327651978\n",
            "step 14753: loss = 1.7213276624679565\n",
            "step 14754: loss = 1.710256814956665\n",
            "step 14755: loss = 1.675616979598999\n",
            "step 14756: loss = 1.721854329109192\n",
            "step 14757: loss = 1.9682153463363647\n",
            "step 14758: loss = 1.8957765102386475\n",
            "step 14759: loss = 1.693762183189392\n",
            "step 14760: loss = 1.8307157754898071\n",
            "step 14761: loss = 1.8270570039749146\n",
            "step 14762: loss = 1.5907690525054932\n",
            "step 14763: loss = 1.6480799913406372\n",
            "step 14764: loss = 1.4718616008758545\n",
            "step 14765: loss = 1.713737964630127\n",
            "step 14766: loss = 1.7428147792816162\n",
            "step 14767: loss = 1.74699068069458\n",
            "step 14768: loss = 1.7263221740722656\n",
            "step 14769: loss = 1.5656766891479492\n",
            "step 14770: loss = 1.5645055770874023\n",
            "step 14771: loss = 1.7151590585708618\n",
            "step 14772: loss = 1.7551120519638062\n",
            "step 14773: loss = 1.6960211992263794\n",
            "step 14774: loss = 1.5082646608352661\n",
            "step 14775: loss = 1.8165230751037598\n",
            "step 14776: loss = 1.7004846334457397\n",
            "step 14777: loss = 1.7576184272766113\n",
            "step 14778: loss = 1.559762954711914\n",
            "step 14779: loss = 1.6492022275924683\n",
            "step 14780: loss = 1.4660718441009521\n",
            "step 14781: loss = 1.5846080780029297\n",
            "step 14782: loss = 1.6512584686279297\n",
            "step 14783: loss = 1.6937906742095947\n",
            "step 14784: loss = 1.870355248451233\n",
            "step 14785: loss = 1.610573172569275\n",
            "step 14786: loss = 1.8459243774414062\n",
            "step 14787: loss = 1.6325006484985352\n",
            "step 14788: loss = 1.629029631614685\n",
            "step 14789: loss = 1.808801531791687\n",
            "step 14790: loss = 1.83258855342865\n",
            "step 14791: loss = 1.713484764099121\n",
            "step 14792: loss = 1.5210548639297485\n",
            "step 14793: loss = 1.7649627923965454\n",
            "step 14794: loss = 1.7303924560546875\n",
            "step 14795: loss = 1.5960991382598877\n",
            "step 14796: loss = 1.7392171621322632\n",
            "step 14797: loss = 1.737161636352539\n",
            "step 14798: loss = 1.6959788799285889\n",
            "step 14799: loss = 1.7394846677780151\n",
            "step 14800: loss = 1.7904696464538574\n",
            "step 14801: loss = 1.6409848928451538\n",
            "step 14802: loss = 1.7031091451644897\n",
            "step 14803: loss = 1.6218608617782593\n",
            "step 14804: loss = 2.067631483078003\n",
            "step 14805: loss = 1.7908823490142822\n",
            "step 14806: loss = 1.912968635559082\n",
            "step 14807: loss = 1.4333521127700806\n",
            "step 14808: loss = 1.6321767568588257\n",
            "step 14809: loss = 1.5208314657211304\n",
            "step 14810: loss = 1.678480863571167\n",
            "step 14811: loss = 1.7189122438430786\n",
            "step 14812: loss = 1.672803282737732\n",
            "step 14813: loss = 1.7513612508773804\n",
            "step 14814: loss = 1.9880393743515015\n",
            "step 14815: loss = 1.7731605768203735\n",
            "step 14816: loss = 1.7130235433578491\n",
            "step 14817: loss = 1.7477144002914429\n",
            "step 14818: loss = 1.3339177370071411\n",
            "step 14819: loss = 1.656296968460083\n",
            "step 14820: loss = 1.7007042169570923\n",
            "step 14821: loss = 1.720307469367981\n",
            "step 14822: loss = 1.7978345155715942\n",
            "step 14823: loss = 1.6162152290344238\n",
            "step 14824: loss = 1.648051142692566\n",
            "step 14825: loss = 1.8064377307891846\n",
            "step 14826: loss = 1.4966604709625244\n",
            "step 14827: loss = 1.75906240940094\n",
            "step 14828: loss = 1.7077363729476929\n",
            "step 14829: loss = 1.613025426864624\n",
            "step 14830: loss = 1.5426255464553833\n",
            "step 14831: loss = 1.5334316492080688\n",
            "step 14832: loss = 1.6945246458053589\n",
            "step 14833: loss = 1.5652520656585693\n",
            "step 14834: loss = 1.7130967378616333\n",
            "step 14835: loss = 1.4936245679855347\n",
            "step 14836: loss = 1.6618740558624268\n",
            "step 14837: loss = 1.7916176319122314\n",
            "step 14838: loss = 1.6492195129394531\n",
            "step 14839: loss = 1.5730092525482178\n",
            "step 14840: loss = 1.6833723783493042\n",
            "step 14841: loss = 1.8516879081726074\n",
            "step 14842: loss = 1.722502589225769\n",
            "step 14843: loss = 1.5009033679962158\n",
            "step 14844: loss = 1.9777235984802246\n",
            "step 14845: loss = 1.5843217372894287\n",
            "step 14846: loss = 1.7651643753051758\n",
            "step 14847: loss = 1.498482346534729\n",
            "step 14848: loss = 1.5971767902374268\n",
            "step 14849: loss = 1.750203013420105\n",
            "step 14850: loss = 1.609585165977478\n",
            "step 14851: loss = 1.4711418151855469\n",
            "step 14852: loss = 1.7296522855758667\n",
            "step 14853: loss = 1.772044062614441\n",
            "step 14854: loss = 1.6317044496536255\n",
            "step 14855: loss = 1.679107666015625\n",
            "step 14856: loss = 1.6927738189697266\n",
            "step 14857: loss = 1.9928300380706787\n",
            "step 14858: loss = 1.6267430782318115\n",
            "step 14859: loss = 1.665859341621399\n",
            "step 14860: loss = 1.906665325164795\n",
            "step 14861: loss = 1.8852521181106567\n",
            "step 14862: loss = 1.7312681674957275\n",
            "step 14863: loss = 1.8125423192977905\n",
            "step 14864: loss = 1.7120922803878784\n",
            "step 14865: loss = 1.833057165145874\n",
            "step 14866: loss = 1.6571964025497437\n",
            "step 14867: loss = 1.6318402290344238\n",
            "step 14868: loss = 1.8703781366348267\n",
            "step 14869: loss = 1.7517409324645996\n",
            "step 14870: loss = 1.5807510614395142\n",
            "step 14871: loss = 1.6621878147125244\n",
            "step 14872: loss = 1.68772554397583\n",
            "step 14873: loss = 1.735156774520874\n",
            "step 14874: loss = 1.8114655017852783\n",
            "step 14875: loss = 1.6975971460342407\n",
            "step 14876: loss = 1.823706865310669\n",
            "step 14877: loss = 1.8242461681365967\n",
            "step 14878: loss = 1.6946665048599243\n",
            "step 14879: loss = 1.6807903051376343\n",
            "step 14880: loss = 1.818679928779602\n",
            "step 14881: loss = 1.6283220052719116\n",
            "step 14882: loss = 1.3668994903564453\n",
            "step 14883: loss = 1.7226009368896484\n",
            "step 14884: loss = 1.7370412349700928\n",
            "step 14885: loss = 1.6029913425445557\n",
            "step 14886: loss = 1.9675718545913696\n",
            "step 14887: loss = 1.7760672569274902\n",
            "step 14888: loss = 1.7288585901260376\n",
            "step 14889: loss = 1.907259225845337\n",
            "step 14890: loss = 1.8087780475616455\n",
            "step 14891: loss = 1.5568381547927856\n",
            "step 14892: loss = 1.6118263006210327\n",
            "step 14893: loss = 1.7125447988510132\n",
            "step 14894: loss = 1.6179808378219604\n",
            "step 14895: loss = 1.6870588064193726\n",
            "step 14896: loss = 1.7328593730926514\n",
            "step 14897: loss = 1.495607614517212\n",
            "step 14898: loss = 1.563096046447754\n",
            "step 14899: loss = 1.6684367656707764\n",
            "step 14900: loss = 1.7473820447921753\n",
            "step 14901: loss = 1.7270820140838623\n",
            "step 14902: loss = 1.6470013856887817\n",
            "step 14903: loss = 1.7392436265945435\n",
            "step 14904: loss = 1.6408441066741943\n",
            "step 14905: loss = 1.6615209579467773\n",
            "step 14906: loss = 1.7467498779296875\n",
            "step 14907: loss = 1.750976800918579\n",
            "step 14908: loss = 1.5880438089370728\n",
            "step 14909: loss = 1.618713617324829\n",
            "step 14910: loss = 1.9091731309890747\n",
            "step 14911: loss = 1.8321515321731567\n",
            "step 14912: loss = 1.7730815410614014\n",
            "step 14913: loss = 1.5841069221496582\n",
            "step 14914: loss = 1.8500779867172241\n",
            "step 14915: loss = 1.5226993560791016\n",
            "step 14916: loss = 1.7732665538787842\n",
            "step 14917: loss = 1.757186770439148\n",
            "step 14918: loss = 1.6489914655685425\n",
            "step 14919: loss = 1.5990893840789795\n",
            "step 14920: loss = 1.6896637678146362\n",
            "step 14921: loss = 1.643889307975769\n",
            "step 14922: loss = 1.7171045541763306\n",
            "step 14923: loss = 1.78399658203125\n",
            "step 14924: loss = 1.6478625535964966\n",
            "step 14925: loss = 1.6998449563980103\n",
            "step 14926: loss = 1.9470056295394897\n",
            "step 14927: loss = 1.880763053894043\n",
            "step 14928: loss = 1.9053785800933838\n",
            "step 14929: loss = 1.6339852809906006\n",
            "step 14930: loss = 1.7995113134384155\n",
            "step 14931: loss = 1.9165923595428467\n",
            "step 14932: loss = 1.785658597946167\n",
            "step 14933: loss = 1.7562108039855957\n",
            "step 14934: loss = 1.7173430919647217\n",
            "step 14935: loss = 1.4692436456680298\n",
            "step 14936: loss = 1.6885547637939453\n",
            "step 14937: loss = 1.9150193929672241\n",
            "step 14938: loss = 1.5676790475845337\n",
            "step 14939: loss = 1.6522005796432495\n",
            "step 14940: loss = 1.606948971748352\n",
            "step 14941: loss = 1.6600934267044067\n",
            "step 14942: loss = 1.6992462873458862\n",
            "step 14943: loss = 1.7544033527374268\n",
            "step 14944: loss = 1.6953856945037842\n",
            "step 14945: loss = 1.635492205619812\n",
            "step 14946: loss = 1.823639154434204\n",
            "step 14947: loss = 1.6366387605667114\n",
            "step 14948: loss = 1.7016539573669434\n",
            "step 14949: loss = 1.7378337383270264\n",
            "step 14950: loss = 1.744621753692627\n",
            "step 14951: loss = 1.4944559335708618\n",
            "step 14952: loss = 1.8376120328903198\n",
            "step 14953: loss = 1.741561770439148\n",
            "step 14954: loss = 1.7897746562957764\n",
            "step 14955: loss = 1.4846833944320679\n",
            "step 14956: loss = 1.8193048238754272\n",
            "step 14957: loss = 1.9545120000839233\n",
            "step 14958: loss = 1.728893756866455\n",
            "step 14959: loss = 1.5379080772399902\n",
            "step 14960: loss = 1.7940839529037476\n",
            "step 14961: loss = 1.8376107215881348\n",
            "step 14962: loss = 1.7437349557876587\n",
            "step 14963: loss = 1.8141124248504639\n",
            "step 14964: loss = 1.9224916696548462\n",
            "step 14965: loss = 1.6224210262298584\n",
            "step 14966: loss = 1.6930989027023315\n",
            "step 14967: loss = 1.6171376705169678\n",
            "step 14968: loss = 1.5762141942977905\n",
            "step 14969: loss = 1.465233564376831\n",
            "step 14970: loss = 1.5792433023452759\n",
            "step 14971: loss = 1.8079768419265747\n",
            "step 14972: loss = 1.7461156845092773\n",
            "step 14973: loss = 1.6200344562530518\n",
            "step 14974: loss = 1.839000940322876\n",
            "step 14975: loss = 1.8091340065002441\n",
            "step 14976: loss = 1.6278096437454224\n",
            "step 14977: loss = 1.605365514755249\n",
            "step 14978: loss = 1.6087825298309326\n",
            "step 14979: loss = 1.7819266319274902\n",
            "step 14980: loss = 1.723542332649231\n",
            "step 14981: loss = 1.8473397493362427\n",
            "step 14982: loss = 1.7382786273956299\n",
            "step 14983: loss = 1.67875075340271\n",
            "step 14984: loss = 1.8120003938674927\n",
            "step 14985: loss = 1.7881965637207031\n",
            "step 14986: loss = 2.00042724609375\n",
            "step 14987: loss = 1.7830132246017456\n",
            "step 14988: loss = 1.6094468832015991\n",
            "step 14989: loss = 1.7646540403366089\n",
            "step 14990: loss = 1.816239833831787\n",
            "step 14991: loss = 1.8869335651397705\n",
            "step 14992: loss = 1.6389816999435425\n",
            "step 14993: loss = 1.994598388671875\n",
            "step 14994: loss = 1.6861828565597534\n",
            "step 14995: loss = 1.8184871673583984\n",
            "step 14996: loss = 1.7067620754241943\n",
            "step 14997: loss = 1.6901301145553589\n",
            "step 14998: loss = 1.5369492769241333\n",
            "step 14999: loss = 1.6013410091400146\n",
            "step 15000: loss = 1.878161907196045\n",
            "step 15001: loss = 1.7471709251403809\n",
            "step 15002: loss = 1.8580116033554077\n",
            "step 15003: loss = 1.7330942153930664\n",
            "step 15004: loss = 1.623649001121521\n",
            "step 15005: loss = 1.9575294256210327\n",
            "step 15006: loss = 1.759677767753601\n",
            "step 15007: loss = 2.053365707397461\n",
            "step 15008: loss = 1.9795267581939697\n",
            "step 15009: loss = 1.5651296377182007\n",
            "step 15010: loss = 1.9375754594802856\n",
            "step 15011: loss = 1.708052635192871\n",
            "step 15012: loss = 1.935780644416809\n",
            "step 15013: loss = 1.5870274305343628\n",
            "step 15014: loss = 1.6457973718643188\n",
            "step 15015: loss = 1.76914381980896\n",
            "step 15016: loss = 1.6768486499786377\n",
            "step 15017: loss = 1.6728618144989014\n",
            "step 15018: loss = 1.743587613105774\n",
            "step 15019: loss = 1.6674011945724487\n",
            "step 15020: loss = 1.9781755208969116\n",
            "step 15021: loss = 1.7850160598754883\n",
            "step 15022: loss = 1.540556788444519\n",
            "step 15023: loss = 1.7601062059402466\n",
            "step 15024: loss = 1.837069034576416\n",
            "step 15025: loss = 1.7403030395507812\n",
            "step 15026: loss = 1.854276418685913\n",
            "step 15027: loss = 1.7918264865875244\n",
            "step 15028: loss = 1.7932244539260864\n",
            "step 15029: loss = 1.7734341621398926\n",
            "step 15030: loss = 1.7821804285049438\n",
            "step 15031: loss = 1.5905250310897827\n",
            "step 15032: loss = 1.5739140510559082\n",
            "step 15033: loss = 1.7755078077316284\n",
            "step 15034: loss = 1.6988112926483154\n",
            "step 15035: loss = 1.5840421915054321\n",
            "step 15036: loss = 1.7744754552841187\n",
            "step 15037: loss = 1.7055732011795044\n",
            "step 15038: loss = 1.6020288467407227\n",
            "step 15039: loss = 1.8747398853302002\n",
            "step 15040: loss = 1.7339369058609009\n",
            "step 15041: loss = 1.6260117292404175\n",
            "step 15042: loss = 1.7197719812393188\n",
            "step 15043: loss = 1.5633457899093628\n",
            "step 15044: loss = 1.7824006080627441\n",
            "step 15045: loss = 1.9758514165878296\n",
            "step 15046: loss = 1.8972892761230469\n",
            "step 15047: loss = 1.6812350749969482\n",
            "step 15048: loss = 1.8818053007125854\n",
            "step 15049: loss = 1.7097996473312378\n",
            "step 15050: loss = 1.9621864557266235\n",
            "step 15051: loss = 1.8297011852264404\n",
            "step 15052: loss = 1.6125072240829468\n",
            "step 15053: loss = 1.728681206703186\n",
            "step 15054: loss = 1.937565803527832\n",
            "step 15055: loss = 1.7550791501998901\n",
            "step 15056: loss = 1.9004814624786377\n",
            "step 15057: loss = 1.654811143875122\n",
            "step 15058: loss = 1.5699551105499268\n",
            "step 15059: loss = 1.8414932489395142\n",
            "step 15060: loss = 1.668241024017334\n",
            "step 15061: loss = 1.7523233890533447\n",
            "step 15062: loss = 1.6832940578460693\n",
            "step 15063: loss = 1.7965022325515747\n",
            "step 15064: loss = 1.949377417564392\n",
            "step 15065: loss = 1.8205432891845703\n",
            "step 15066: loss = 1.8308298587799072\n",
            "step 15067: loss = 1.6225504875183105\n",
            "step 15068: loss = 1.6648083925247192\n",
            "step 15069: loss = 1.7444204092025757\n",
            "step 15070: loss = 1.954723834991455\n",
            "step 15071: loss = 1.8059002161026\n",
            "step 15072: loss = 1.7378281354904175\n",
            "step 15073: loss = 1.850084900856018\n",
            "step 15074: loss = 1.7373590469360352\n",
            "step 15075: loss = 1.674691915512085\n",
            "step 15076: loss = 1.4200290441513062\n",
            "step 15077: loss = 1.6800062656402588\n",
            "step 15078: loss = 1.5826544761657715\n",
            "step 15079: loss = 1.744537353515625\n",
            "step 15080: loss = 1.9559204578399658\n",
            "step 15081: loss = 1.4985965490341187\n",
            "step 15082: loss = 1.8532462120056152\n",
            "step 15083: loss = 1.7321707010269165\n",
            "step 15084: loss = 1.6825190782546997\n",
            "step 15085: loss = 1.573354959487915\n",
            "step 15086: loss = 1.8260819911956787\n",
            "step 15087: loss = 1.8012826442718506\n",
            "step 15088: loss = 1.7382361888885498\n",
            "step 15089: loss = 1.8098362684249878\n",
            "step 15090: loss = 1.4210511445999146\n",
            "step 15091: loss = 1.7106767892837524\n",
            "step 15092: loss = 1.8344358205795288\n",
            "step 15093: loss = 1.7236756086349487\n",
            "step 15094: loss = 1.7158758640289307\n",
            "step 15095: loss = 1.6289730072021484\n",
            "step 15096: loss = 1.7004250288009644\n",
            "step 15097: loss = 1.4668844938278198\n",
            "step 15098: loss = 1.7420378923416138\n",
            "step 15099: loss = 1.6243129968643188\n",
            "step 15100: loss = 1.8386256694793701\n",
            "step 15101: loss = 1.9872833490371704\n",
            "step 15102: loss = 1.6994812488555908\n",
            "step 15103: loss = 1.7002006769180298\n",
            "step 15104: loss = 1.6630165576934814\n",
            "step 15105: loss = 1.6869646310806274\n",
            "step 15106: loss = 1.7800472974777222\n",
            "step 15107: loss = 1.8714842796325684\n",
            "step 15108: loss = 1.8333579301834106\n",
            "step 15109: loss = 1.5721161365509033\n",
            "step 15110: loss = 1.7415120601654053\n",
            "step 15111: loss = 1.8031772375106812\n",
            "step 15112: loss = 1.6848653554916382\n",
            "step 15113: loss = 1.8458139896392822\n",
            "step 15114: loss = 1.8094041347503662\n",
            "step 15115: loss = 1.7306127548217773\n",
            "step 15116: loss = 1.9677622318267822\n",
            "step 15117: loss = 2.050365924835205\n",
            "step 15118: loss = 1.6112431287765503\n",
            "step 15119: loss = 1.8470702171325684\n",
            "step 15120: loss = 1.5660499334335327\n",
            "step 15121: loss = 2.0367400646209717\n",
            "step 15122: loss = 1.6101672649383545\n",
            "step 15123: loss = 1.6607691049575806\n",
            "step 15124: loss = 1.8344405889511108\n",
            "step 15125: loss = 1.725988745689392\n",
            "step 15126: loss = 1.6960314512252808\n",
            "step 15127: loss = 1.659101128578186\n",
            "step 15128: loss = 1.9128018617630005\n",
            "step 15129: loss = 1.7742767333984375\n",
            "step 15130: loss = 1.7717926502227783\n",
            "step 15131: loss = 1.976180076599121\n",
            "step 15132: loss = 1.7820428609848022\n",
            "step 15133: loss = 1.8084170818328857\n",
            "step 15134: loss = 1.9009318351745605\n",
            "step 15135: loss = 1.7867355346679688\n",
            "step 15136: loss = 1.8353017568588257\n",
            "step 15137: loss = 1.7641003131866455\n",
            "step 15138: loss = 1.953757882118225\n",
            "step 15139: loss = 1.7638754844665527\n",
            "step 15140: loss = 1.7578052282333374\n",
            "step 15141: loss = 1.803614616394043\n",
            "step 15142: loss = 1.6482077836990356\n",
            "step 15143: loss = 1.5004596710205078\n",
            "step 15144: loss = 1.6996299028396606\n",
            "step 15145: loss = 1.7621583938598633\n",
            "step 15146: loss = 1.8833796977996826\n",
            "step 15147: loss = 1.8197059631347656\n",
            "step 15148: loss = 1.8445336818695068\n",
            "step 15149: loss = 1.4515597820281982\n",
            "step 15150: loss = 1.7780922651290894\n",
            "step 15151: loss = 1.7205629348754883\n",
            "step 15152: loss = 1.6868185997009277\n",
            "step 15153: loss = 1.8541887998580933\n",
            "step 15154: loss = 1.730510950088501\n",
            "step 15155: loss = 1.3590786457061768\n",
            "step 15156: loss = 1.7985731363296509\n",
            "step 15157: loss = 1.7766574621200562\n",
            "step 15158: loss = 1.7438329458236694\n",
            "step 15159: loss = 1.801755666732788\n",
            "step 15160: loss = 1.5294253826141357\n",
            "step 15161: loss = 1.8181124925613403\n",
            "step 15162: loss = 1.7790602445602417\n",
            "step 15163: loss = 1.9630264043807983\n",
            "step 15164: loss = 1.886276364326477\n",
            "step 15165: loss = 1.7820219993591309\n",
            "step 15166: loss = 1.8986523151397705\n",
            "step 15167: loss = 1.7698107957839966\n",
            "step 15168: loss = 1.8378981351852417\n",
            "step 15169: loss = 1.8715733289718628\n",
            "step 15170: loss = 1.7701444625854492\n",
            "step 15171: loss = 1.894607663154602\n",
            "step 15172: loss = 1.728087306022644\n",
            "step 15173: loss = 1.7409532070159912\n",
            "step 15174: loss = 1.7952462434768677\n",
            "step 15175: loss = 1.6191277503967285\n",
            "step 15176: loss = 1.7673100233078003\n",
            "step 15177: loss = 1.88687002658844\n",
            "step 15178: loss = 1.7870014905929565\n",
            "step 15179: loss = 1.6749588251113892\n",
            "step 15180: loss = 1.810157060623169\n",
            "step 15181: loss = 1.5739820003509521\n",
            "step 15182: loss = 1.5832839012145996\n",
            "step 15183: loss = 1.595476746559143\n",
            "step 15184: loss = 1.7193971872329712\n",
            "step 15185: loss = 1.7114672660827637\n",
            "step 15186: loss = 1.647106409072876\n",
            "step 15187: loss = 1.678533911705017\n",
            "step 15188: loss = 1.6396441459655762\n",
            "step 15189: loss = 1.8847392797470093\n",
            "step 15190: loss = 1.8759708404541016\n",
            "step 15191: loss = 1.8830374479293823\n",
            "step 15192: loss = 1.5910613536834717\n",
            "step 15193: loss = 1.7812589406967163\n",
            "step 15194: loss = 1.7661184072494507\n",
            "step 15195: loss = 1.6487420797348022\n",
            "step 15196: loss = 1.8227670192718506\n",
            "step 15197: loss = 1.9126462936401367\n",
            "step 15198: loss = 1.8000928163528442\n",
            "step 15199: loss = 2.0799357891082764\n",
            "step 15200: loss = 1.8935973644256592\n",
            "step 15201: loss = 1.7587544918060303\n",
            "step 15202: loss = 1.7335022687911987\n",
            "step 15203: loss = 1.7419530153274536\n",
            "step 15204: loss = 1.8275184631347656\n",
            "step 15205: loss = 1.9361635446548462\n",
            "step 15206: loss = 1.7949166297912598\n",
            "step 15207: loss = 1.5566269159317017\n",
            "step 15208: loss = 1.57440984249115\n",
            "step 15209: loss = 1.9288182258605957\n",
            "step 15210: loss = 1.8752281665802002\n",
            "step 15211: loss = 1.979218602180481\n",
            "step 15212: loss = 1.9125490188598633\n",
            "step 15213: loss = 1.9011178016662598\n",
            "step 15214: loss = 1.7983973026275635\n",
            "step 15215: loss = 1.9360138177871704\n",
            "step 15216: loss = 2.0391926765441895\n",
            "step 15217: loss = 1.7420276403427124\n",
            "step 15218: loss = 1.741714596748352\n",
            "step 15219: loss = 1.700386881828308\n",
            "step 15220: loss = 1.734110951423645\n",
            "step 15221: loss = 1.7612942457199097\n",
            "step 15222: loss = 1.7885832786560059\n",
            "step 15223: loss = 1.6843302249908447\n",
            "step 15224: loss = 1.7201790809631348\n",
            "step 15225: loss = 1.8975887298583984\n",
            "step 15226: loss = 1.6459258794784546\n",
            "step 15227: loss = 1.6293015480041504\n",
            "step 15228: loss = 1.757320523262024\n",
            "step 15229: loss = 1.5138053894042969\n",
            "step 15230: loss = 1.6961450576782227\n",
            "step 15231: loss = 1.7308193445205688\n",
            "step 15232: loss = 1.7136332988739014\n",
            "step 15233: loss = 1.690953254699707\n",
            "step 15234: loss = 1.784409999847412\n",
            "step 15235: loss = 1.9691945314407349\n",
            "step 15236: loss = 1.7444208860397339\n",
            "step 15237: loss = 1.9057762622833252\n",
            "step 15238: loss = 1.6713637113571167\n",
            "step 15239: loss = 1.610917568206787\n",
            "step 15240: loss = 1.9541569948196411\n",
            "step 15241: loss = 1.6622689962387085\n",
            "step 15242: loss = 1.8387607336044312\n",
            "step 15243: loss = 1.6051609516143799\n",
            "step 15244: loss = 1.9806790351867676\n",
            "step 15245: loss = 1.7588249444961548\n",
            "step 15246: loss = 1.6094523668289185\n",
            "step 15247: loss = 1.6265913248062134\n",
            "step 15248: loss = 1.674187421798706\n",
            "step 15249: loss = 1.7195795774459839\n",
            "step 15250: loss = 1.7974812984466553\n",
            "step 15251: loss = 1.482862114906311\n",
            "step 15252: loss = 1.5836374759674072\n",
            "step 15253: loss = 1.9819831848144531\n",
            "step 15254: loss = 1.5514500141143799\n",
            "step 15255: loss = 1.8985146284103394\n",
            "step 15256: loss = 1.7543516159057617\n",
            "step 15257: loss = 2.013739824295044\n",
            "step 15258: loss = 1.9018874168395996\n",
            "step 15259: loss = 1.4985767602920532\n",
            "step 15260: loss = 1.812151551246643\n",
            "step 15261: loss = 1.7564693689346313\n",
            "step 15262: loss = 1.6801090240478516\n",
            "step 15263: loss = 1.7808055877685547\n",
            "step 15264: loss = 1.6370354890823364\n",
            "step 15265: loss = 1.7592172622680664\n",
            "step 15266: loss = 1.842529296875\n",
            "step 15267: loss = 1.8364434242248535\n",
            "step 15268: loss = 1.8530302047729492\n",
            "step 15269: loss = 1.6355938911437988\n",
            "step 15270: loss = 1.7502164840698242\n",
            "step 15271: loss = 1.7190033197402954\n",
            "step 15272: loss = 1.8921948671340942\n",
            "step 15273: loss = 1.7547345161437988\n",
            "step 15274: loss = 1.7390100955963135\n",
            "step 15275: loss = 1.7874300479888916\n",
            "step 15276: loss = 1.7681304216384888\n",
            "step 15277: loss = 1.6886892318725586\n",
            "step 15278: loss = 1.8698076009750366\n",
            "step 15279: loss = 2.001401901245117\n",
            "step 15280: loss = 1.6528432369232178\n",
            "step 15281: loss = 1.7988868951797485\n",
            "step 15282: loss = 1.8820732831954956\n",
            "step 15283: loss = 1.6272553205490112\n",
            "step 15284: loss = 1.6386053562164307\n",
            "step 15285: loss = 1.4903920888900757\n",
            "step 15286: loss = 1.762657880783081\n",
            "step 15287: loss = 1.730635643005371\n",
            "step 15288: loss = 1.9705760478973389\n",
            "step 15289: loss = 1.6790794134140015\n",
            "step 15290: loss = 1.8810207843780518\n",
            "step 15291: loss = 1.7741634845733643\n",
            "step 15292: loss = 1.8472282886505127\n",
            "step 15293: loss = 1.9169285297393799\n",
            "step 15294: loss = 1.5064493417739868\n",
            "step 15295: loss = 1.7823541164398193\n",
            "step 15296: loss = 1.6789213418960571\n",
            "step 15297: loss = 1.5766338109970093\n",
            "step 15298: loss = 1.745162844657898\n",
            "step 15299: loss = 1.8123761415481567\n",
            "step 15300: loss = 1.8297405242919922\n",
            "step 15301: loss = 1.7710058689117432\n",
            "step 15302: loss = 1.48359215259552\n",
            "step 15303: loss = 1.7857666015625\n",
            "step 15304: loss = 1.9877429008483887\n",
            "step 15305: loss = 1.9069983959197998\n",
            "step 15306: loss = 1.727266788482666\n",
            "step 15307: loss = 1.7390843629837036\n",
            "step 15308: loss = 1.83009672164917\n",
            "step 15309: loss = 1.9008930921554565\n",
            "step 15310: loss = 1.7312442064285278\n",
            "step 15311: loss = 1.9790656566619873\n",
            "step 15312: loss = 1.712273120880127\n",
            "step 15313: loss = 1.426769495010376\n",
            "step 15314: loss = 1.8690911531448364\n",
            "step 15315: loss = 1.9134786128997803\n",
            "step 15316: loss = 1.7595888376235962\n",
            "step 15317: loss = 1.815659999847412\n",
            "step 15318: loss = 1.924774169921875\n",
            "step 15319: loss = 1.7331476211547852\n",
            "step 15320: loss = 1.7717679738998413\n",
            "step 15321: loss = 1.7905691862106323\n",
            "step 15322: loss = 1.9147498607635498\n",
            "step 15323: loss = 1.4332468509674072\n",
            "step 15324: loss = 1.7192184925079346\n",
            "step 15325: loss = 1.7413785457611084\n",
            "step 15326: loss = 1.6554800271987915\n",
            "step 15327: loss = 1.6585228443145752\n",
            "step 15328: loss = 1.9637356996536255\n",
            "step 15329: loss = 1.8023408651351929\n",
            "step 15330: loss = 1.5826325416564941\n",
            "step 15331: loss = 1.728506326675415\n",
            "step 15332: loss = 1.8146090507507324\n",
            "step 15333: loss = 1.7613146305084229\n",
            "step 15334: loss = 1.8198883533477783\n",
            "step 15335: loss = 1.790533185005188\n",
            "step 15336: loss = 1.6876928806304932\n",
            "step 15337: loss = 1.83555006980896\n",
            "step 15338: loss = 1.6622226238250732\n",
            "step 15339: loss = 1.8565222024917603\n",
            "step 15340: loss = 1.9298951625823975\n",
            "step 15341: loss = 1.918345332145691\n",
            "step 15342: loss = 1.5224218368530273\n",
            "step 15343: loss = 1.845162272453308\n",
            "step 15344: loss = 1.8033133745193481\n",
            "step 15345: loss = 1.842059850692749\n",
            "step 15346: loss = 1.715032696723938\n",
            "step 15347: loss = 1.708024024963379\n",
            "step 15348: loss = 1.7595882415771484\n",
            "step 15349: loss = 1.7713614702224731\n",
            "step 15350: loss = 1.914068579673767\n",
            "step 15351: loss = 1.8149738311767578\n",
            "step 15352: loss = 1.6468727588653564\n",
            "step 15353: loss = 1.904589056968689\n",
            "step 15354: loss = 1.8179645538330078\n",
            "step 15355: loss = 2.005333423614502\n",
            "step 15356: loss = 1.7750117778778076\n",
            "step 15357: loss = 1.8099168539047241\n",
            "step 15358: loss = 1.9278123378753662\n",
            "step 15359: loss = 1.8879525661468506\n",
            "step 15360: loss = 1.9071664810180664\n",
            "step 15361: loss = 2.0597620010375977\n",
            "step 15362: loss = 1.7751212120056152\n",
            "step 15363: loss = 1.630603313446045\n",
            "step 15364: loss = 1.6931102275848389\n",
            "step 15365: loss = 1.9709866046905518\n",
            "step 15366: loss = 1.8792741298675537\n",
            "step 15367: loss = 1.7054098844528198\n",
            "step 15368: loss = 2.0719830989837646\n",
            "step 15369: loss = 1.7438571453094482\n",
            "step 15370: loss = 1.6587692499160767\n",
            "step 15371: loss = 1.7051128149032593\n",
            "step 15372: loss = 1.7940136194229126\n",
            "step 15373: loss = 1.9801464080810547\n",
            "step 15374: loss = 1.9666218757629395\n",
            "step 15375: loss = 1.7949204444885254\n",
            "step 15376: loss = 1.8053042888641357\n",
            "step 15377: loss = 1.790506362915039\n",
            "step 15378: loss = 1.9160481691360474\n",
            "step 15379: loss = 1.8289769887924194\n",
            "step 15380: loss = 1.8473057746887207\n",
            "step 15381: loss = 1.9572128057479858\n",
            "step 15382: loss = 1.8500189781188965\n",
            "step 15383: loss = 2.027907371520996\n",
            "step 15384: loss = 1.8177083730697632\n",
            "step 15385: loss = 1.7357115745544434\n",
            "step 15386: loss = 1.7687684297561646\n",
            "step 15387: loss = 1.971717357635498\n",
            "step 15388: loss = 1.790866732597351\n",
            "step 15389: loss = 1.7319862842559814\n",
            "step 15390: loss = 1.8753713369369507\n",
            "step 15391: loss = 2.069847822189331\n",
            "step 15392: loss = 1.771447777748108\n",
            "step 15393: loss = 1.7641146183013916\n",
            "step 15394: loss = 1.8694263696670532\n",
            "step 15395: loss = 1.7642467021942139\n",
            "step 15396: loss = 1.6764947175979614\n",
            "step 15397: loss = 1.7146433591842651\n",
            "step 15398: loss = 2.0525338649749756\n",
            "step 15399: loss = 1.8855903148651123\n",
            "step 15400: loss = 1.915807843208313\n",
            "step 15401: loss = 1.9699369668960571\n",
            "step 15402: loss = 1.8462260961532593\n",
            "step 15403: loss = 1.6272399425506592\n",
            "step 15404: loss = 1.6684256792068481\n",
            "step 15405: loss = 1.895601511001587\n",
            "step 15406: loss = 1.7742066383361816\n",
            "step 15407: loss = 1.834281086921692\n",
            "step 15408: loss = 1.7528197765350342\n",
            "step 15409: loss = 2.01692271232605\n",
            "step 15410: loss = 1.858687400817871\n",
            "step 15411: loss = 1.865964412689209\n",
            "step 15412: loss = 1.6407972574234009\n",
            "step 15413: loss = 1.9547302722930908\n",
            "step 15414: loss = 1.859692931175232\n",
            "step 15415: loss = 1.8998005390167236\n",
            "step 15416: loss = 1.8357763290405273\n",
            "step 15417: loss = 1.7058740854263306\n",
            "step 15418: loss = 1.6819849014282227\n",
            "step 15419: loss = 1.7256348133087158\n",
            "step 15420: loss = 1.7704243659973145\n",
            "step 15421: loss = 1.9147456884384155\n",
            "step 15422: loss = 1.9473265409469604\n",
            "step 15423: loss = 1.832951307296753\n",
            "step 15424: loss = 1.6547164916992188\n",
            "step 15425: loss = 1.6491554975509644\n",
            "step 15426: loss = 1.632254719734192\n",
            "step 15427: loss = 1.9764474630355835\n",
            "step 15428: loss = 1.7431995868682861\n",
            "step 15429: loss = 1.694578766822815\n",
            "step 15430: loss = 1.6189260482788086\n",
            "step 15431: loss = 1.8698313236236572\n",
            "step 15432: loss = 1.7260984182357788\n",
            "step 15433: loss = 1.9757795333862305\n",
            "step 15434: loss = 1.9146337509155273\n",
            "step 15435: loss = 1.8066059350967407\n",
            "step 15436: loss = 1.7662843465805054\n",
            "step 15437: loss = 2.0953176021575928\n",
            "step 15438: loss = 1.5858397483825684\n",
            "step 15439: loss = 1.8391335010528564\n",
            "step 15440: loss = 1.81697416305542\n",
            "step 15441: loss = 1.728780746459961\n",
            "step 15442: loss = 1.62041437625885\n",
            "step 15443: loss = 1.8725093603134155\n",
            "step 15444: loss = 1.7731516361236572\n",
            "step 15445: loss = 1.792001724243164\n",
            "step 15446: loss = 1.9246855974197388\n",
            "step 15447: loss = 1.7232675552368164\n",
            "step 15448: loss = 1.7735331058502197\n",
            "step 15449: loss = 1.7536487579345703\n",
            "step 15450: loss = 1.6632853746414185\n",
            "step 15451: loss = 1.5556968450546265\n",
            "step 15452: loss = 1.942949891090393\n",
            "step 15453: loss = 1.9213941097259521\n",
            "step 15454: loss = 1.9007340669631958\n",
            "step 15455: loss = 1.7156376838684082\n",
            "step 15456: loss = 1.6657001972198486\n",
            "step 15457: loss = 2.0296037197113037\n",
            "step 15458: loss = 1.8531675338745117\n",
            "step 15459: loss = 1.7506271600723267\n",
            "step 15460: loss = 1.7904731035232544\n",
            "step 15461: loss = 1.8052699565887451\n",
            "step 15462: loss = 1.8118255138397217\n",
            "step 15463: loss = 1.9045841693878174\n",
            "step 15464: loss = 1.7436269521713257\n",
            "step 15465: loss = 1.7412699460983276\n",
            "step 15466: loss = 1.8079097270965576\n",
            "step 15467: loss = 1.888360857963562\n",
            "step 15468: loss = 1.7167414426803589\n",
            "step 15469: loss = 1.8481287956237793\n",
            "step 15470: loss = 1.9851640462875366\n",
            "step 15471: loss = 1.857418179512024\n",
            "step 15472: loss = 1.672864556312561\n",
            "step 15473: loss = 1.8018754720687866\n",
            "step 15474: loss = 1.9527106285095215\n",
            "step 15475: loss = 1.8313277959823608\n",
            "step 15476: loss = 1.9029803276062012\n",
            "step 15477: loss = 1.6928130388259888\n",
            "step 15478: loss = 1.6986933946609497\n",
            "step 15479: loss = 1.7492700815200806\n",
            "step 15480: loss = 1.5647226572036743\n",
            "step 15481: loss = 1.899675965309143\n",
            "step 15482: loss = 1.881415843963623\n",
            "step 15483: loss = 1.7326215505599976\n",
            "step 15484: loss = 1.6228681802749634\n",
            "step 15485: loss = 1.9364707469940186\n",
            "step 15486: loss = 1.714322566986084\n",
            "step 15487: loss = 1.906158447265625\n",
            "step 15488: loss = 1.6596559286117554\n",
            "step 15489: loss = 2.024609327316284\n",
            "step 15490: loss = 1.894097089767456\n",
            "step 15491: loss = 1.9284077882766724\n",
            "step 15492: loss = 1.604714274406433\n",
            "step 15493: loss = 1.7591753005981445\n",
            "step 15494: loss = 1.8043828010559082\n",
            "step 15495: loss = 1.820444941520691\n",
            "step 15496: loss = 1.9802464246749878\n",
            "step 15497: loss = 1.753137230873108\n",
            "step 15498: loss = 1.6246106624603271\n",
            "step 15499: loss = 1.7864105701446533\n",
            "step 15500: loss = 1.6654516458511353\n",
            "step 15501: loss = 1.750451922416687\n",
            "step 15502: loss = 1.8407710790634155\n",
            "step 15503: loss = 1.9669784307479858\n",
            "step 15504: loss = 1.7747142314910889\n",
            "step 15505: loss = 1.9207682609558105\n",
            "step 15506: loss = 1.8231441974639893\n",
            "step 15507: loss = 1.879162311553955\n",
            "step 15508: loss = 1.6842231750488281\n",
            "step 15509: loss = 1.7896829843521118\n",
            "step 15510: loss = 1.598999261856079\n",
            "step 15511: loss = 1.8842777013778687\n",
            "step 15512: loss = 1.7871778011322021\n",
            "step 15513: loss = 1.8488277196884155\n",
            "step 15514: loss = 1.7436842918395996\n",
            "step 15515: loss = 1.812029480934143\n",
            "step 15516: loss = 1.9630789756774902\n",
            "step 15517: loss = 1.7603728771209717\n",
            "step 15518: loss = 1.793574333190918\n",
            "step 15519: loss = 2.168259382247925\n",
            "step 15520: loss = 1.8622760772705078\n",
            "step 15521: loss = 1.691749095916748\n",
            "step 15522: loss = 1.8583327531814575\n",
            "step 15523: loss = 1.8086305856704712\n",
            "step 15524: loss = 1.6867748498916626\n",
            "step 15525: loss = 1.6330552101135254\n",
            "step 15526: loss = 1.836777687072754\n",
            "step 15527: loss = 1.6680022478103638\n",
            "step 15528: loss = 1.739017367362976\n",
            "step 15529: loss = 1.9885404109954834\n",
            "step 15530: loss = 1.7164078950881958\n",
            "step 15531: loss = 1.92146635055542\n",
            "step 15532: loss = 1.7496309280395508\n",
            "step 15533: loss = 1.8349827527999878\n",
            "step 15534: loss = 1.892280101776123\n",
            "step 15535: loss = 2.13051700592041\n",
            "step 15536: loss = 1.8671411275863647\n",
            "step 15537: loss = 1.7219916582107544\n",
            "step 15538: loss = 1.714119791984558\n",
            "step 15539: loss = 1.6989351511001587\n",
            "step 15540: loss = 1.7896080017089844\n",
            "step 15541: loss = 1.5217000246047974\n",
            "step 15542: loss = 1.6934934854507446\n",
            "step 15543: loss = 1.8658126592636108\n",
            "step 15544: loss = 1.791346549987793\n",
            "step 15545: loss = 1.9048433303833008\n",
            "step 15546: loss = 1.947338581085205\n",
            "step 15547: loss = 1.6986972093582153\n",
            "step 15548: loss = 1.8306018114089966\n",
            "step 15549: loss = 1.814328670501709\n",
            "step 15550: loss = 1.6605618000030518\n",
            "step 15551: loss = 1.6718695163726807\n",
            "step 15552: loss = 1.912576675415039\n",
            "step 15553: loss = 1.8672761917114258\n",
            "step 15554: loss = 1.8847732543945312\n",
            "step 15555: loss = 1.7462238073349\n",
            "step 15556: loss = 1.922961711883545\n",
            "step 15557: loss = 1.7626234292984009\n",
            "step 15558: loss = 1.731447696685791\n",
            "step 15559: loss = 1.6023426055908203\n",
            "step 15560: loss = 1.6630244255065918\n",
            "step 15561: loss = 1.5903373956680298\n",
            "step 15562: loss = 1.7903879880905151\n",
            "step 15563: loss = 2.0265228748321533\n",
            "step 15564: loss = 1.755280613899231\n",
            "step 15565: loss = 1.90273916721344\n",
            "step 15566: loss = 1.814776062965393\n",
            "step 15567: loss = 1.7500947713851929\n",
            "step 15568: loss = 1.8322079181671143\n",
            "step 15569: loss = 1.7800577878952026\n",
            "step 15570: loss = 1.988743782043457\n",
            "step 15571: loss = 1.75184965133667\n",
            "step 15572: loss = 1.4607023000717163\n",
            "step 15573: loss = 1.865807056427002\n",
            "step 15574: loss = 1.7416510581970215\n",
            "step 15575: loss = 1.7836885452270508\n",
            "step 15576: loss = 2.0131473541259766\n",
            "step 15577: loss = 1.83126699924469\n",
            "step 15578: loss = 1.772015929222107\n",
            "step 15579: loss = 1.8463972806930542\n",
            "step 15580: loss = 1.740692138671875\n",
            "step 15581: loss = 1.6975499391555786\n",
            "step 15582: loss = 2.013155460357666\n",
            "step 15583: loss = 1.8313910961151123\n",
            "step 15584: loss = 1.8406041860580444\n",
            "step 15585: loss = 1.6351829767227173\n",
            "step 15586: loss = 1.6740758419036865\n",
            "step 15587: loss = 1.7052912712097168\n",
            "step 15588: loss = 1.841728925704956\n",
            "step 15589: loss = 1.734681248664856\n",
            "step 15590: loss = 1.8617087602615356\n",
            "step 15591: loss = 1.7743144035339355\n",
            "step 15592: loss = 1.7681301832199097\n",
            "step 15593: loss = 1.6175010204315186\n",
            "step 15594: loss = 2.020220994949341\n",
            "step 15595: loss = 2.043883800506592\n",
            "step 15596: loss = 1.8837530612945557\n",
            "step 15597: loss = 1.9124186038970947\n",
            "step 15598: loss = 1.8872368335723877\n",
            "step 15599: loss = 1.6927828788757324\n",
            "step 15600: loss = 1.583207130432129\n",
            "step 15601: loss = 1.718714952468872\n",
            "step 15602: loss = 1.7579691410064697\n",
            "step 15603: loss = 1.7425917387008667\n",
            "step 15604: loss = 1.7454071044921875\n",
            "step 15605: loss = 1.7190824747085571\n",
            "step 15606: loss = 1.599707841873169\n",
            "step 15607: loss = 1.766944169998169\n",
            "step 15608: loss = 2.0199666023254395\n",
            "step 15609: loss = 1.7896206378936768\n",
            "step 15610: loss = 1.7506541013717651\n",
            "step 15611: loss = 1.9431785345077515\n",
            "step 15612: loss = 1.642861008644104\n",
            "step 15613: loss = 1.929835319519043\n",
            "step 15614: loss = 1.6626484394073486\n",
            "step 15615: loss = 1.886653184890747\n",
            "step 15616: loss = 1.9770891666412354\n",
            "step 15617: loss = 1.9787826538085938\n",
            "step 15618: loss = 1.840920329093933\n",
            "step 15619: loss = 1.8221746683120728\n",
            "step 15620: loss = 1.9297393560409546\n",
            "Finish epoch 10\n",
            "New model saved, minimum loss: 1.6929742412286286 \n",
            "\n",
            "\n",
            "Training time (mins):  61.48624985218048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g70Od7FHKMb",
        "colab_type": "text"
      },
      "source": [
        "### **Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyuQk5ptv9fP",
        "colab_type": "code",
        "outputId": "4d492d3c-21b3-49c3-8b02-30b3279a32e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "# The hyper-parameters of the Model (should be the same as in Training)\n",
        "params=dict()\n",
        "params['num_layers']=2\n",
        "params['num_hiddens']=150\n",
        "params['learning_rate']=0.001\n",
        "params['keep_prob']=0.85\n",
        "params['beam_width']=10\n",
        "\n",
        "# Path of the saved model\n",
        "checkpoint = \"Saved Models/Text Summarization/Using Datasets/TSModel.ckpt\"\n",
        "\n",
        "# Reset the default graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Get the first 10 validation articles and validation titles\n",
        "valid_article=getSentList(valid_article_path,100)\n",
        "valid_title=getSentList(valid_title_path,100)\n",
        "\n",
        "test_dataset=get_testDatasets(valid_article,title_max_len,word2int,batch_size)\n",
        "\n",
        "# Create an Initializable iterator\n",
        "test_iterator = test_dataset.make_initializable_iterator()\n",
        "\n",
        "# Set paths to the saved model\n",
        "checkpoint = \"Saved Models/Text Summarization/Using Datasets/TSModel.ckpt\"\n",
        "\n",
        "num_batches_epoch = len(valid_article)//batch_size\n",
        "with tf.Session() as sess:\n",
        "  # Run initializer\n",
        "  sess.run(tf.tables_initializer())\n",
        "  sess.run(test_iterator.initializer)  \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Load saved model \n",
        "  # Use Seq2SeqModel to create the same graph as saved model\n",
        "  loaded_model=Seq2SeqModel(test_iterator, params, batch_size, word_emb,train=False)\n",
        "  \n",
        "  # Load the value of variables in saved model\n",
        "  saver = tf.train.Saver(tf.global_variables())\n",
        "  saver.restore(sess, checkpoint)\n",
        "  \n",
        "  # Interate over batches\n",
        "  for batch_i in range(num_batches_epoch):\n",
        "    \n",
        "    # Get the decoder output by Inference\n",
        "    decoder_outputs=sess.run(loaded_model.decoder_outputs)    \n",
        "    \n",
        "    # Convert from sequence of int to actual sentence\n",
        "    output_titles=[]    \n",
        "    # Loop through each seq in decoder_outputs\n",
        "    for out_seq in decoder_outputs:  \n",
        "      out_sent=list()\n",
        "      for word_int in out_seq:    \n",
        "        # Convert int to word \n",
        "        word=int2word[word_int]  \n",
        "        # Stop converting when it reach to the end of ouput sentence\n",
        "        if word == \"</s>\":\n",
        "          break\n",
        "        else:\n",
        "          out_sent.append(word)\n",
        "      # Combine list of word to sentence and add this sentence to output_titles\n",
        "      output_titles.append(\" \".join(out_sent))\n",
        "      \n",
        "# Display the results      \n",
        "for i in range(10):\n",
        "  print(\"Article: \",valid_article[i])\n",
        "  print(\"Actual title: \",valid_title[i])\n",
        "  print(\"Generated title: \",output_titles[i],'\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from Saved Models/Text Summarization/Using Datasets/TSModel.ckpt\n",
            "Article:  five-time world champion michelle kwan withdrew from the # us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the # turin olympics .\n",
            "Actual title:  injury leaves kwan 's olympic hopes in limbo\n",
            "Generated title:  new zealand and sri lanka look on target \n",
            "\n",
            "Article:  us business leaders lashed out wednesday at legislation that would penalize companies for employing illegal immigrants .\n",
            "Actual title:  us business attacks tough immigration law\n",
            "Generated title:  final results of key iraqi general elections \n",
            "\n",
            "Article:  general motors corp. said wednesday its us sales fell # percent in december and four percent in # with the biggest losses coming from passenger car sales .\n",
            "Actual title:  gm december sales fall # percent\n",
            "Generated title:  <unk> of burkina faso \n",
            "\n",
            "Article:  several thousand people gathered on wednesday evening on the main square in zagreb for a public draw and an open air party to celebrate the croatian capital 's second chance to host the women 's slalom world cup .\n",
            "Actual title:  thousands of croatians celebrate before world cup slalom\n",
            "Generated title:  australian fm calls for un security council \n",
            "\n",
            "Article:  us first lady laura bush and us secretary of state condoleezza rice will represent the united states later this month at the inauguration of liberia 's president-elect ellen johnson sirleaf , the white house said wednesday .\n",
            "Actual title:  laura bush <unk> rice to attend sirleaf 's inauguration in liberia\n",
            "Generated title:  supersub cowboys release for nfl \n",
            "\n",
            "Article:  jack abramoff , a former lobbyist at the center of a mushrooming political scandal in washington , on wednesday pleaded guilty in a us court in miami to defrauding lenders in a florida gambling boat deal .\n",
            "Actual title:  top republican lobbyist pleads guilty to florida fraud\n",
            "Generated title:  bush says first lady and the first lady address \n",
            "\n",
            "Article:  somalia 's feuding president and parliament speaker have agreed a compromise in a bitter row over the appropriate seat for their fledgling transitional government , a yemeni official said wednesday .\n",
            "Actual title:  somalia rivals in compromise on seat of government\n",
            "Generated title:  two new bird vaccine panel \n",
            "\n",
            "Article:  a # hour strike by airport workers in portugal planned for friday over job security issues could lead to flight cancellations , union and airport officials said .\n",
            "Actual title:  portuguese airport workers strike could ground flights on friday\n",
            "Generated title:  agassi withdraws from agassi \n",
            "\n",
            "Article:  jose mourinho renewed his partnership with portuguese international maniche on wednesday when he completed the loan signing of the # year-old midfielder from dynamo moscow .\n",
            "Actual title:  maniche renews partnership with mourinho\n",
            "Generated title:  skorea to build up shipbuilding fleet \n",
            "\n",
            "Article:  hollywood is planning a new sequel to adventure flick `` ocean 's eleven , '' with star george clooney set to reprise his role as a charismatic thief in `` ocean 's thirteen , '' the entertainment press said wednesday .\n",
            "Actual title:  hollywood shores up support for ocean 's thirteen\n",
            "Generated title:  dutch face world cup final \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQh3SsThogt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}